//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 16 .b8 	__local_depot0[352];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<79>;
	.reg .b16 	%rs<27>;
	.reg .b32 	%r<616>;
	.reg .b64 	%rd<88>;


	mov.u64 	%rd87, __local_depot0;
	cvta.local.u64 	%SP, %rd87;
	ld.param.u64 	%rd35, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0];
	ld.param.u64 	%rd32, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2];
	ld.param.u64 	%rd33, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3];
	ld.param.u64 	%rd36, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4];
	ld.param.u32 	%r85, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6];
	cvta.to.global.u64 	%rd2, %rd36;
	cvta.to.global.u64 	%rd3, %rd35;
	add.u64 	%rd38, %SP, 256;
	cvta.to.local.u64 	%rd4, %rd38;
	mov.u32 	%r87, %ntid.x;
	mov.u32 	%r88, %ctaid.x;
	mov.u32 	%r89, %tid.x;
	mad.lo.s32 	%r1, %r87, %r88, %r89;
	mov.u32 	%r594, -8;
	mov.u64 	%rd78, %rd4;
	mov.u64 	%rd79, %rd3;

BB0_1:
	.pragma "nounroll";
	ld.global.u8 	%r90, [%rd79];
	shl.b32 	%r91, %r90, 24;
	ld.global.u8 	%r92, [%rd79+1];
	shl.b32 	%r93, %r92, 16;
	or.b32  	%r94, %r93, %r91;
	ld.global.u8 	%rs1, [%rd79+2];
	mul.wide.u16 	%r95, %rs1, 256;
	or.b32  	%r96, %r94, %r95;
	ld.global.u8 	%r97, [%rd79+3];
	or.b32  	%r98, %r96, %r97;
	st.local.u32 	[%rd78], %r98;
	add.s64 	%rd79, %rd79, 4;
	add.s64 	%rd78, %rd78, 4;
	add.s32 	%r594, %r594, 1;
	setp.ne.s32	%p2, %r594, 0;
	@%p2 bra 	BB0_1;

	cvta.to.global.u64 	%rd10, %rd33;
	cvta.to.global.u64 	%rd11, %rd32;
	add.u64 	%rd40, %SP, 0;
	cvta.to.local.u64 	%rd12, %rd40;
	mov.u32 	%r595, -5;
	mov.u64 	%rd80, -8;
	mov.u64 	%rd81, %rd11;

BB0_3:
	.pragma "nounroll";
	ld.global.u8 	%r100, [%rd81];
	shl.b32 	%r101, %r100, 24;
	ld.global.u8 	%r102, [%rd81+1];
	shl.b32 	%r103, %r102, 16;
	or.b32  	%r104, %r103, %r101;
	ld.global.u8 	%rs2, [%rd81+2];
	mul.wide.u16 	%r105, %rs2, 256;
	or.b32  	%r106, %r104, %r105;
	ld.global.u8 	%r107, [%rd81+3];
	or.b32  	%r108, %r106, %r107;
	shl.b64 	%rd41, %rd80, 2;
	sub.s64 	%rd42, %rd4, %rd41;
	st.local.u32 	[%rd42], %r108;
	add.s64 	%rd81, %rd81, 4;
	add.s64 	%rd80, %rd80, -1;
	add.s32 	%r595, %r595, 1;
	setp.ne.s32	%p3, %r595, 0;
	@%p3 bra 	BB0_3;

	ld.global.u8 	%r109, [%rd11+20];
	shl.b32 	%r110, %r109, 24;
	ld.global.u8 	%r111, [%rd11+21];
	shl.b32 	%r112, %r111, 16;
	ld.global.u8 	%rs3, [%rd11+22];
	mul.wide.u16 	%r113, %rs3, 256;
	or.b32  	%r114, %r110, %r112;
	or.b32  	%r115, %r114, %r113;
	or.b32  	%r116, %r115, 128;
	st.local.u32 	[%rd4+52], %r116;
	mov.u32 	%r117, 0;
	st.local.u32 	[%rd4+56], %r117;
	mov.u32 	%r118, 440;
	st.local.u32 	[%rd4+60], %r118;
	ld.local.u32 	%r119, [%rd4+36];
	xor.b32  	%r120, %r119, %r1;
	ld.local.u32 	%r121, [%rd4+40];
	st.local.u32 	[%rd4+36], %r120;
	xor.b32  	%r122, %r121, %r85;
	st.local.u32 	[%rd4+40], %r122;
	mov.u64 	%rd82, 0;
	bra.uni 	BB0_5;

BB0_10:
	ld.local.u32 	%r123, [%rd4+44];
	add.s32 	%r6, %r123, 1;
	ld.local.u32 	%r596, [%rd4+48];
	st.local.u32 	[%rd4+44], %r6;
	setp.ne.s32	%p7, %r6, 0;
	@%p7 bra 	BB0_12;

	add.s32 	%r596, %r596, 1;
	st.local.u32 	[%rd4+48], %r596;

BB0_12:
	mov.u32 	%r598, 0;
	mov.u64 	%rd53, 3144134277;
	st.local.u32 	[%rd4+68], %rd53;
	mov.u64 	%rd54, 1779033703;
	st.local.u32 	[%rd4+64], %rd54;
	mov.u64 	%rd55, 2773480762;
	st.local.u32 	[%rd4+76], %rd55;
	mov.u64 	%rd56, 1013904242;
	st.local.u32 	[%rd4+72], %rd56;
	mov.u64 	%rd57, 2600822924;
	st.local.u32 	[%rd4+84], %rd57;
	mov.u64 	%rd58, 1359893119;
	st.local.u32 	[%rd4+80], %rd58;
	mov.u64 	%rd59, 1541459225;
	st.local.u32 	[%rd4+92], %rd59;
	mov.u64 	%rd60, 528734635;
	st.local.u32 	[%rd4+88], %rd60;
	ld.local.u32 	%r597, [%rd4];
	ld.local.u32 	%r133, [%rd4+12];
	ld.local.u32 	%r129, [%rd4+8];
	ld.local.u32 	%r125, [%rd4+4];
	ld.local.u32 	%r149, [%rd4+28];
	ld.local.u32 	%r145, [%rd4+24];
	ld.local.u32 	%r141, [%rd4+20];
	ld.local.u32 	%r137, [%rd4+16];
	ld.local.u32 	%r161, [%rd4+40];
	ld.local.u32 	%r157, [%rd4+36];
	ld.local.u32 	%r153, [%rd4+32];
	ld.local.u32 	%r131, [%rd4+60];
	ld.local.u32 	%r127, [%rd4+56];
	ld.local.u32 	%r173, [%rd4+52];
	st.local.v4.u32 	[%rd12], {%r597, %r125, %r129, %r133};
	st.local.v4.u32 	[%rd12+16], {%r137, %r141, %r145, %r149};
	add.s32 	%r587, %r123, 1;
	st.local.v4.u32 	[%rd12+32], {%r153, %r157, %r161, %r587};
	st.local.v4.u32 	[%rd12+48], {%r596, %r173, %r127, %r131};
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r125, %r125, 7;
	 shf.r.clamp.b32    t2, %r125, %r125, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r125, 3;
	 xor.b32            %r124, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r127, %r127, 17;
	 shf.r.clamp.b32    t2, %r127, %r127, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r127, 10;
	 xor.b32            %r126, t1, t2;
	}
	// inline asm
	add.s32 	%r325, %r126, %r124;
	add.s32 	%r326, %r325, %r157;
	add.s32 	%r185, %r326, %r597;
	st.local.u32 	[%rd12+64], %r185;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r129, %r129, 7;
	 shf.r.clamp.b32    t2, %r129, %r129, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r129, 3;
	 xor.b32            %r128, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r131, %r131, 17;
	 shf.r.clamp.b32    t2, %r131, %r131, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r131, 10;
	 xor.b32            %r130, t1, t2;
	}
	// inline asm
	add.s32 	%r327, %r130, %r128;
	add.s32 	%r328, %r327, %r161;
	add.s32 	%r189, %r328, %r125;
	st.local.u32 	[%rd12+68], %r189;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r133, %r133, 7;
	 shf.r.clamp.b32    t2, %r133, %r133, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r133, 3;
	 xor.b32            %r132, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r185, %r185, 17;
	 shf.r.clamp.b32    t2, %r185, %r185, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r185, 10;
	 xor.b32            %r134, t1, t2;
	}
	// inline asm
	add.s32 	%r329, %r134, %r132;
	add.s32 	%r330, %r329, %r6;
	add.s32 	%r193, %r330, %r129;
	st.local.u32 	[%rd12+72], %r193;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r137, %r137, 7;
	 shf.r.clamp.b32    t2, %r137, %r137, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r137, 3;
	 xor.b32            %r136, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r189, %r189, 17;
	 shf.r.clamp.b32    t2, %r189, %r189, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r189, 10;
	 xor.b32            %r138, t1, t2;
	}
	// inline asm
	add.s32 	%r331, %r138, %r136;
	add.s32 	%r332, %r331, %r596;
	add.s32 	%r197, %r332, %r133;
	st.local.u32 	[%rd12+76], %r197;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r141, %r141, 7;
	 shf.r.clamp.b32    t2, %r141, %r141, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r141, 3;
	 xor.b32            %r140, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r193, %r193, 17;
	 shf.r.clamp.b32    t2, %r193, %r193, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r193, 10;
	 xor.b32            %r142, t1, t2;
	}
	// inline asm
	add.s32 	%r333, %r142, %r140;
	add.s32 	%r334, %r333, %r173;
	add.s32 	%r201, %r334, %r137;
	st.local.u32 	[%rd12+80], %r201;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r145, %r145, 7;
	 shf.r.clamp.b32    t2, %r145, %r145, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r145, 3;
	 xor.b32            %r144, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r197, %r197, 17;
	 shf.r.clamp.b32    t2, %r197, %r197, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r197, 10;
	 xor.b32            %r146, t1, t2;
	}
	// inline asm
	add.s32 	%r335, %r146, %r144;
	add.s32 	%r336, %r335, %r127;
	add.s32 	%r205, %r336, %r141;
	st.local.u32 	[%rd12+84], %r205;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r149, %r149, 7;
	 shf.r.clamp.b32    t2, %r149, %r149, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r149, 3;
	 xor.b32            %r148, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r201, %r201, 17;
	 shf.r.clamp.b32    t2, %r201, %r201, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r201, 10;
	 xor.b32            %r150, t1, t2;
	}
	// inline asm
	add.s32 	%r337, %r150, %r148;
	add.s32 	%r338, %r337, %r131;
	add.s32 	%r209, %r338, %r145;
	st.local.u32 	[%rd12+88], %r209;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r153, %r153, 7;
	 shf.r.clamp.b32    t2, %r153, %r153, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r153, 3;
	 xor.b32            %r152, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r205, %r205, 17;
	 shf.r.clamp.b32    t2, %r205, %r205, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r205, 10;
	 xor.b32            %r154, t1, t2;
	}
	// inline asm
	add.s32 	%r339, %r154, %r152;
	add.s32 	%r340, %r339, %r185;
	add.s32 	%r213, %r340, %r149;
	st.local.u32 	[%rd12+92], %r213;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r157, %r157, 7;
	 shf.r.clamp.b32    t2, %r157, %r157, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r157, 3;
	 xor.b32            %r156, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r209, %r209, 17;
	 shf.r.clamp.b32    t2, %r209, %r209, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r209, 10;
	 xor.b32            %r158, t1, t2;
	}
	// inline asm
	add.s32 	%r341, %r158, %r156;
	add.s32 	%r342, %r341, %r189;
	add.s32 	%r217, %r342, %r153;
	st.local.u32 	[%rd12+96], %r217;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r161, %r161, 7;
	 shf.r.clamp.b32    t2, %r161, %r161, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r161, 3;
	 xor.b32            %r160, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r213, %r213, 17;
	 shf.r.clamp.b32    t2, %r213, %r213, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r213, 10;
	 xor.b32            %r162, t1, t2;
	}
	// inline asm
	add.s32 	%r343, %r162, %r160;
	add.s32 	%r344, %r343, %r193;
	add.s32 	%r221, %r344, %r157;
	st.local.u32 	[%rd12+100], %r221;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r6, %r6, 7;
	 shf.r.clamp.b32    t2, %r6, %r6, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r6, 3;
	 xor.b32            %r164, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r217, %r217, 17;
	 shf.r.clamp.b32    t2, %r217, %r217, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r217, 10;
	 xor.b32            %r166, t1, t2;
	}
	// inline asm
	add.s32 	%r345, %r166, %r164;
	add.s32 	%r346, %r345, %r197;
	add.s32 	%r225, %r346, %r161;
	st.local.u32 	[%rd12+104], %r225;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r596, %r596, 7;
	 shf.r.clamp.b32    t2, %r596, %r596, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r596, 3;
	 xor.b32            %r168, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r221, %r221, 17;
	 shf.r.clamp.b32    t2, %r221, %r221, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r221, 10;
	 xor.b32            %r170, t1, t2;
	}
	// inline asm
	add.s32 	%r347, %r170, %r168;
	add.s32 	%r348, %r347, %r201;
	add.s32 	%r229, %r348, %r6;
	st.local.u32 	[%rd12+108], %r229;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r173, %r173, 7;
	 shf.r.clamp.b32    t2, %r173, %r173, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r173, 3;
	 xor.b32            %r172, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r225, %r225, 17;
	 shf.r.clamp.b32    t2, %r225, %r225, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r225, 10;
	 xor.b32            %r174, t1, t2;
	}
	// inline asm
	add.s32 	%r349, %r174, %r172;
	add.s32 	%r350, %r349, %r205;
	add.s32 	%r233, %r350, %r596;
	st.local.u32 	[%rd12+112], %r233;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r127, %r127, 7;
	 shf.r.clamp.b32    t2, %r127, %r127, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r127, 3;
	 xor.b32            %r176, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r229, %r229, 17;
	 shf.r.clamp.b32    t2, %r229, %r229, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r229, 10;
	 xor.b32            %r178, t1, t2;
	}
	// inline asm
	add.s32 	%r351, %r178, %r176;
	add.s32 	%r352, %r351, %r209;
	add.s32 	%r237, %r352, %r173;
	st.local.u32 	[%rd12+116], %r237;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r131, %r131, 7;
	 shf.r.clamp.b32    t2, %r131, %r131, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r131, 3;
	 xor.b32            %r180, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r233, %r233, 17;
	 shf.r.clamp.b32    t2, %r233, %r233, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r233, 10;
	 xor.b32            %r182, t1, t2;
	}
	// inline asm
	add.s32 	%r353, %r182, %r180;
	add.s32 	%r354, %r353, %r213;
	add.s32 	%r241, %r354, %r127;
	st.local.u32 	[%rd12+120], %r241;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r185, %r185, 7;
	 shf.r.clamp.b32    t2, %r185, %r185, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r185, 3;
	 xor.b32            %r184, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r237, %r237, 17;
	 shf.r.clamp.b32    t2, %r237, %r237, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 10;
	 xor.b32            %r186, t1, t2;
	}
	// inline asm
	add.s32 	%r355, %r186, %r184;
	add.s32 	%r356, %r355, %r217;
	add.s32 	%r245, %r356, %r131;
	st.local.u32 	[%rd12+124], %r245;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r189, %r189, 7;
	 shf.r.clamp.b32    t2, %r189, %r189, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r189, 3;
	 xor.b32            %r188, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r241, %r241, 17;
	 shf.r.clamp.b32    t2, %r241, %r241, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 10;
	 xor.b32            %r190, t1, t2;
	}
	// inline asm
	add.s32 	%r357, %r190, %r188;
	add.s32 	%r358, %r357, %r221;
	add.s32 	%r249, %r358, %r185;
	st.local.u32 	[%rd12+128], %r249;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r193, %r193, 7;
	 shf.r.clamp.b32    t2, %r193, %r193, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r193, 3;
	 xor.b32            %r192, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r245, %r245, 17;
	 shf.r.clamp.b32    t2, %r245, %r245, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r245, 10;
	 xor.b32            %r194, t1, t2;
	}
	// inline asm
	add.s32 	%r359, %r194, %r192;
	add.s32 	%r360, %r359, %r225;
	add.s32 	%r253, %r360, %r189;
	st.local.u32 	[%rd12+132], %r253;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r197, %r197, 7;
	 shf.r.clamp.b32    t2, %r197, %r197, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r197, 3;
	 xor.b32            %r196, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r249, %r249, 17;
	 shf.r.clamp.b32    t2, %r249, %r249, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r249, 10;
	 xor.b32            %r198, t1, t2;
	}
	// inline asm
	add.s32 	%r361, %r198, %r196;
	add.s32 	%r362, %r361, %r229;
	add.s32 	%r257, %r362, %r193;
	st.local.u32 	[%rd12+136], %r257;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r201, %r201, 7;
	 shf.r.clamp.b32    t2, %r201, %r201, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r201, 3;
	 xor.b32            %r200, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 17;
	 shf.r.clamp.b32    t2, %r253, %r253, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 10;
	 xor.b32            %r202, t1, t2;
	}
	// inline asm
	add.s32 	%r363, %r202, %r200;
	add.s32 	%r364, %r363, %r233;
	add.s32 	%r261, %r364, %r197;
	st.local.u32 	[%rd12+140], %r261;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r205, %r205, 7;
	 shf.r.clamp.b32    t2, %r205, %r205, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r205, 3;
	 xor.b32            %r204, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r257, %r257, 17;
	 shf.r.clamp.b32    t2, %r257, %r257, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r257, 10;
	 xor.b32            %r206, t1, t2;
	}
	// inline asm
	add.s32 	%r365, %r206, %r204;
	add.s32 	%r366, %r365, %r237;
	add.s32 	%r265, %r366, %r201;
	st.local.u32 	[%rd12+144], %r265;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r209, %r209, 7;
	 shf.r.clamp.b32    t2, %r209, %r209, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r209, 3;
	 xor.b32            %r208, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r261, %r261, 17;
	 shf.r.clamp.b32    t2, %r261, %r261, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r261, 10;
	 xor.b32            %r210, t1, t2;
	}
	// inline asm
	add.s32 	%r367, %r210, %r208;
	add.s32 	%r368, %r367, %r241;
	add.s32 	%r269, %r368, %r205;
	st.local.u32 	[%rd12+148], %r269;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r213, %r213, 7;
	 shf.r.clamp.b32    t2, %r213, %r213, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r213, 3;
	 xor.b32            %r212, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 17;
	 shf.r.clamp.b32    t2, %r265, %r265, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 10;
	 xor.b32            %r214, t1, t2;
	}
	// inline asm
	add.s32 	%r369, %r214, %r212;
	add.s32 	%r370, %r369, %r245;
	add.s32 	%r273, %r370, %r209;
	st.local.u32 	[%rd12+152], %r273;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r217, %r217, 7;
	 shf.r.clamp.b32    t2, %r217, %r217, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r217, 3;
	 xor.b32            %r216, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r269, %r269, 17;
	 shf.r.clamp.b32    t2, %r269, %r269, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r269, 10;
	 xor.b32            %r218, t1, t2;
	}
	// inline asm
	add.s32 	%r371, %r218, %r216;
	add.s32 	%r372, %r371, %r249;
	add.s32 	%r277, %r372, %r213;
	st.local.u32 	[%rd12+156], %r277;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r221, %r221, 7;
	 shf.r.clamp.b32    t2, %r221, %r221, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r221, 3;
	 xor.b32            %r220, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r273, %r273, 17;
	 shf.r.clamp.b32    t2, %r273, %r273, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r273, 10;
	 xor.b32            %r222, t1, t2;
	}
	// inline asm
	add.s32 	%r373, %r222, %r220;
	add.s32 	%r374, %r373, %r253;
	add.s32 	%r281, %r374, %r217;
	st.local.u32 	[%rd12+160], %r281;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r225, %r225, 7;
	 shf.r.clamp.b32    t2, %r225, %r225, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r225, 3;
	 xor.b32            %r224, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 17;
	 shf.r.clamp.b32    t2, %r277, %r277, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 10;
	 xor.b32            %r226, t1, t2;
	}
	// inline asm
	add.s32 	%r375, %r226, %r224;
	add.s32 	%r376, %r375, %r257;
	add.s32 	%r285, %r376, %r221;
	st.local.u32 	[%rd12+164], %r285;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r229, %r229, 7;
	 shf.r.clamp.b32    t2, %r229, %r229, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r229, 3;
	 xor.b32            %r228, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r281, %r281, 17;
	 shf.r.clamp.b32    t2, %r281, %r281, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r281, 10;
	 xor.b32            %r230, t1, t2;
	}
	// inline asm
	add.s32 	%r377, %r230, %r228;
	add.s32 	%r378, %r377, %r261;
	add.s32 	%r289, %r378, %r225;
	st.local.u32 	[%rd12+168], %r289;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r233, %r233, 7;
	 shf.r.clamp.b32    t2, %r233, %r233, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r233, 3;
	 xor.b32            %r232, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r285, %r285, 17;
	 shf.r.clamp.b32    t2, %r285, %r285, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r285, 10;
	 xor.b32            %r234, t1, t2;
	}
	// inline asm
	add.s32 	%r379, %r234, %r232;
	add.s32 	%r380, %r379, %r265;
	add.s32 	%r293, %r380, %r229;
	st.local.u32 	[%rd12+172], %r293;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r237, %r237, 7;
	 shf.r.clamp.b32    t2, %r237, %r237, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 3;
	 xor.b32            %r236, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 17;
	 shf.r.clamp.b32    t2, %r289, %r289, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 10;
	 xor.b32            %r238, t1, t2;
	}
	// inline asm
	add.s32 	%r381, %r238, %r236;
	add.s32 	%r382, %r381, %r269;
	add.s32 	%r297, %r382, %r233;
	st.local.u32 	[%rd12+176], %r297;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r241, %r241, 7;
	 shf.r.clamp.b32    t2, %r241, %r241, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 3;
	 xor.b32            %r240, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r293, %r293, 17;
	 shf.r.clamp.b32    t2, %r293, %r293, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r293, 10;
	 xor.b32            %r242, t1, t2;
	}
	// inline asm
	add.s32 	%r383, %r242, %r240;
	add.s32 	%r384, %r383, %r273;
	add.s32 	%r301, %r384, %r237;
	st.local.u32 	[%rd12+180], %r301;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r245, %r245, 7;
	 shf.r.clamp.b32    t2, %r245, %r245, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r245, 3;
	 xor.b32            %r244, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r297, %r297, 17;
	 shf.r.clamp.b32    t2, %r297, %r297, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r297, 10;
	 xor.b32            %r246, t1, t2;
	}
	// inline asm
	add.s32 	%r385, %r246, %r244;
	add.s32 	%r386, %r385, %r277;
	add.s32 	%r305, %r386, %r241;
	st.local.u32 	[%rd12+184], %r305;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r249, %r249, 7;
	 shf.r.clamp.b32    t2, %r249, %r249, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r249, 3;
	 xor.b32            %r248, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 17;
	 shf.r.clamp.b32    t2, %r301, %r301, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 10;
	 xor.b32            %r250, t1, t2;
	}
	// inline asm
	add.s32 	%r387, %r250, %r248;
	add.s32 	%r388, %r387, %r281;
	add.s32 	%r309, %r388, %r245;
	st.local.u32 	[%rd12+188], %r309;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 7;
	 shf.r.clamp.b32    t2, %r253, %r253, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 3;
	 xor.b32            %r252, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r305, %r305, 17;
	 shf.r.clamp.b32    t2, %r305, %r305, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r305, 10;
	 xor.b32            %r254, t1, t2;
	}
	// inline asm
	add.s32 	%r389, %r254, %r252;
	add.s32 	%r390, %r389, %r285;
	add.s32 	%r313, %r390, %r249;
	st.local.u32 	[%rd12+192], %r313;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r257, %r257, 7;
	 shf.r.clamp.b32    t2, %r257, %r257, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r257, 3;
	 xor.b32            %r256, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r309, %r309, 17;
	 shf.r.clamp.b32    t2, %r309, %r309, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r309, 10;
	 xor.b32            %r258, t1, t2;
	}
	// inline asm
	add.s32 	%r391, %r258, %r256;
	add.s32 	%r392, %r391, %r289;
	add.s32 	%r267, %r392, %r253;
	st.local.u32 	[%rd12+196], %r267;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r261, %r261, 7;
	 shf.r.clamp.b32    t2, %r261, %r261, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r261, 3;
	 xor.b32            %r260, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r313, %r313, 17;
	 shf.r.clamp.b32    t2, %r313, %r313, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r313, 10;
	 xor.b32            %r262, t1, t2;
	}
	// inline asm
	add.s32 	%r393, %r262, %r260;
	add.s32 	%r394, %r393, %r293;
	add.s32 	%r271, %r394, %r257;
	st.local.u32 	[%rd12+200], %r271;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 7;
	 shf.r.clamp.b32    t2, %r265, %r265, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 3;
	 xor.b32            %r264, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r267, %r267, 17;
	 shf.r.clamp.b32    t2, %r267, %r267, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r267, 10;
	 xor.b32            %r266, t1, t2;
	}
	// inline asm
	add.s32 	%r395, %r266, %r264;
	add.s32 	%r396, %r395, %r297;
	add.s32 	%r275, %r396, %r261;
	st.local.u32 	[%rd12+204], %r275;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r269, %r269, 7;
	 shf.r.clamp.b32    t2, %r269, %r269, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r269, 3;
	 xor.b32            %r268, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r271, %r271, 17;
	 shf.r.clamp.b32    t2, %r271, %r271, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r271, 10;
	 xor.b32            %r270, t1, t2;
	}
	// inline asm
	add.s32 	%r397, %r270, %r268;
	add.s32 	%r398, %r397, %r301;
	add.s32 	%r279, %r398, %r265;
	st.local.u32 	[%rd12+208], %r279;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r273, %r273, 7;
	 shf.r.clamp.b32    t2, %r273, %r273, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r273, 3;
	 xor.b32            %r272, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r275, %r275, 17;
	 shf.r.clamp.b32    t2, %r275, %r275, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r275, 10;
	 xor.b32            %r274, t1, t2;
	}
	// inline asm
	add.s32 	%r399, %r274, %r272;
	add.s32 	%r400, %r399, %r305;
	add.s32 	%r283, %r400, %r269;
	st.local.u32 	[%rd12+212], %r283;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 7;
	 shf.r.clamp.b32    t2, %r277, %r277, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 3;
	 xor.b32            %r276, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r279, %r279, 17;
	 shf.r.clamp.b32    t2, %r279, %r279, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r279, 10;
	 xor.b32            %r278, t1, t2;
	}
	// inline asm
	add.s32 	%r401, %r278, %r276;
	add.s32 	%r402, %r401, %r309;
	add.s32 	%r287, %r402, %r273;
	st.local.u32 	[%rd12+216], %r287;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r281, %r281, 7;
	 shf.r.clamp.b32    t2, %r281, %r281, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r281, 3;
	 xor.b32            %r280, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r283, %r283, 17;
	 shf.r.clamp.b32    t2, %r283, %r283, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r283, 10;
	 xor.b32            %r282, t1, t2;
	}
	// inline asm
	add.s32 	%r403, %r282, %r280;
	add.s32 	%r404, %r403, %r313;
	add.s32 	%r291, %r404, %r277;
	st.local.u32 	[%rd12+220], %r291;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r285, %r285, 7;
	 shf.r.clamp.b32    t2, %r285, %r285, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r285, 3;
	 xor.b32            %r284, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r287, %r287, 17;
	 shf.r.clamp.b32    t2, %r287, %r287, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r287, 10;
	 xor.b32            %r286, t1, t2;
	}
	// inline asm
	add.s32 	%r405, %r286, %r284;
	add.s32 	%r406, %r405, %r267;
	add.s32 	%r295, %r406, %r281;
	st.local.u32 	[%rd12+224], %r295;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 7;
	 shf.r.clamp.b32    t2, %r289, %r289, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 3;
	 xor.b32            %r288, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r291, %r291, 17;
	 shf.r.clamp.b32    t2, %r291, %r291, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r291, 10;
	 xor.b32            %r290, t1, t2;
	}
	// inline asm
	add.s32 	%r407, %r290, %r288;
	add.s32 	%r408, %r407, %r271;
	add.s32 	%r299, %r408, %r285;
	st.local.u32 	[%rd12+228], %r299;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r293, %r293, 7;
	 shf.r.clamp.b32    t2, %r293, %r293, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r293, 3;
	 xor.b32            %r292, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r295, %r295, 17;
	 shf.r.clamp.b32    t2, %r295, %r295, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r295, 10;
	 xor.b32            %r294, t1, t2;
	}
	// inline asm
	add.s32 	%r409, %r294, %r292;
	add.s32 	%r410, %r409, %r275;
	add.s32 	%r303, %r410, %r289;
	st.local.u32 	[%rd12+232], %r303;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r297, %r297, 7;
	 shf.r.clamp.b32    t2, %r297, %r297, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r297, 3;
	 xor.b32            %r296, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r299, %r299, 17;
	 shf.r.clamp.b32    t2, %r299, %r299, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r299, 10;
	 xor.b32            %r298, t1, t2;
	}
	// inline asm
	add.s32 	%r411, %r298, %r296;
	add.s32 	%r412, %r411, %r279;
	add.s32 	%r307, %r412, %r293;
	st.local.u32 	[%rd12+236], %r307;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 7;
	 shf.r.clamp.b32    t2, %r301, %r301, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 3;
	 xor.b32            %r300, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r303, %r303, 17;
	 shf.r.clamp.b32    t2, %r303, %r303, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r303, 10;
	 xor.b32            %r302, t1, t2;
	}
	// inline asm
	add.s32 	%r413, %r302, %r300;
	add.s32 	%r414, %r413, %r283;
	add.s32 	%r311, %r414, %r297;
	st.local.u32 	[%rd12+240], %r311;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r305, %r305, 7;
	 shf.r.clamp.b32    t2, %r305, %r305, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r305, 3;
	 xor.b32            %r304, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r307, %r307, 17;
	 shf.r.clamp.b32    t2, %r307, %r307, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r307, 10;
	 xor.b32            %r306, t1, t2;
	}
	// inline asm
	add.s32 	%r415, %r306, %r304;
	add.s32 	%r416, %r415, %r287;
	add.s32 	%r315, %r416, %r301;
	st.local.u32 	[%rd12+244], %r315;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r309, %r309, 7;
	 shf.r.clamp.b32    t2, %r309, %r309, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r309, 3;
	 xor.b32            %r308, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r311, %r311, 17;
	 shf.r.clamp.b32    t2, %r311, %r311, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r311, 10;
	 xor.b32            %r310, t1, t2;
	}
	// inline asm
	add.s32 	%r417, %r310, %r308;
	add.s32 	%r418, %r417, %r291;
	add.s32 	%r419, %r418, %r305;
	st.local.u32 	[%rd12+248], %r419;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r313, %r313, 7;
	 shf.r.clamp.b32    t2, %r313, %r313, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r313, 3;
	 xor.b32            %r312, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r315, %r315, 17;
	 shf.r.clamp.b32    t2, %r315, %r315, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r315, 10;
	 xor.b32            %r314, t1, t2;
	}
	// inline asm
	add.s32 	%r420, %r314, %r312;
	add.s32 	%r421, %r420, %r295;
	add.s32 	%r422, %r421, %r309;
	st.local.u32 	[%rd12+252], %r422;
	mov.u32 	%r606, 1359893119;
	mov.u32 	%r605, -1521486534;
	mov.u32 	%r604, 1013904242;
	mov.u32 	%r603, -1150833019;
	mov.u32 	%r602, 1779033703;
	mov.u32 	%r601, -1694144372;
	mov.u32 	%r600, 528734635;
	mov.u32 	%r599, 1541459225;
	bra.uni 	BB0_13;

BB0_14:
	ld.local.u32 	%r597, [%rd23+12];

BB0_13:
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r602, %r602, 2;
	 shf.r.clamp.b32    t2, %r602, %r602, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r602, %r602, 22;
	 xor.b32            %r423, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r606, %r606, 6;
	 shf.r.clamp.b32    t2, %r606, %r606, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r606, %r606, 25;
	 xor.b32            %r425, t1, t2;
	}
	// inline asm
	not.b32 	%r439, %r606;
	and.b32  	%r440, %r600, %r439;
	and.b32  	%r441, %r601, %r606;
	xor.b32  	%r442, %r440, %r441;
	mul.wide.u32 	%rd61, %r598, 4;
	mov.u64 	%rd62, k;
	add.s64 	%rd63, %rd62, %rd61;
	add.s32 	%r443, %r442, %r599;
	add.s32 	%r444, %r443, %r425;
	ld.const.u32 	%r445, [%rd63];
	add.s32 	%r446, %r444, %r445;
	add.s32 	%r447, %r446, %r597;
	add.s32 	%r599, %r447, %r605;
	xor.b32  	%r448, %r603, %r604;
	and.b32  	%r449, %r602, %r448;
	and.b32  	%r450, %r603, %r604;
	xor.b32  	%r451, %r449, %r450;
	add.s32 	%r452, %r423, %r451;
	add.s32 	%r605, %r452, %r447;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r605, %r605, 2;
	 shf.r.clamp.b32    t2, %r605, %r605, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r605, %r605, 22;
	 xor.b32            %r427, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r599, %r599, 6;
	 shf.r.clamp.b32    t2, %r599, %r599, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r599, %r599, 25;
	 xor.b32            %r429, t1, t2;
	}
	// inline asm
	and.b32  	%r453, %r606, %r599;
	not.b32 	%r454, %r599;
	and.b32  	%r455, %r601, %r454;
	xor.b32  	%r456, %r455, %r453;
	add.s32 	%r457, %r598, 1;
	mul.wide.u32 	%rd64, %r457, 4;
	add.s64 	%rd23, %rd12, %rd64;
	add.s32 	%r458, %r456, %r600;
	add.s32 	%r459, %r458, %r429;
	ld.const.u32 	%r460, [%rd63+4];
	add.s32 	%r461, %r459, %r460;
	ld.local.u32 	%r462, [%rd23];
	add.s32 	%r463, %r461, %r462;
	add.s32 	%r600, %r463, %r604;
	xor.b32  	%r464, %r602, %r603;
	and.b32  	%r465, %r605, %r464;
	and.b32  	%r466, %r602, %r603;
	xor.b32  	%r467, %r465, %r466;
	add.s32 	%r468, %r427, %r467;
	add.s32 	%r604, %r468, %r463;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r604, %r604, 2;
	 shf.r.clamp.b32    t2, %r604, %r604, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r604, %r604, 22;
	 xor.b32            %r431, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r600, %r600, 6;
	 shf.r.clamp.b32    t2, %r600, %r600, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r600, %r600, 25;
	 xor.b32            %r433, t1, t2;
	}
	// inline asm
	and.b32  	%r469, %r599, %r600;
	not.b32 	%r470, %r600;
	and.b32  	%r471, %r606, %r470;
	xor.b32  	%r472, %r471, %r469;
	add.s32 	%r473, %r472, %r601;
	add.s32 	%r474, %r473, %r433;
	ld.const.u32 	%r475, [%rd63+8];
	add.s32 	%r476, %r474, %r475;
	ld.local.v2.u32 	{%r477, %r478}, [%rd23+4];
	add.s32 	%r480, %r476, %r477;
	add.s32 	%r601, %r480, %r603;
	xor.b32  	%r481, %r605, %r602;
	and.b32  	%r482, %r604, %r481;
	and.b32  	%r483, %r605, %r602;
	xor.b32  	%r484, %r482, %r483;
	add.s32 	%r485, %r431, %r484;
	add.s32 	%r603, %r485, %r480;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r603, %r603, 2;
	 shf.r.clamp.b32    t2, %r603, %r603, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r603, %r603, 22;
	 xor.b32            %r435, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r601, %r601, 6;
	 shf.r.clamp.b32    t2, %r601, %r601, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r601, %r601, 25;
	 xor.b32            %r437, t1, t2;
	}
	// inline asm
	and.b32  	%r486, %r600, %r601;
	not.b32 	%r487, %r601;
	and.b32  	%r488, %r599, %r487;
	xor.b32  	%r489, %r488, %r486;
	add.s32 	%r490, %r489, %r606;
	add.s32 	%r491, %r490, %r437;
	ld.const.u32 	%r492, [%rd63+12];
	add.s32 	%r493, %r491, %r492;
	add.s32 	%r495, %r493, %r478;
	add.s32 	%r606, %r495, %r602;
	xor.b32  	%r496, %r604, %r605;
	and.b32  	%r497, %r603, %r496;
	and.b32  	%r498, %r604, %r605;
	xor.b32  	%r499, %r497, %r498;
	add.s32 	%r500, %r435, %r499;
	add.s32 	%r602, %r500, %r495;
	add.s32 	%r598, %r598, 4;
	setp.eq.s32	%p8, %r598, 64;
	@%p8 bra 	BB0_15;
	bra.uni 	BB0_14;

BB0_15:
	add.s32 	%r32, %r602, 1779033703;
	st.local.u32 	[%rd4+64], %r32;
	add.s32 	%r503, %r603, -1150833019;
	st.local.u32 	[%rd4+68], %r503;
	add.s32 	%r504, %r604, 1013904242;
	st.local.u32 	[%rd4+72], %r504;
	add.s32 	%r505, %r605, -1521486534;
	st.local.u32 	[%rd4+76], %r505;
	add.s32 	%r506, %r606, 1359893119;
	st.local.u32 	[%rd4+80], %r506;
	add.s32 	%r507, %r601, -1694144372;
	st.local.u32 	[%rd4+84], %r507;
	add.s32 	%r508, %r600, 528734635;
	st.local.u32 	[%rd4+88], %r508;
	add.s32 	%r509, %r599, 1541459225;
	st.local.u32 	[%rd4+92], %r509;
	mov.u32 	%r33, 0;
	mov.u32 	%r609, %r33;
	bra.uni 	BB0_16;

BB0_59:
	mul.wide.s32 	%rd69, %r609, 4;
	add.s64 	%rd70, %rd4, %rd69;
	ld.local.u32 	%r32, [%rd70+64];

BB0_16:
	mov.pred 	%p78, 0;
	mov.u32 	%r610, 1;
	setp.lt.s32	%p10, %r32, 0;
	@%p10 bra 	BB0_17;

	add.s32 	%r611, %r33, 1;
	and.b32  	%r512, %r32, 1073741824;
	mov.u32 	%r610, 2;
	setp.ne.s32	%p12, %r512, 0;
	@%p12 bra 	BB0_18;

	add.s32 	%r611, %r33, 2;
	and.b32  	%r514, %r32, 536870912;
	mov.u32 	%r610, 3;
	setp.ne.s32	%p14, %r514, 0;
	@%p14 bra 	BB0_18;

	add.s32 	%r611, %r33, 3;
	and.b32  	%r516, %r32, 268435456;
	mov.u32 	%r610, 4;
	setp.ne.s32	%p16, %r516, 0;
	@%p16 bra 	BB0_18;

	add.s32 	%r611, %r33, 4;
	and.b32  	%r518, %r32, 134217728;
	mov.u32 	%r610, 5;
	setp.ne.s32	%p18, %r518, 0;
	@%p18 bra 	BB0_18;

	add.s32 	%r611, %r33, 5;
	and.b32  	%r520, %r32, 67108864;
	mov.u32 	%r610, 6;
	setp.ne.s32	%p20, %r520, 0;
	@%p20 bra 	BB0_18;

	add.s32 	%r611, %r33, 6;
	and.b32  	%r522, %r32, 33554432;
	mov.u32 	%r610, 7;
	setp.ne.s32	%p22, %r522, 0;
	@%p22 bra 	BB0_18;

	add.s32 	%r611, %r33, 7;
	and.b32  	%r524, %r32, 16777216;
	mov.u32 	%r610, 8;
	setp.ne.s32	%p24, %r524, 0;
	@%p24 bra 	BB0_18;

	add.s32 	%r611, %r33, 8;
	and.b32  	%r526, %r32, 8388608;
	mov.u32 	%r610, 9;
	setp.ne.s32	%p26, %r526, 0;
	@%p26 bra 	BB0_18;

	add.s32 	%r611, %r33, 9;
	and.b32  	%r528, %r32, 4194304;
	mov.u32 	%r610, 10;
	setp.ne.s32	%p28, %r528, 0;
	@%p28 bra 	BB0_18;

	add.s32 	%r611, %r33, 10;
	and.b32  	%r530, %r32, 2097152;
	mov.u32 	%r610, 11;
	setp.ne.s32	%p30, %r530, 0;
	@%p30 bra 	BB0_18;

	add.s32 	%r611, %r33, 11;
	and.b32  	%r532, %r32, 1048576;
	mov.u32 	%r610, 12;
	setp.ne.s32	%p32, %r532, 0;
	@%p32 bra 	BB0_18;

	add.s32 	%r611, %r33, 12;
	and.b32  	%r534, %r32, 524288;
	mov.u32 	%r610, 13;
	setp.ne.s32	%p34, %r534, 0;
	@%p34 bra 	BB0_18;

	add.s32 	%r611, %r33, 13;
	and.b32  	%r536, %r32, 262144;
	mov.u32 	%r610, 14;
	setp.ne.s32	%p36, %r536, 0;
	@%p36 bra 	BB0_18;

	add.s32 	%r611, %r33, 14;
	and.b32  	%r538, %r32, 131072;
	mov.u32 	%r610, 15;
	setp.ne.s32	%p38, %r538, 0;
	@%p38 bra 	BB0_18;

	add.s32 	%r611, %r33, 15;
	and.b32  	%r540, %r32, 65536;
	mov.u32 	%r610, 16;
	setp.ne.s32	%p40, %r540, 0;
	@%p40 bra 	BB0_18;

	add.s32 	%r611, %r33, 16;
	and.b32  	%r542, %r32, 32768;
	mov.u32 	%r610, 17;
	setp.ne.s32	%p42, %r542, 0;
	@%p42 bra 	BB0_18;

	add.s32 	%r611, %r33, 17;
	and.b32  	%r544, %r32, 16384;
	mov.u32 	%r610, 18;
	setp.ne.s32	%p44, %r544, 0;
	@%p44 bra 	BB0_18;

	add.s32 	%r611, %r33, 18;
	and.b32  	%r546, %r32, 8192;
	mov.u32 	%r610, 19;
	setp.ne.s32	%p46, %r546, 0;
	@%p46 bra 	BB0_18;

	add.s32 	%r611, %r33, 19;
	and.b32  	%r548, %r32, 4096;
	mov.u32 	%r610, 20;
	setp.ne.s32	%p48, %r548, 0;
	@%p48 bra 	BB0_18;

	add.s32 	%r611, %r33, 20;
	and.b32  	%r550, %r32, 2048;
	mov.u32 	%r610, 21;
	setp.ne.s32	%p50, %r550, 0;
	@%p50 bra 	BB0_18;

	add.s32 	%r611, %r33, 21;
	and.b32  	%r552, %r32, 1024;
	mov.u32 	%r610, 22;
	setp.ne.s32	%p52, %r552, 0;
	@%p52 bra 	BB0_18;

	add.s32 	%r611, %r33, 22;
	and.b32  	%r554, %r32, 512;
	mov.u32 	%r610, 23;
	setp.ne.s32	%p54, %r554, 0;
	@%p54 bra 	BB0_18;

	add.s32 	%r611, %r33, 23;
	and.b32  	%r556, %r32, 256;
	mov.u32 	%r610, 24;
	setp.ne.s32	%p56, %r556, 0;
	@%p56 bra 	BB0_18;

	add.s32 	%r611, %r33, 24;
	and.b32  	%r558, %r32, 128;
	mov.u32 	%r610, 25;
	setp.ne.s32	%p58, %r558, 0;
	@%p58 bra 	BB0_18;

	add.s32 	%r611, %r33, 25;
	and.b32  	%r560, %r32, 64;
	mov.u32 	%r610, 26;
	setp.ne.s32	%p60, %r560, 0;
	@%p60 bra 	BB0_18;

	add.s32 	%r611, %r33, 26;
	and.b32  	%r562, %r32, 32;
	mov.u32 	%r610, 27;
	setp.ne.s32	%p62, %r562, 0;
	@%p62 bra 	BB0_18;

	add.s32 	%r611, %r33, 27;
	and.b32  	%r564, %r32, 16;
	mov.u32 	%r610, 28;
	setp.ne.s32	%p64, %r564, 0;
	@%p64 bra 	BB0_18;

	add.s32 	%r611, %r33, 28;
	and.b32  	%r566, %r32, 8;
	mov.u32 	%r610, 29;
	setp.ne.s32	%p66, %r566, 0;
	@%p66 bra 	BB0_18;

	add.s32 	%r611, %r33, 29;
	and.b32  	%r568, %r32, 4;
	mov.u32 	%r610, 30;
	setp.ne.s32	%p68, %r568, 0;
	@%p68 bra 	BB0_18;

	add.s32 	%r611, %r33, 30;
	and.b32  	%r570, %r32, 2;
	mov.u32 	%r610, 31;
	setp.ne.s32	%p70, %r570, 0;
	@%p70 bra 	BB0_18;

	add.s32 	%r611, %r33, 31;
	and.b32  	%r572, %r32, 1;
	setp.eq.b32	%p72, %r572, 1;
	mov.pred 	%p78, -1;
	mov.u32 	%r610, 32;
	@!%p72 bra 	BB0_53;
	bra.uni 	BB0_18;

BB0_53:
	add.s32 	%r33, %r33, 32;
	add.s32 	%r609, %r609, 1;
	setp.lt.s32	%p74, %r609, 8;
	@%p74 bra 	BB0_59;
	bra.uni 	BB0_54;

BB0_17:
	mov.u32 	%r611, %r33;

BB0_18:
	selp.u32	%r573, 1, 0, %p78;
	add.s32 	%r68, %r573, %r609;
	selp.b32	%r69, 0, %r610, %p78;
	mov.u32 	%r574, 24;
	sub.s32 	%r70, %r574, %r69;
	setp.gt.s32	%p73, %r70, -1;
	mul.wide.s32 	%rd65, %r68, 4;
	add.s64 	%rd66, %rd4, %rd65;
	ld.local.u32 	%r71, [%rd66+64];
	@%p73 bra 	BB0_51;
	bra.uni 	BB0_19;

BB0_51:
	shr.u32 	%r583, %r71, %r70;
	and.b32  	%r612, %r583, 255;
	bra.uni 	BB0_52;

BB0_19:
	add.s32 	%r575, %r69, -24;
	shl.b32 	%r576, %r71, %r575;
	add.s32 	%r577, %r68, 1;
	mul.wide.s32 	%rd67, %r577, 4;
	add.s64 	%rd68, %rd4, %rd67;
	ld.local.u32 	%r578, [%rd68+64];
	shr.u32 	%r579, %r578, 1;
	mov.u32 	%r580, 55;
	sub.s32 	%r581, %r580, %r69;
	shr.u32 	%r582, %r579, %r581;
	add.s32 	%r612, %r582, %r576;

BB0_52:
	shl.b32 	%r584, %r611, 8;
	add.s32 	%r33, %r612, %r584;

BB0_54:
	add.s64 	%rd82, %rd21, 1;
	ld.param.u32 	%r588, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	setp.le.u32	%p75, %r33, %r588;
	@%p75 bra 	BB0_5;

	mov.u16 	%rs5, 1;
	st.volatile.global.u8 	[%rd10], %rs5;
	st.volatile.global.u8 	[%rd2], %rs5;
	mov.u32 	%r614, 0;
	mov.u64 	%rd83, %rd4;
	mov.u64 	%rd84, %rd11;

BB0_56:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs6, %rs7, %rs8, %rs9}, [%rd83+32];
	st.global.u8 	[%rd84], %rs9;
	st.global.u8 	[%rd84+1], %rs8;
	st.global.u8 	[%rd84+2], %rs7;
	st.global.u8 	[%rd84+3], %rs6;
	add.s64 	%rd84, %rd84, 4;
	add.s64 	%rd83, %rd83, 4;
	add.s32 	%r614, %r614, 4;
	setp.lt.s32	%p76, %r614, 20;
	@%p76 bra 	BB0_56;

	ld.local.v2.u8 	{%rs14, %rs15}, [%rd4+54];
	ld.local.u8 	%rs18, [%rd4+53];
	st.global.u8 	[%rd11+20], %rs15;
	st.global.u8 	[%rd11+21], %rs14;
	st.global.u8 	[%rd11+22], %rs18;
	mov.u32 	%r615, 0;
	mov.u64 	%rd85, %rd4;
	mov.u64 	%rd86, %rd3;

BB0_58:
	.pragma "nounroll";
	add.s64 	%rd82, %rd21, 1;
	ld.local.v4.u8 	{%rs19, %rs20, %rs21, %rs22}, [%rd85+64];
	st.global.u8 	[%rd86], %rs22;
	st.global.u8 	[%rd86+1], %rs21;
	st.global.u8 	[%rd86+2], %rs20;
	st.global.u8 	[%rd86+3], %rs19;
	add.s64 	%rd86, %rd86, 4;
	add.s64 	%rd85, %rd85, 4;
	add.s32 	%r615, %r615, 4;
	setp.lt.s32	%p77, %r615, 32;
	@%p77 bra 	BB0_58;

BB0_5:
	mov.u64 	%rd21, %rd82;
	mul.hi.s64 	%rd44, %rd21, -6640827866535438581;
	add.s64 	%rd45, %rd44, %rd21;
	shr.u64 	%rd46, %rd45, 63;
	shr.s64 	%rd47, %rd45, 6;
	add.s64 	%rd48, %rd47, %rd46;
	mul.lo.s64 	%rd49, %rd48, 100;
	sub.s64 	%rd50, %rd21, %rd49;
	setp.ne.s64	%p4, %rd50, 99;
	@%p4 bra 	BB0_10;

	ld.volatile.global.u8 	%rs4, [%rd2];
	setp.eq.s16	%p5, %rs4, 0;
	@%p5 bra 	BB0_10;

	mov.u32 	%r593, %tid.x;
	mov.u32 	%r592, %ctaid.x;
	mov.u32 	%r591, %ntid.x;
	mad.lo.s32 	%r590, %r591, %r592, %r593;
	ld.param.u64 	%rd73, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd72, %rd73;
	cvt.u64.u32	%rd71, %r590;
	shl.b64 	%rd51, %rd71, 3;
	add.s64 	%rd52, %rd72, %rd51;
	st.global.u64 	[%rd52], %rd21;
	setp.ne.s32	%p6, %r590, 1;
	@%p6 bra 	BB0_9;

	ld.param.u64 	%rd75, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	cvta.to.global.u64 	%rd74, %rd75;
	st.volatile.global.u64 	[%rd74], %rd21;

BB0_9:
	ret;
}


