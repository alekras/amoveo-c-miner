//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};
.const .align 4 .u32 a0 = 1779033703;
.const .align 4 .u32 b0 = -1150833019;
.const .align 4 .u32 c0 = 1013904242;
.const .align 4 .u32 d0 = -1521486534;
.const .align 4 .u32 e0 = 1359893119;
.const .align 4 .u32 f0 = -1694144372;
.const .align 4 .u32 g0 = 528734635;
.const .align 4 .u32 h0 = 1541459225;

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 16 .b8 	__local_depot0[400];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<28>;
	.reg .b32 	%r<678>;
	.reg .b64 	%rd<90>;


	mov.u64 	%rd89, __local_depot0;
	cvta.local.u64 	%SP, %rd89;
	ld.param.u64 	%rd43, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0];
	ld.param.u64 	%rd40, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2];
	ld.param.u64 	%rd41, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3];
	ld.param.u64 	%rd44, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4];
	ld.param.u32 	%r134, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6];
	cvta.to.global.u64 	%rd2, %rd44;
	cvta.to.global.u64 	%rd3, %rd43;
	add.u64 	%rd46, %SP, 0;
	cvta.to.local.u64 	%rd4, %rd46;
	mov.u32 	%r136, %ntid.x;
	mov.u32 	%r137, %ctaid.x;
	mov.u32 	%r138, %tid.x;
	mad.lo.s32 	%r1, %r136, %r137, %r138;
	mov.u32 	%r649, -8;
	mov.u64 	%rd78, %rd3;
	mov.u64 	%rd79, %rd4;

BB0_1:
	.pragma "nounroll";
	ld.global.u8 	%r139, [%rd78];
	shl.b32 	%r140, %r139, 24;
	ld.global.u8 	%r141, [%rd78+1];
	shl.b32 	%r142, %r141, 16;
	or.b32  	%r143, %r142, %r140;
	ld.global.u8 	%rs3, [%rd78+2];
	mul.wide.u16 	%r144, %rs3, 256;
	or.b32  	%r145, %r143, %r144;
	ld.global.u8 	%r146, [%rd78+3];
	or.b32  	%r147, %r145, %r146;
	st.local.u32 	[%rd79], %r147;
	add.s64 	%rd79, %rd79, 4;
	add.s64 	%rd78, %rd78, 4;
	add.s32 	%r649, %r649, 1;
	setp.ne.s32	%p1, %r649, 0;
	@%p1 bra 	BB0_1;

	cvta.to.global.u64 	%rd10, %rd41;
	cvta.to.global.u64 	%rd11, %rd40;
	add.u64 	%rd48, %SP, 112;
	cvta.to.local.u64 	%rd12, %rd48;
	mov.u64 	%rd81, -8;
	mov.u32 	%r650, -5;
	mov.u64 	%rd80, %rd11;

BB0_3:
	.pragma "nounroll";
	ld.global.u8 	%r149, [%rd80];
	shl.b32 	%r150, %r149, 24;
	ld.global.u8 	%r151, [%rd80+1];
	shl.b32 	%r152, %r151, 16;
	or.b32  	%r153, %r152, %r150;
	ld.global.u8 	%rs4, [%rd80+2];
	mul.wide.u16 	%r154, %rs4, 256;
	or.b32  	%r155, %r153, %r154;
	ld.global.u8 	%r156, [%rd80+3];
	or.b32  	%r157, %r155, %r156;
	shl.b64 	%rd50, %rd81, 2;
	sub.s64 	%rd51, %rd4, %rd50;
	st.local.u32 	[%rd51], %r157;
	add.s64 	%rd81, %rd81, -1;
	add.s64 	%rd80, %rd80, 4;
	add.s32 	%r650, %r650, 1;
	setp.ne.s32	%p2, %r650, 0;
	@%p2 bra 	BB0_3;

	ld.global.u8 	%r160, [%rd11+20];
	shl.b32 	%r161, %r160, 24;
	ld.global.u8 	%r162, [%rd11+21];
	shl.b32 	%r163, %r162, 16;
	ld.global.u8 	%rs5, [%rd11+22];
	mul.wide.u16 	%r164, %rs5, 256;
	or.b32  	%r165, %r161, %r163;
	or.b32  	%r166, %r165, %r164;
	or.b32  	%r167, %r166, 128;
	st.local.u32 	[%rd4+52], %r167;
	mov.u32 	%r168, 0;
	st.local.u32 	[%rd4+56], %r168;
	mov.u32 	%r169, 440;
	st.local.u32 	[%rd4+60], %r169;
	ld.local.u32 	%r170, [%rd4+40];
	xor.b32  	%r171, %r170, %r1;
	ld.local.u32 	%r172, [%rd4+44];
	ld.local.u32 	%r652, [%rd4];
	st.local.u32 	[%rd4+40], %r171;
	xor.b32  	%r173, %r172, %r134;
	st.local.u32 	[%rd4+44], %r173;
	mov.u32 	%r651, -5;
	mov.u64 	%rd82, %rd4;

BB0_5:
	.pragma "nounroll";
	add.s64 	%rd22, %rd82, 4;
	ld.local.u32 	%r10, [%rd82+4];
	ld.local.u32 	%r176, [%rd82+56];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r10, %r10, 7;
	 shf.r.clamp.b32    t2, %r10, %r10, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r10, 3;
	 xor.b32            %r653, t1, t2;
	 shf.r.clamp.b32    t1, %r176, %r176, 17;
	 shf.r.clamp.b32    t2, %r176, %r176, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r176, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r653, %r653, t1;
	}
	// inline asm
	ld.local.u32 	%r178, [%rd82+36];
	add.s32 	%r179, %r178, %r653;
	add.s32 	%r180, %r179, %r652;
	st.local.u32 	[%rd82+64], %r180;
	add.s32 	%r651, %r651, 1;
	setp.ne.s32	%p3, %r651, 0;
	mov.u64 	%rd82, %rd22;
	mov.u32 	%r652, %r10;
	@%p3 bra 	BB0_5;

	ld.local.u32 	%r655, [%rd4+20];
	mov.u32 	%r654, -6;
	mov.u64 	%rd83, %rd4;

BB0_7:
	.pragma "nounroll";
	ld.local.u32 	%r16, [%rd83+24];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r16, %r16, 7;
	 shf.r.clamp.b32    t2, %r16, %r16, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r16, 3;
	 xor.b32            %r656, t1, t2;
	}
	// inline asm
	ld.local.u32 	%r184, [%rd83+56];
	add.s32 	%r185, %r184, %r656;
	add.s32 	%r186, %r185, %r655;
	st.local.u32 	[%rd83+84], %r186;
	add.s64 	%rd83, %rd83, 4;
	add.s32 	%r654, %r654, 1;
	setp.ne.s32	%p4, %r654, 0;
	mov.u32 	%r655, %r16;
	@%p4 bra 	BB0_7;

	mov.u64 	%rd84, 0;
	bra.uni 	BB0_9;

BB0_14:
	ld.local.u32 	%r187, [%rd4+48];
	add.s32 	%r20, %r187, 1;
	ld.local.u32 	%r658, [%rd4+52];
	st.local.u32 	[%rd4+48], %r20;
	setp.eq.s32	%p8, %r20, 0;
	@%p8 bra 	BB0_16;
	bra.uni 	BB0_15;

BB0_16:
	add.s32 	%r658, %r658, 16;
	st.local.u32 	[%rd4+52], %r658;
	ld.local.u32 	%r188, [%rd4+80];
	add.s32 	%r657, %r188, 16;
	st.local.u32 	[%rd4+80], %r657;
	bra.uni 	BB0_17;

BB0_15:
	ld.local.u32 	%r657, [%rd4+80];

BB0_17:
	mov.u32 	%r661, 0;
	ld.local.u32 	%r358, [%rd4+76];
	ld.local.u32 	%r359, [%rd4+104];
	ld.local.u32 	%r360, [%rd4+12];
	ld.local.u32 	%r361, [%rd4+8];
	ld.local.u32 	%r362, [%rd4+4];
	ld.local.u32 	%r659, [%rd4];
	ld.local.u32 	%r363, [%rd4+28];
	ld.local.u32 	%r364, [%rd4+24];
	ld.local.u32 	%r365, [%rd4+20];
	ld.local.u32 	%r366, [%rd4+16];
	ld.local.u32 	%r367, [%rd4+44];
	ld.local.u32 	%r368, [%rd4+40];
	ld.local.u32 	%r369, [%rd4+36];
	ld.local.u32 	%r370, [%rd4+32];
	ld.local.u32 	%r214, [%rd4+60];
	ld.local.u32 	%r210, [%rd4+56];
	ld.local.u32 	%r226, [%rd4+72];
	ld.local.u32 	%r222, [%rd4+68];
	ld.local.u32 	%r218, [%rd4+64];
	add.s32 	%r230, %r358, 1;
	st.local.u32 	[%rd4+76], %r230;
	add.s32 	%r371, %r359, 1;
	st.local.u32 	[%rd4+104], %r371;
	st.local.v4.u32 	[%rd12], {%r659, %r362, %r361, %r360};
	st.local.v4.u32 	[%rd12+16], {%r366, %r365, %r364, %r363};
	st.local.v4.u32 	[%rd12+32], {%r370, %r369, %r368, %r367};
	add.s32 	%r640, %r187, 1;
	st.local.v4.u32 	[%rd12+48], {%r640, %r658, %r210, %r214};
	st.local.v4.u32 	[%rd12+64], {%r218, %r222, %r226, %r230};
	st.local.u32 	[%rd12+80], %r657;
	ld.local.u32 	%r372, [%rd4+84];
	ld.local.u32 	%r373, [%rd4+88];
	ld.local.u32 	%r374, [%rd4+92];
	ld.local.u32 	%r375, [%rd4+96];
	ld.local.u32 	%r376, [%rd4+100];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r230, %r230, 17;
	 shf.r.clamp.b32    t2, %r230, %r230, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r230, 10;
	 xor.b32            %r189, t1, t2;
	}
	// inline asm
	add.s32 	%r238, %r372, %r189;
	st.local.u32 	[%rd12+84], %r238;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r657, %r657, 17;
	 shf.r.clamp.b32    t2, %r657, %r657, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r657, 10;
	 xor.b32            %r191, t1, t2;
	}
	// inline asm
	add.s32 	%r242, %r373, %r191;
	st.local.u32 	[%rd12+88], %r242;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r238, %r238, 17;
	 shf.r.clamp.b32    t2, %r238, %r238, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r238, 10;
	 xor.b32            %r193, t1, t2;
	}
	// inline asm
	add.s32 	%r246, %r374, %r193;
	st.local.u32 	[%rd12+92], %r246;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r242, %r242, 17;
	 shf.r.clamp.b32    t2, %r242, %r242, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r242, 10;
	 xor.b32            %r195, t1, t2;
	}
	// inline asm
	add.s32 	%r250, %r375, %r195;
	st.local.u32 	[%rd12+96], %r250;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r246, %r246, 17;
	 shf.r.clamp.b32    t2, %r246, %r246, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r246, 10;
	 xor.b32            %r197, t1, t2;
	}
	// inline asm
	add.s32 	%r254, %r376, %r197;
	st.local.u32 	[%rd12+100], %r254;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r250, %r250, 17;
	 shf.r.clamp.b32    t2, %r250, %r250, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r250, 10;
	 xor.b32            %r199, t1, t2;
	}
	// inline asm
	add.s32 	%r258, %r371, %r199;
	st.local.u32 	[%rd12+104], %r258;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r20, %r20, 7;
	 shf.r.clamp.b32    t2, %r20, %r20, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r20, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r254, %r254, 17;
	 shf.r.clamp.b32    t2, %r254, %r254, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r254, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r377, %r657, %r656;
	add.s32 	%r262, %r377, %r367;
	st.local.u32 	[%rd12+108], %r262;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r658, %r658, 7;
	 shf.r.clamp.b32    t2, %r658, %r658, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r658, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r258, %r258, 17;
	 shf.r.clamp.b32    t2, %r258, %r258, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r258, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r378, %r238, %r656;
	add.s32 	%r266, %r378, %r20;
	st.local.u32 	[%rd12+112], %r266;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r210, %r210, 7;
	 shf.r.clamp.b32    t2, %r210, %r210, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r210, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r262, %r262, 17;
	 shf.r.clamp.b32    t2, %r262, %r262, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r262, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r379, %r242, %r656;
	add.s32 	%r270, %r379, %r658;
	st.local.u32 	[%rd12+116], %r270;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r214, %r214, 7;
	 shf.r.clamp.b32    t2, %r214, %r214, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r214, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r266, %r266, 17;
	 shf.r.clamp.b32    t2, %r266, %r266, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r266, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r380, %r246, %r656;
	add.s32 	%r274, %r380, %r210;
	st.local.u32 	[%rd12+120], %r274;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r218, %r218, 7;
	 shf.r.clamp.b32    t2, %r218, %r218, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r218, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r270, %r270, 17;
	 shf.r.clamp.b32    t2, %r270, %r270, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r270, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r381, %r250, %r656;
	add.s32 	%r278, %r381, %r214;
	st.local.u32 	[%rd12+124], %r278;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r222, %r222, 7;
	 shf.r.clamp.b32    t2, %r222, %r222, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r222, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r274, %r274, 17;
	 shf.r.clamp.b32    t2, %r274, %r274, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r274, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r382, %r254, %r656;
	add.s32 	%r282, %r382, %r218;
	st.local.u32 	[%rd12+128], %r282;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r226, %r226, 7;
	 shf.r.clamp.b32    t2, %r226, %r226, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r226, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r278, %r278, 17;
	 shf.r.clamp.b32    t2, %r278, %r278, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r278, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r383, %r258, %r656;
	add.s32 	%r286, %r383, %r222;
	st.local.u32 	[%rd12+132], %r286;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r230, %r230, 7;
	 shf.r.clamp.b32    t2, %r230, %r230, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r230, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r282, %r282, 17;
	 shf.r.clamp.b32    t2, %r282, %r282, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r282, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r384, %r262, %r656;
	add.s32 	%r290, %r384, %r226;
	st.local.u32 	[%rd12+136], %r290;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r657, %r657, 7;
	 shf.r.clamp.b32    t2, %r657, %r657, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r657, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r286, %r286, 17;
	 shf.r.clamp.b32    t2, %r286, %r286, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r286, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r385, %r266, %r656;
	add.s32 	%r294, %r385, %r230;
	st.local.u32 	[%rd12+140], %r294;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r238, %r238, 7;
	 shf.r.clamp.b32    t2, %r238, %r238, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r238, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r290, %r290, 17;
	 shf.r.clamp.b32    t2, %r290, %r290, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r290, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r386, %r270, %r656;
	add.s32 	%r298, %r386, %r657;
	st.local.u32 	[%rd12+144], %r298;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r242, %r242, 7;
	 shf.r.clamp.b32    t2, %r242, %r242, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r242, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r294, %r294, 17;
	 shf.r.clamp.b32    t2, %r294, %r294, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r294, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r387, %r274, %r656;
	add.s32 	%r302, %r387, %r238;
	st.local.u32 	[%rd12+148], %r302;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r246, %r246, 7;
	 shf.r.clamp.b32    t2, %r246, %r246, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r246, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r298, %r298, 17;
	 shf.r.clamp.b32    t2, %r298, %r298, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r298, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r388, %r278, %r656;
	add.s32 	%r306, %r388, %r242;
	st.local.u32 	[%rd12+152], %r306;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r250, %r250, 7;
	 shf.r.clamp.b32    t2, %r250, %r250, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r250, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r302, %r302, 17;
	 shf.r.clamp.b32    t2, %r302, %r302, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r302, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r389, %r282, %r656;
	add.s32 	%r310, %r389, %r246;
	st.local.u32 	[%rd12+156], %r310;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r254, %r254, 7;
	 shf.r.clamp.b32    t2, %r254, %r254, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r254, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r306, %r306, 17;
	 shf.r.clamp.b32    t2, %r306, %r306, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r306, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r390, %r286, %r656;
	add.s32 	%r314, %r390, %r250;
	st.local.u32 	[%rd12+160], %r314;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r258, %r258, 7;
	 shf.r.clamp.b32    t2, %r258, %r258, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r258, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r310, %r310, 17;
	 shf.r.clamp.b32    t2, %r310, %r310, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r310, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r391, %r290, %r656;
	add.s32 	%r318, %r391, %r254;
	st.local.u32 	[%rd12+164], %r318;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r262, %r262, 7;
	 shf.r.clamp.b32    t2, %r262, %r262, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r262, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r314, %r314, 17;
	 shf.r.clamp.b32    t2, %r314, %r314, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r314, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r392, %r294, %r656;
	add.s32 	%r322, %r392, %r258;
	st.local.u32 	[%rd12+168], %r322;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r266, %r266, 7;
	 shf.r.clamp.b32    t2, %r266, %r266, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r266, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r318, %r318, 17;
	 shf.r.clamp.b32    t2, %r318, %r318, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r318, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r393, %r298, %r656;
	add.s32 	%r326, %r393, %r262;
	st.local.u32 	[%rd12+172], %r326;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r270, %r270, 7;
	 shf.r.clamp.b32    t2, %r270, %r270, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r270, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r322, %r322, 17;
	 shf.r.clamp.b32    t2, %r322, %r322, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r322, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r394, %r302, %r656;
	add.s32 	%r330, %r394, %r266;
	st.local.u32 	[%rd12+176], %r330;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r274, %r274, 7;
	 shf.r.clamp.b32    t2, %r274, %r274, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r274, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r326, %r326, 17;
	 shf.r.clamp.b32    t2, %r326, %r326, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r326, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r395, %r306, %r656;
	add.s32 	%r334, %r395, %r270;
	st.local.u32 	[%rd12+180], %r334;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r278, %r278, 7;
	 shf.r.clamp.b32    t2, %r278, %r278, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r278, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r330, %r330, 17;
	 shf.r.clamp.b32    t2, %r330, %r330, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r330, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r396, %r310, %r656;
	add.s32 	%r338, %r396, %r274;
	st.local.u32 	[%rd12+184], %r338;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r282, %r282, 7;
	 shf.r.clamp.b32    t2, %r282, %r282, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r282, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r334, %r334, 17;
	 shf.r.clamp.b32    t2, %r334, %r334, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r334, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r397, %r314, %r656;
	add.s32 	%r342, %r397, %r278;
	st.local.u32 	[%rd12+188], %r342;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r286, %r286, 7;
	 shf.r.clamp.b32    t2, %r286, %r286, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r286, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r338, %r338, 17;
	 shf.r.clamp.b32    t2, %r338, %r338, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r338, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r398, %r318, %r656;
	add.s32 	%r346, %r398, %r282;
	st.local.u32 	[%rd12+192], %r346;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r290, %r290, 7;
	 shf.r.clamp.b32    t2, %r290, %r290, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r290, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r342, %r342, 17;
	 shf.r.clamp.b32    t2, %r342, %r342, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r342, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r399, %r322, %r656;
	add.s32 	%r299, %r399, %r286;
	st.local.u32 	[%rd12+196], %r299;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r294, %r294, 7;
	 shf.r.clamp.b32    t2, %r294, %r294, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r294, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r346, %r346, 17;
	 shf.r.clamp.b32    t2, %r346, %r346, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r346, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r400, %r326, %r656;
	add.s32 	%r303, %r400, %r290;
	st.local.u32 	[%rd12+200], %r303;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r298, %r298, 7;
	 shf.r.clamp.b32    t2, %r298, %r298, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r298, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r299, %r299, 17;
	 shf.r.clamp.b32    t2, %r299, %r299, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r299, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r401, %r330, %r656;
	add.s32 	%r307, %r401, %r294;
	st.local.u32 	[%rd12+204], %r307;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r302, %r302, 7;
	 shf.r.clamp.b32    t2, %r302, %r302, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r302, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r303, %r303, 17;
	 shf.r.clamp.b32    t2, %r303, %r303, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r303, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r402, %r334, %r656;
	add.s32 	%r311, %r402, %r298;
	st.local.u32 	[%rd12+208], %r311;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r306, %r306, 7;
	 shf.r.clamp.b32    t2, %r306, %r306, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r306, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r307, %r307, 17;
	 shf.r.clamp.b32    t2, %r307, %r307, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r307, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r403, %r338, %r656;
	add.s32 	%r315, %r403, %r302;
	st.local.u32 	[%rd12+212], %r315;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r310, %r310, 7;
	 shf.r.clamp.b32    t2, %r310, %r310, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r310, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r311, %r311, 17;
	 shf.r.clamp.b32    t2, %r311, %r311, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r311, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r404, %r342, %r656;
	add.s32 	%r319, %r404, %r306;
	st.local.u32 	[%rd12+216], %r319;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r314, %r314, 7;
	 shf.r.clamp.b32    t2, %r314, %r314, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r314, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r315, %r315, 17;
	 shf.r.clamp.b32    t2, %r315, %r315, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r315, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r405, %r346, %r656;
	add.s32 	%r323, %r405, %r310;
	st.local.u32 	[%rd12+220], %r323;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r318, %r318, 7;
	 shf.r.clamp.b32    t2, %r318, %r318, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r318, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r319, %r319, 17;
	 shf.r.clamp.b32    t2, %r319, %r319, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r319, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r406, %r299, %r656;
	add.s32 	%r327, %r406, %r314;
	st.local.u32 	[%rd12+224], %r327;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r322, %r322, 7;
	 shf.r.clamp.b32    t2, %r322, %r322, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r322, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r323, %r323, 17;
	 shf.r.clamp.b32    t2, %r323, %r323, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r323, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r407, %r303, %r656;
	add.s32 	%r331, %r407, %r318;
	st.local.u32 	[%rd12+228], %r331;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r326, %r326, 7;
	 shf.r.clamp.b32    t2, %r326, %r326, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r326, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r327, %r327, 17;
	 shf.r.clamp.b32    t2, %r327, %r327, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r327, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r408, %r307, %r656;
	add.s32 	%r335, %r408, %r322;
	st.local.u32 	[%rd12+232], %r335;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r330, %r330, 7;
	 shf.r.clamp.b32    t2, %r330, %r330, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r330, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r331, %r331, 17;
	 shf.r.clamp.b32    t2, %r331, %r331, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r331, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r409, %r311, %r656;
	add.s32 	%r339, %r409, %r326;
	st.local.u32 	[%rd12+236], %r339;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r334, %r334, 7;
	 shf.r.clamp.b32    t2, %r334, %r334, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r334, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r335, %r335, 17;
	 shf.r.clamp.b32    t2, %r335, %r335, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r335, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r410, %r315, %r656;
	add.s32 	%r343, %r410, %r330;
	st.local.u32 	[%rd12+240], %r343;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r338, %r338, 7;
	 shf.r.clamp.b32    t2, %r338, %r338, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r338, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r339, %r339, 17;
	 shf.r.clamp.b32    t2, %r339, %r339, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r339, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r411, %r319, %r656;
	add.s32 	%r347, %r411, %r334;
	st.local.u32 	[%rd12+244], %r347;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r342, %r342, 7;
	 shf.r.clamp.b32    t2, %r342, %r342, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r342, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r343, %r343, 17;
	 shf.r.clamp.b32    t2, %r343, %r343, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r343, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r412, %r323, %r656;
	add.s32 	%r413, %r412, %r338;
	st.local.u32 	[%rd12+248], %r413;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r346, %r346, 7;
	 shf.r.clamp.b32    t2, %r346, %r346, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r346, 3;
	 xor.b32            %r656, t1, t2;
	 shf.r.clamp.b32    t1, %r347, %r347, 17;
	 shf.r.clamp.b32    t2, %r347, %r347, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r347, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r656, %r656, t1;
	}
	// inline asm
	add.s32 	%r414, %r327, %r656;
	add.s32 	%r415, %r414, %r342;
	st.local.u32 	[%rd12+252], %r415;
	mov.u32 	%r668, 1359893119;
	mov.u32 	%r667, -1521486534;
	mov.u32 	%r666, 1013904242;
	mov.u32 	%r665, -1150833019;
	mov.u32 	%r664, 1779033703;
	mov.u32 	%r663, -1694144372;
	mov.u32 	%r662, 528734635;
	mov.u32 	%r660, 1541459225;
	bra.uni 	BB0_18;

BB0_19:
	ld.local.u32 	%r659, [%rd30+12];

BB0_18:
	mul.wide.u32 	%rd63, %r661, 4;
	mov.u64 	%rd64, k;
	add.s64 	%rd65, %rd64, %rd63;
	ld.const.u32 	%r484, [%rd65];
	add.s32 	%r424, %r659, %r484;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 .reg .u32 res0;
	 .reg .u32 res1;
	 shf.r.clamp.b32    t1, %r664, %r664, 2;
	 shf.r.clamp.b32    t2, %r664, %r664, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r664, %r664, 22;
	 xor.b32            res0, t1, t2;
	 shf.r.clamp.b32    t1, %r668, %r668, 6;
	 shf.r.clamp.b32    t2, %r668, %r668, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r668, %r668, 25;
	 xor.b32            res1, t1, t2;
	 add.s32            t1, %r660, res1;
	 add.s32            t1, t1, %r424;
	 not.b32            t2, %r668;
	 and.b32            t2, t2, %r662;
	 and.b32            t3, %r668, %r663;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, t2;
	 mov.u32            %r660, %r662;
	 mov.u32            %r662, %r663;
	 mov.u32            %r663, %r668;
	 add.s32            %r668, %r667, t1;
	 mov.u32            %r667, %r666;
	 xor.b32            t3, %r665, %r666;
	 and.b32            t3, %r664, t3;
	 and.b32            t2, %r665, %r666;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, res0;
	 add.s32            t1, t1, t2;
	 mov.u32            %r666, %r665;
	 mov.u32            %r665, %r664;
	 mov.u32            %r664, t1;
	}
	// inline asm
	add.s32 	%r485, %r661, 1;
	mul.wide.u32 	%rd66, %r485, 4;
	add.s64 	%rd30, %rd12, %rd66;
	ld.local.u32 	%r486, [%rd30];
	ld.const.u32 	%r487, [%rd65+4];
	add.s32 	%r441, %r486, %r487;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 .reg .u32 res0;
	 .reg .u32 res1;
	 shf.r.clamp.b32    t1, %r664, %r664, 2;
	 shf.r.clamp.b32    t2, %r664, %r664, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r664, %r664, 22;
	 xor.b32            res0, t1, t2;
	 shf.r.clamp.b32    t1, %r668, %r668, 6;
	 shf.r.clamp.b32    t2, %r668, %r668, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r668, %r668, 25;
	 xor.b32            res1, t1, t2;
	 add.s32            t1, %r660, res1;
	 add.s32            t1, t1, %r441;
	 not.b32            t2, %r668;
	 and.b32            t2, t2, %r662;
	 and.b32            t3, %r668, %r663;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, t2;
	 mov.u32            %r660, %r662;
	 mov.u32            %r662, %r663;
	 mov.u32            %r663, %r668;
	 add.s32            %r668, %r667, t1;
	 mov.u32            %r667, %r666;
	 xor.b32            t3, %r665, %r666;
	 and.b32            t3, %r664, t3;
	 and.b32            t2, %r665, %r666;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, res0;
	 add.s32            t1, t1, t2;
	 mov.u32            %r666, %r665;
	 mov.u32            %r665, %r664;
	 mov.u32            %r664, t1;
	}
	// inline asm
	ld.const.u32 	%r488, [%rd65+8];
	ld.local.v2.u32 	{%r489, %r490}, [%rd30+4];
	add.s32 	%r458, %r489, %r488;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 .reg .u32 res0;
	 .reg .u32 res1;
	 shf.r.clamp.b32    t1, %r664, %r664, 2;
	 shf.r.clamp.b32    t2, %r664, %r664, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r664, %r664, 22;
	 xor.b32            res0, t1, t2;
	 shf.r.clamp.b32    t1, %r668, %r668, 6;
	 shf.r.clamp.b32    t2, %r668, %r668, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r668, %r668, 25;
	 xor.b32            res1, t1, t2;
	 add.s32            t1, %r660, res1;
	 add.s32            t1, t1, %r458;
	 not.b32            t2, %r668;
	 and.b32            t2, t2, %r662;
	 and.b32            t3, %r668, %r663;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, t2;
	 mov.u32            %r660, %r662;
	 mov.u32            %r662, %r663;
	 mov.u32            %r663, %r668;
	 add.s32            %r668, %r667, t1;
	 mov.u32            %r667, %r666;
	 xor.b32            t3, %r665, %r666;
	 and.b32            t3, %r664, t3;
	 and.b32            t2, %r665, %r666;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, res0;
	 add.s32            t1, t1, t2;
	 mov.u32            %r666, %r665;
	 mov.u32            %r665, %r664;
	 mov.u32            %r664, t1;
	}
	// inline asm
	ld.const.u32 	%r493, [%rd65+12];
	add.s32 	%r656, %r490, %r493;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 .reg .u32 res0;
	 .reg .u32 res1;
	 shf.r.clamp.b32    t1, %r664, %r664, 2;
	 shf.r.clamp.b32    t2, %r664, %r664, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r664, %r664, 22;
	 xor.b32            res0, t1, t2;
	 shf.r.clamp.b32    t1, %r668, %r668, 6;
	 shf.r.clamp.b32    t2, %r668, %r668, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r668, %r668, 25;
	 xor.b32            res1, t1, t2;
	 add.s32            t1, %r660, res1;
	 add.s32            t1, t1, %r656;
	 not.b32            t2, %r668;
	 and.b32            t2, t2, %r662;
	 and.b32            t3, %r668, %r663;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, t2;
	 mov.u32            %r660, %r662;
	 mov.u32            %r662, %r663;
	 mov.u32            %r663, %r668;
	 add.s32            %r668, %r667, t1;
	 mov.u32            %r667, %r666;
	 xor.b32            t3, %r665, %r666;
	 and.b32            t3, %r664, t3;
	 and.b32            t2, %r665, %r666;
	 xor.b32            t2, t2, t3;
	 add.s32            t1, t1, res0;
	 add.s32            t1, t1, t2;
	 mov.u32            %r666, %r665;
	 mov.u32            %r665, %r664;
	 mov.u32            %r664, t1;
	}
	// inline asm
	add.s32 	%r661, %r661, 4;
	setp.eq.s32	%p9, %r661, 64;
	@%p9 bra 	BB0_20;
	bra.uni 	BB0_19;

BB0_20:
	add.s64 	%rd84, %rd84, 1;
	add.s32 	%r49, %r665, -1150833019;
	add.s32 	%r50, %r666, 1013904242;
	add.s32 	%r51, %r667, -1521486534;
	setp.ne.s32	%p10, %r664, -1779033703;
	@%p10 bra 	BB0_9;

	mov.u32 	%r671, 0;
	mov.u32 	%r669, 8192;
	setp.lt.s32	%p11, %r49, 0;
	@%p11 bra 	BB0_22;

	shl.b32 	%r670, %r49, 1;
	mov.u32 	%r671, 1;
	mov.u32 	%r669, 8448;
	setp.lt.s32	%p12, %r670, 0;
	@%p12 bra 	BB0_54;

	shl.b32 	%r670, %r49, 2;
	mov.u32 	%r671, 2;
	mov.u32 	%r669, 8704;
	setp.lt.s32	%p13, %r670, 0;
	@%p13 bra 	BB0_54;

	shl.b32 	%r670, %r49, 3;
	mov.u32 	%r671, 3;
	mov.u32 	%r669, 8960;
	setp.lt.s32	%p14, %r670, 0;
	@%p14 bra 	BB0_54;

	shl.b32 	%r670, %r49, 4;
	mov.u32 	%r671, 4;
	mov.u32 	%r669, 9216;
	setp.lt.s32	%p15, %r670, 0;
	@%p15 bra 	BB0_54;

	shl.b32 	%r670, %r49, 5;
	mov.u32 	%r671, 5;
	mov.u32 	%r669, 9472;
	setp.lt.s32	%p16, %r670, 0;
	@%p16 bra 	BB0_54;

	shl.b32 	%r670, %r49, 6;
	mov.u32 	%r671, 6;
	mov.u32 	%r669, 9728;
	setp.lt.s32	%p17, %r670, 0;
	@%p17 bra 	BB0_54;

	shl.b32 	%r670, %r49, 7;
	mov.u32 	%r671, 7;
	mov.u32 	%r669, 9984;
	setp.lt.s32	%p18, %r670, 0;
	@%p18 bra 	BB0_54;

	shl.b32 	%r670, %r49, 8;
	mov.u32 	%r671, 8;
	mov.u32 	%r669, 10240;
	setp.lt.s32	%p19, %r670, 0;
	@%p19 bra 	BB0_54;

	shl.b32 	%r670, %r49, 9;
	mov.u32 	%r671, 9;
	mov.u32 	%r669, 10496;
	setp.lt.s32	%p20, %r670, 0;
	@%p20 bra 	BB0_54;

	shl.b32 	%r670, %r49, 10;
	mov.u32 	%r671, 10;
	mov.u32 	%r669, 10752;
	setp.lt.s32	%p21, %r670, 0;
	@%p21 bra 	BB0_54;

	shl.b32 	%r670, %r49, 11;
	mov.u32 	%r671, 11;
	mov.u32 	%r669, 11008;
	setp.lt.s32	%p22, %r670, 0;
	@%p22 bra 	BB0_54;

	shl.b32 	%r670, %r49, 12;
	mov.u32 	%r671, 12;
	mov.u32 	%r669, 11264;
	setp.lt.s32	%p23, %r670, 0;
	@%p23 bra 	BB0_54;

	shl.b32 	%r670, %r49, 13;
	mov.u32 	%r671, 13;
	mov.u32 	%r669, 11520;
	setp.lt.s32	%p24, %r670, 0;
	@%p24 bra 	BB0_54;

	shl.b32 	%r670, %r49, 14;
	mov.u32 	%r671, 14;
	mov.u32 	%r669, 11776;
	setp.lt.s32	%p25, %r670, 0;
	@%p25 bra 	BB0_54;

	shl.b32 	%r670, %r49, 15;
	mov.u32 	%r671, 15;
	mov.u32 	%r669, 12032;
	setp.lt.s32	%p26, %r670, 0;
	@%p26 bra 	BB0_54;

	shl.b32 	%r670, %r49, 16;
	mov.u32 	%r671, 16;
	mov.u32 	%r669, 12288;
	setp.lt.s32	%p27, %r670, 0;
	@%p27 bra 	BB0_54;

	shl.b32 	%r670, %r49, 17;
	mov.u32 	%r671, 17;
	mov.u32 	%r669, 12544;
	setp.lt.s32	%p28, %r670, 0;
	@%p28 bra 	BB0_54;

	shl.b32 	%r670, %r49, 18;
	mov.u32 	%r671, 18;
	mov.u32 	%r669, 12800;
	setp.lt.s32	%p29, %r670, 0;
	@%p29 bra 	BB0_54;

	shl.b32 	%r670, %r49, 19;
	mov.u32 	%r671, 19;
	mov.u32 	%r669, 13056;
	setp.lt.s32	%p30, %r670, 0;
	@%p30 bra 	BB0_54;

	shl.b32 	%r670, %r49, 20;
	mov.u32 	%r671, 20;
	mov.u32 	%r669, 13312;
	setp.lt.s32	%p31, %r670, 0;
	@%p31 bra 	BB0_54;

	shl.b32 	%r670, %r49, 21;
	mov.u32 	%r671, 21;
	mov.u32 	%r669, 13568;
	setp.lt.s32	%p32, %r670, 0;
	@%p32 bra 	BB0_54;

	shl.b32 	%r670, %r49, 22;
	mov.u32 	%r671, 22;
	mov.u32 	%r669, 13824;
	setp.lt.s32	%p33, %r670, 0;
	@%p33 bra 	BB0_54;

	shl.b32 	%r670, %r49, 23;
	mov.u32 	%r671, 23;
	mov.u32 	%r669, 14080;
	setp.lt.s32	%p34, %r670, 0;
	@%p34 bra 	BB0_54;

	shl.b32 	%r670, %r49, 24;
	mov.u32 	%r671, 24;
	mov.u32 	%r669, 14336;
	setp.lt.s32	%p35, %r670, 0;
	@%p35 bra 	BB0_54;

	shl.b32 	%r670, %r49, 25;
	mov.u32 	%r671, 25;
	mov.u32 	%r669, 14592;
	setp.lt.s32	%p36, %r670, 0;
	@%p36 bra 	BB0_54;

	shl.b32 	%r670, %r49, 26;
	mov.u32 	%r671, 26;
	mov.u32 	%r669, 14848;
	setp.lt.s32	%p37, %r670, 0;
	@%p37 bra 	BB0_54;

	shl.b32 	%r670, %r49, 27;
	mov.u32 	%r671, 27;
	mov.u32 	%r669, 15104;
	setp.lt.s32	%p38, %r670, 0;
	@%p38 bra 	BB0_54;

	shl.b32 	%r670, %r49, 28;
	mov.u32 	%r671, 28;
	mov.u32 	%r669, 15360;
	setp.lt.s32	%p39, %r670, 0;
	@%p39 bra 	BB0_54;

	shl.b32 	%r670, %r49, 29;
	mov.u32 	%r671, 29;
	mov.u32 	%r669, 15616;
	setp.lt.s32	%p40, %r670, 0;
	@%p40 bra 	BB0_54;

	shl.b32 	%r670, %r49, 30;
	mov.u32 	%r671, 30;
	mov.u32 	%r669, 15872;
	setp.lt.s32	%p41, %r670, 0;
	@%p41 bra 	BB0_54;

	shl.b32 	%r670, %r49, 31;
	setp.eq.s32	%p42, %r670, 0;
	mov.u32 	%r671, 31;
	mov.u32 	%r669, 16128;
	@%p42 bra 	BB0_57;
	bra.uni 	BB0_54;

BB0_57:
	mov.u32 	%r674, 0;
	mov.u32 	%r672, 16384;
	setp.lt.s32	%p45, %r50, 0;
	mov.u32 	%r673, %r50;
	@%p45 bra 	BB0_89;

	shl.b32 	%r673, %r50, 1;
	mov.u32 	%r674, 1;
	mov.u32 	%r672, 16640;
	setp.lt.s32	%p46, %r673, 0;
	@%p46 bra 	BB0_89;

	shl.b32 	%r673, %r50, 2;
	mov.u32 	%r674, 2;
	mov.u32 	%r672, 16896;
	setp.lt.s32	%p47, %r673, 0;
	@%p47 bra 	BB0_89;

	shl.b32 	%r673, %r50, 3;
	mov.u32 	%r674, 3;
	mov.u32 	%r672, 17152;
	setp.lt.s32	%p48, %r673, 0;
	@%p48 bra 	BB0_89;

	shl.b32 	%r673, %r50, 4;
	mov.u32 	%r674, 4;
	mov.u32 	%r672, 17408;
	setp.lt.s32	%p49, %r673, 0;
	@%p49 bra 	BB0_89;

	shl.b32 	%r673, %r50, 5;
	mov.u32 	%r674, 5;
	mov.u32 	%r672, 17664;
	setp.lt.s32	%p50, %r673, 0;
	@%p50 bra 	BB0_89;

	shl.b32 	%r673, %r50, 6;
	mov.u32 	%r674, 6;
	mov.u32 	%r672, 17920;
	setp.lt.s32	%p51, %r673, 0;
	@%p51 bra 	BB0_89;

	shl.b32 	%r673, %r50, 7;
	mov.u32 	%r674, 7;
	mov.u32 	%r672, 18176;
	setp.lt.s32	%p52, %r673, 0;
	@%p52 bra 	BB0_89;

	shl.b32 	%r673, %r50, 8;
	mov.u32 	%r674, 8;
	mov.u32 	%r672, 18432;
	setp.lt.s32	%p53, %r673, 0;
	@%p53 bra 	BB0_89;

	shl.b32 	%r673, %r50, 9;
	mov.u32 	%r674, 9;
	mov.u32 	%r672, 18688;
	setp.lt.s32	%p54, %r673, 0;
	@%p54 bra 	BB0_89;

	shl.b32 	%r673, %r50, 10;
	mov.u32 	%r674, 10;
	mov.u32 	%r672, 18944;
	setp.lt.s32	%p55, %r673, 0;
	@%p55 bra 	BB0_89;

	shl.b32 	%r673, %r50, 11;
	mov.u32 	%r674, 11;
	mov.u32 	%r672, 19200;
	setp.lt.s32	%p56, %r673, 0;
	@%p56 bra 	BB0_89;

	shl.b32 	%r673, %r50, 12;
	mov.u32 	%r674, 12;
	mov.u32 	%r672, 19456;
	setp.lt.s32	%p57, %r673, 0;
	@%p57 bra 	BB0_89;

	shl.b32 	%r673, %r50, 13;
	mov.u32 	%r674, 13;
	mov.u32 	%r672, 19712;
	setp.lt.s32	%p58, %r673, 0;
	@%p58 bra 	BB0_89;

	shl.b32 	%r673, %r50, 14;
	mov.u32 	%r674, 14;
	mov.u32 	%r672, 19968;
	setp.lt.s32	%p59, %r673, 0;
	@%p59 bra 	BB0_89;

	shl.b32 	%r673, %r50, 15;
	mov.u32 	%r674, 15;
	mov.u32 	%r672, 20224;
	setp.lt.s32	%p60, %r673, 0;
	@%p60 bra 	BB0_89;

	shl.b32 	%r673, %r50, 16;
	mov.u32 	%r674, 16;
	mov.u32 	%r672, 20480;
	setp.lt.s32	%p61, %r673, 0;
	@%p61 bra 	BB0_89;

	shl.b32 	%r673, %r50, 17;
	mov.u32 	%r674, 17;
	mov.u32 	%r672, 20736;
	setp.lt.s32	%p62, %r673, 0;
	@%p62 bra 	BB0_89;

	shl.b32 	%r673, %r50, 18;
	mov.u32 	%r674, 18;
	mov.u32 	%r672, 20992;
	setp.lt.s32	%p63, %r673, 0;
	@%p63 bra 	BB0_89;

	shl.b32 	%r673, %r50, 19;
	mov.u32 	%r674, 19;
	mov.u32 	%r672, 21248;
	setp.lt.s32	%p64, %r673, 0;
	@%p64 bra 	BB0_89;

	shl.b32 	%r673, %r50, 20;
	mov.u32 	%r674, 20;
	mov.u32 	%r672, 21504;
	setp.lt.s32	%p65, %r673, 0;
	@%p65 bra 	BB0_89;

	shl.b32 	%r673, %r50, 21;
	mov.u32 	%r674, 21;
	mov.u32 	%r672, 21760;
	setp.lt.s32	%p66, %r673, 0;
	@%p66 bra 	BB0_89;

	shl.b32 	%r673, %r50, 22;
	mov.u32 	%r674, 22;
	mov.u32 	%r672, 22016;
	setp.lt.s32	%p67, %r673, 0;
	@%p67 bra 	BB0_89;

	shl.b32 	%r673, %r50, 23;
	mov.u32 	%r674, 23;
	mov.u32 	%r672, 22272;
	setp.lt.s32	%p68, %r673, 0;
	@%p68 bra 	BB0_89;

	shl.b32 	%r673, %r50, 24;
	mov.u32 	%r674, 24;
	mov.u32 	%r672, 22528;
	setp.lt.s32	%p69, %r673, 0;
	@%p69 bra 	BB0_89;

	shl.b32 	%r673, %r50, 25;
	mov.u32 	%r674, 25;
	mov.u32 	%r672, 22784;
	setp.lt.s32	%p70, %r673, 0;
	@%p70 bra 	BB0_89;

	shl.b32 	%r673, %r50, 26;
	mov.u32 	%r674, 26;
	mov.u32 	%r672, 23040;
	setp.lt.s32	%p71, %r673, 0;
	@%p71 bra 	BB0_89;

	shl.b32 	%r673, %r50, 27;
	mov.u32 	%r674, 27;
	mov.u32 	%r672, 23296;
	setp.lt.s32	%p72, %r673, 0;
	@%p72 bra 	BB0_89;

	shl.b32 	%r673, %r50, 28;
	mov.u32 	%r674, 28;
	mov.u32 	%r672, 23552;
	setp.lt.s32	%p73, %r673, 0;
	@%p73 bra 	BB0_89;

	shl.b32 	%r673, %r50, 29;
	mov.u32 	%r674, 29;
	mov.u32 	%r672, 23808;
	setp.lt.s32	%p74, %r673, 0;
	@%p74 bra 	BB0_89;

	shl.b32 	%r673, %r50, 30;
	mov.u32 	%r674, 30;
	mov.u32 	%r672, 24064;
	setp.lt.s32	%p75, %r673, 0;
	@%p75 bra 	BB0_89;

	shl.b32 	%r673, %r50, 31;
	setp.eq.s32	%p76, %r673, 0;
	mov.u32 	%r674, 31;
	mov.u32 	%r672, 24320;
	mov.u32 	%r675, 96;
	@%p76 bra 	BB0_92;

BB0_89:
	setp.eq.s32	%p77, %r674, 31;
	@%p77 bra 	BB0_91;
	bra.uni 	BB0_90;

BB0_91:
	shr.u32 	%r636, %r51, 24;
	add.s32 	%r675, %r672, %r636;
	bra.uni 	BB0_92;

BB0_22:
	mov.u32 	%r670, %r49;

BB0_54:
	setp.eq.s32	%p43, %r671, 31;
	@%p43 bra 	BB0_56;
	bra.uni 	BB0_55;

BB0_56:
	shr.u32 	%r564, %r50, 24;
	add.s32 	%r675, %r669, %r564;
	bra.uni 	BB0_92;

BB0_55:
	bfe.u32 	%r558, %r670, 23, 8;
	add.s32 	%r559, %r558, %r669;
	mov.u32 	%r560, 56;
	sub.s32 	%r561, %r560, %r671;
	shr.u32 	%r562, %r50, %r561;
	setp.gt.u32	%p44, %r671, 23;
	selp.b32	%r563, %r562, 0, %p44;
	add.s32 	%r675, %r559, %r563;

BB0_92:
	ld.param.u32 	%r648, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	setp.le.u32	%p79, %r675, %r648;
	@%p79 bra 	BB0_9;

	add.s64 	%rd86, %rd4, 35;
	mov.u16 	%rs7, 1;
	st.volatile.global.u8 	[%rd10], %rs7;
	st.volatile.global.u8 	[%rd2], %rs7;
	add.s32 	%r125, %r660, 1541459225;
	add.s32 	%r126, %r662, 528734635;
	add.s32 	%r127, %r663, -1694144372;
	mov.u32 	%r676, 0;
	mov.u64 	%rd85, %rd11;

BB0_94:
	.pragma "nounroll";
	add.s64 	%rd75, %rd4, 32;
	ld.local.v4.u8 	{%rs8, %rs9, %rs10, %rs11}, [%rd86+-3];
	st.global.u8 	[%rd85], %rs11;
	st.global.u8 	[%rd85+1], %rs10;
	st.global.u8 	[%rd85+2], %rs9;
	st.global.u8 	[%rd85+3], %rs8;
	add.s32 	%r676, %r676, 4;
	setp.lt.u32	%p80, %r676, 20;
	cvt.u64.u32	%rd67, %r676;
	add.s64 	%rd68, %rd67, %rd75;
	add.s64 	%rd86, %rd68, 3;
	add.s64 	%rd85, %rd11, %rd67;
	@%p80 bra 	BB0_94;

	add.u64 	%rd77, %SP, 368;
	cvta.to.local.u64 	%rd88, %rd77;
	ld.local.v2.u8 	{%rs17, %rs18}, [%rd4+54];
	ld.local.u8 	%rs21, [%rd4+53];
	st.global.u8 	[%rd11+20], %rs18;
	st.global.u8 	[%rd11+21], %rs17;
	st.global.u8 	[%rd11+22], %rs21;
	mov.u32 	%r639, 0;
	add.s32 	%r641, %r665, -1150833019;
	st.local.v4.u32 	[%rd88], {%r639, %r641, %r50, %r51};
	add.s32 	%r642, %r668, 1359893119;
	st.local.v4.u32 	[%rd88+16], {%r642, %r127, %r126, %r125};
	mov.u16 	%rs27, 0;
	mov.u32 	%r677, 4;
	mov.u64 	%rd87, %rd3;
	bra.uni 	BB0_96;

BB0_97:
	add.s64 	%rd38, %rd88, 4;
	ld.local.u8 	%rs27, [%rd88+4];
	add.s64 	%rd87, %rd87, 4;
	add.s32 	%r677, %r677, 4;
	mov.u64 	%rd88, %rd38;

BB0_96:
	ld.local.v2.u8 	{%rs22, %rs23}, [%rd88+2];
	ld.local.u8 	%rs26, [%rd88+1];
	st.global.u8 	[%rd87], %rs23;
	st.global.u8 	[%rd87+1], %rs22;
	st.global.u8 	[%rd87+2], %rs26;
	st.global.u8 	[%rd87+3], %rs27;
	setp.gt.u32	%p81, %r677, 31;
	@%p81 bra 	BB0_9;
	bra.uni 	BB0_97;

BB0_90:
	bfe.u32 	%r630, %r673, 23, 8;
	add.s32 	%r631, %r630, %r672;
	mov.u32 	%r632, 56;
	sub.s32 	%r633, %r632, %r674;
	shr.u32 	%r634, %r51, %r633;
	setp.gt.u32	%p78, %r674, 23;
	selp.b32	%r635, %r634, 0, %p78;
	add.s32 	%r675, %r631, %r635;
	bra.uni 	BB0_92;

BB0_9:
	mul.hi.s64 	%rd54, %rd84, -6640827866535438581;
	add.s64 	%rd55, %rd54, %rd84;
	shr.u64 	%rd56, %rd55, 63;
	shr.s64 	%rd57, %rd55, 6;
	add.s64 	%rd58, %rd57, %rd56;
	mul.lo.s64 	%rd59, %rd58, 100;
	sub.s64 	%rd60, %rd84, %rd59;
	setp.ne.s64	%p5, %rd60, 9;
	@%p5 bra 	BB0_14;

	ld.volatile.global.u8 	%rs6, [%rd2];
	setp.eq.s16	%p6, %rs6, 0;
	@%p6 bra 	BB0_14;

	mov.u32 	%r647, %tid.x;
	mov.u32 	%r646, %ctaid.x;
	mov.u32 	%r645, %ntid.x;
	mad.lo.s32 	%r644, %r645, %r646, %r647;
	ld.param.u64 	%rd71, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd70, %rd71;
	cvt.u64.u32	%rd69, %r644;
	shl.b64 	%rd61, %rd69, 3;
	add.s64 	%rd62, %rd70, %rd61;
	st.global.u64 	[%rd62], %rd84;
	setp.ne.s32	%p7, %r644, 1;
	@%p7 bra 	BB0_13;

	ld.param.u64 	%rd73, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	cvta.to.global.u64 	%rd72, %rd73;
	st.volatile.global.u64 	[%rd72], %rd84;

BB0_13:
	ret;
}


