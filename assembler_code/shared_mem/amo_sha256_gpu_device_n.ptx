//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};
.extern .shared .align 4 .b8 shared_memory[];

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 4 .b8 	__local_depot0[96];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<92>;
	.reg .b16 	%rs<27>;
	.reg .b32 	%r<432>;
	.reg .b64 	%rd<86>;


	mov.u64 	%rd85, __local_depot0;
	cvta.local.u64 	%SP, %rd85;
	ld.param.u64 	%rd33, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0];
	ld.param.u32 	%r100, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	ld.param.u64 	%rd30, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2];
	ld.param.u64 	%rd31, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3];
	ld.param.u64 	%rd34, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4];
	ld.param.u64 	%rd32, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	ld.param.u32 	%r101, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6];
	ld.param.u64 	%rd35, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd1, %rd35;
	cvta.to.global.u64 	%rd2, %rd34;
	cvta.to.global.u64 	%rd3, %rd33;
	add.u64 	%rd36, %SP, 0;
	cvta.to.local.u64 	%rd4, %rd36;
	mov.u32 	%r103, %ntid.x;
	mov.u32 	%r104, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r103, %r104, %r1;
	cvt.u64.u32	%rd5, %r2;
	mov.u32 	%r409, -8;
	mov.u64 	%rd76, %rd4;
	mov.u64 	%rd77, %rd3;

BB0_1:
	.pragma "nounroll";
	ld.global.u8 	%r105, [%rd77];
	shl.b32 	%r106, %r105, 24;
	ld.global.u8 	%r107, [%rd77+1];
	shl.b32 	%r108, %r107, 16;
	or.b32  	%r109, %r108, %r106;
	ld.global.u8 	%rs1, [%rd77+2];
	mul.wide.u16 	%r110, %rs1, 256;
	or.b32  	%r111, %r109, %r110;
	ld.global.u8 	%r112, [%rd77+3];
	or.b32  	%r113, %r111, %r112;
	st.local.u32 	[%rd76], %r113;
	add.s64 	%rd77, %rd77, 4;
	add.s64 	%rd76, %rd76, 4;
	add.s32 	%r409, %r409, 1;
	setp.ne.s32	%p2, %r409, 0;
	@%p2 bra 	BB0_1;

	cvta.to.global.u64 	%rd10, %rd32;
	cvta.to.global.u64 	%rd11, %rd31;
	cvta.to.global.u64 	%rd12, %rd30;
	shl.b32 	%r5, %r1, 6;
	mov.u32 	%r410, -5;
	mov.u64 	%rd78, -8;
	mov.u64 	%rd79, %rd12;

BB0_3:
	.pragma "nounroll";
	ld.global.u8 	%r115, [%rd79];
	shl.b32 	%r116, %r115, 24;
	ld.global.u8 	%r117, [%rd79+1];
	shl.b32 	%r118, %r117, 16;
	or.b32  	%r119, %r118, %r116;
	ld.global.u8 	%rs2, [%rd79+2];
	mul.wide.u16 	%r120, %rs2, 256;
	or.b32  	%r121, %r119, %r120;
	ld.global.u8 	%r122, [%rd79+3];
	or.b32  	%r123, %r121, %r122;
	shl.b64 	%rd38, %rd78, 2;
	sub.s64 	%rd39, %rd4, %rd38;
	st.local.u32 	[%rd39], %r123;
	add.s64 	%rd79, %rd79, 4;
	add.s64 	%rd78, %rd78, -1;
	add.s32 	%r410, %r410, 1;
	setp.ne.s32	%p3, %r410, 0;
	@%p3 bra 	BB0_3;

	ld.global.u8 	%r124, [%rd12+20];
	shl.b32 	%r125, %r124, 24;
	ld.global.u8 	%r126, [%rd12+21];
	shl.b32 	%r127, %r126, 16;
	ld.global.u8 	%rs3, [%rd12+22];
	mul.wide.u16 	%r128, %rs3, 256;
	or.b32  	%r129, %r125, %r127;
	or.b32  	%r130, %r129, %r128;
	or.b32  	%r131, %r130, 128;
	st.local.u32 	[%rd4+52], %r131;
	mov.u32 	%r132, 0;
	st.local.u32 	[%rd4+56], %r132;
	mov.u32 	%r133, 440;
	st.local.u32 	[%rd4+60], %r133;
	ld.local.u32 	%r134, [%rd4+36];
	xor.b32  	%r135, %r134, %r2;
	ld.local.u32 	%r136, [%rd4+40];
	st.local.u32 	[%rd4+36], %r135;
	xor.b32  	%r137, %r136, %r101;
	st.local.u32 	[%rd4+40], %r137;
	shl.b32 	%r138, %r5, 2;
	mov.u32 	%r139, shared_memory;
	add.s32 	%r8, %r139, %r138;
	shl.b32 	%r140, %r1, 8;
	add.s32 	%r9, %r139, %r140;
	mov.u64 	%rd80, 0;
	bra.uni 	BB0_5;

BB0_10:
	ld.local.u32 	%r142, [%rd4+44];
	add.s32 	%r10, %r142, 1;
	ld.local.u32 	%r411, [%rd4+48];
	st.local.u32 	[%rd4+44], %r10;
	setp.ne.s32	%p7, %r10, 0;
	@%p7 bra 	BB0_12;

	add.s32 	%r411, %r411, 1;
	st.local.u32 	[%rd4+48], %r411;

BB0_12:
	mov.u64 	%rd50, 3144134277;
	st.local.u32 	[%rd4+68], %rd50;
	mov.u64 	%rd51, 1779033703;
	st.local.u32 	[%rd4+64], %rd51;
	mov.u64 	%rd52, 2773480762;
	st.local.u32 	[%rd4+76], %rd52;
	mov.u64 	%rd53, 1013904242;
	st.local.u32 	[%rd4+72], %rd53;
	mov.u64 	%rd54, 2600822924;
	st.local.u32 	[%rd4+84], %rd54;
	mov.u64 	%rd55, 1359893119;
	st.local.u32 	[%rd4+80], %rd55;
	mov.u64 	%rd56, 1541459225;
	st.local.u32 	[%rd4+92], %rd56;
	mov.u64 	%rd57, 528734635;
	st.local.u32 	[%rd4+88], %rd57;
	ld.local.u32 	%r144, [%rd4];
	ld.local.u32 	%r145, [%rd4+4];
	ld.local.u32 	%r146, [%rd4+8];
	ld.local.u32 	%r147, [%rd4+12];
	ld.local.u32 	%r148, [%rd4+16];
	ld.local.u32 	%r149, [%rd4+20];
	ld.local.u32 	%r150, [%rd4+24];
	ld.local.u32 	%r151, [%rd4+28];
	ld.local.u32 	%r152, [%rd4+32];
	ld.local.u32 	%r153, [%rd4+36];
	ld.local.u32 	%r154, [%rd4+40];
	ld.local.u32 	%r155, [%rd4+52];
	ld.local.u32 	%r156, [%rd4+56];
	ld.local.u32 	%r157, [%rd4+60];
	st.shared.u32 	[%r8], %r144;
	st.shared.u32 	[%r8+4], %r145;
	st.shared.u32 	[%r8+8], %r146;
	st.shared.u32 	[%r8+12], %r147;
	st.shared.u32 	[%r8+16], %r148;
	st.shared.u32 	[%r8+20], %r149;
	st.shared.u32 	[%r8+24], %r150;
	st.shared.u32 	[%r8+28], %r151;
	st.shared.u32 	[%r8+32], %r152;
	st.shared.u32 	[%r8+36], %r153;
	st.shared.u32 	[%r8+40], %r154;
	add.s32 	%r407, %r142, 1;
	st.shared.u32 	[%r8+44], %r407;
	st.shared.u32 	[%r8+48], %r411;
	st.shared.u32 	[%r8+52], %r155;
	st.shared.u32 	[%r8+56], %r156;
	st.shared.u32 	[%r8+60], %r157;
	add.s64 	%rd80, %rd80, 1;
	mov.u32 	%r413, -48;
	mov.u32 	%r412, %r9;

BB0_13:
	ld.shared.u32 	%r159, [%r412+4];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r159, %r159, 7;
	 shf.r.clamp.b32    t2, %r159, %r159, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r159, 3;
	 xor.b32            %r158, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r161, [%r412+56];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r161, %r161, 17;
	 shf.r.clamp.b32    t2, %r161, %r161, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r161, 10;
	 xor.b32            %r160, t1, t2;
	}
	// inline asm
	add.s32 	%r199, %r160, %r158;
	ld.shared.u32 	%r200, [%r412+36];
	add.s32 	%r201, %r199, %r200;
	ld.shared.u32 	%r202, [%r412];
	add.s32 	%r203, %r201, %r202;
	ld.shared.u32 	%r204, [%r412+40];
	st.shared.u32 	[%r412+64], %r203;
	ld.shared.u32 	%r163, [%r412+8];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r163, %r163, 7;
	 shf.r.clamp.b32    t2, %r163, %r163, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r163, 3;
	 xor.b32            %r162, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r165, [%r412+60];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r165, %r165, 17;
	 shf.r.clamp.b32    t2, %r165, %r165, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r165, 10;
	 xor.b32            %r164, t1, t2;
	}
	// inline asm
	add.s32 	%r205, %r164, %r162;
	add.s32 	%r206, %r205, %r204;
	ld.shared.u32 	%r207, [%r412+4];
	add.s32 	%r208, %r206, %r207;
	ld.shared.u32 	%r209, [%r412+44];
	st.shared.u32 	[%r412+68], %r208;
	ld.shared.u32 	%r167, [%r412+12];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r167, %r167, 7;
	 shf.r.clamp.b32    t2, %r167, %r167, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r167, 3;
	 xor.b32            %r166, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r169, [%r412+64];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r169, %r169, 17;
	 shf.r.clamp.b32    t2, %r169, %r169, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r169, 10;
	 xor.b32            %r168, t1, t2;
	}
	// inline asm
	add.s32 	%r210, %r168, %r166;
	add.s32 	%r211, %r210, %r209;
	ld.shared.u32 	%r212, [%r412+8];
	add.s32 	%r213, %r211, %r212;
	ld.shared.u32 	%r214, [%r412+48];
	st.shared.u32 	[%r412+72], %r213;
	ld.shared.u32 	%r171, [%r412+16];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r171, %r171, 7;
	 shf.r.clamp.b32    t2, %r171, %r171, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r171, 3;
	 xor.b32            %r170, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r173, [%r412+68];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r173, %r173, 17;
	 shf.r.clamp.b32    t2, %r173, %r173, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r173, 10;
	 xor.b32            %r172, t1, t2;
	}
	// inline asm
	add.s32 	%r215, %r172, %r170;
	add.s32 	%r216, %r215, %r214;
	ld.shared.u32 	%r217, [%r412+12];
	add.s32 	%r218, %r216, %r217;
	ld.shared.u32 	%r219, [%r412+52];
	st.shared.u32 	[%r412+76], %r218;
	ld.shared.u32 	%r175, [%r412+20];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r175, %r175, 7;
	 shf.r.clamp.b32    t2, %r175, %r175, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r175, 3;
	 xor.b32            %r174, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r177, [%r412+72];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r177, %r177, 17;
	 shf.r.clamp.b32    t2, %r177, %r177, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r177, 10;
	 xor.b32            %r176, t1, t2;
	}
	// inline asm
	add.s32 	%r220, %r176, %r174;
	add.s32 	%r221, %r220, %r219;
	ld.shared.u32 	%r222, [%r412+16];
	add.s32 	%r223, %r221, %r222;
	ld.shared.u32 	%r224, [%r412+56];
	st.shared.u32 	[%r412+80], %r223;
	ld.shared.u32 	%r179, [%r412+24];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r179, %r179, 7;
	 shf.r.clamp.b32    t2, %r179, %r179, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r179, 3;
	 xor.b32            %r178, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r181, [%r412+76];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r181, %r181, 17;
	 shf.r.clamp.b32    t2, %r181, %r181, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r181, 10;
	 xor.b32            %r180, t1, t2;
	}
	// inline asm
	add.s32 	%r225, %r180, %r178;
	add.s32 	%r226, %r225, %r224;
	ld.shared.u32 	%r227, [%r412+20];
	add.s32 	%r228, %r226, %r227;
	ld.shared.u32 	%r229, [%r412+60];
	st.shared.u32 	[%r412+84], %r228;
	ld.shared.u32 	%r183, [%r412+28];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r183, %r183, 7;
	 shf.r.clamp.b32    t2, %r183, %r183, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r183, 3;
	 xor.b32            %r182, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r185, [%r412+80];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r185, %r185, 17;
	 shf.r.clamp.b32    t2, %r185, %r185, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r185, 10;
	 xor.b32            %r184, t1, t2;
	}
	// inline asm
	add.s32 	%r230, %r184, %r182;
	add.s32 	%r231, %r230, %r229;
	ld.shared.u32 	%r232, [%r412+24];
	add.s32 	%r233, %r231, %r232;
	ld.shared.u32 	%r234, [%r412+64];
	st.shared.u32 	[%r412+88], %r233;
	add.s32 	%r17, %r412, 32;
	ld.shared.u32 	%r187, [%r412+32];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r187, %r187, 7;
	 shf.r.clamp.b32    t2, %r187, %r187, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r187, 3;
	 xor.b32            %r186, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r189, [%r412+84];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r189, %r189, 17;
	 shf.r.clamp.b32    t2, %r189, %r189, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r189, 10;
	 xor.b32            %r188, t1, t2;
	}
	// inline asm
	add.s32 	%r235, %r188, %r186;
	add.s32 	%r236, %r235, %r234;
	ld.shared.u32 	%r237, [%r412+28];
	add.s32 	%r238, %r236, %r237;
	st.shared.u32 	[%r412+92], %r238;
	add.s32 	%r413, %r413, 8;
	mov.u32 	%r27, -1694144372;
	mov.u32 	%r26, 1359893119;
	mov.u32 	%r257, -1521486534;
	mov.u32 	%r419, 1013904242;
	mov.u32 	%r23, -1150833019;
	mov.u32 	%r22, 1779033703;
	mov.u32 	%r416, 528734635;
	mov.u32 	%r31, 1541459225;
	setp.ne.s32	%p8, %r413, 0;
	mov.u32 	%r412, %r17;
	@%p8 bra 	BB0_13;

	mov.u32 	%r19, %r132;

BB0_15:
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r22, %r22, 2;
	 shf.r.clamp.b32    t2, %r22, %r22, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r22, %r22, 22;
	 xor.b32            %r239, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r26, %r26, 6;
	 shf.r.clamp.b32    t2, %r26, %r26, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r26, %r26, 25;
	 xor.b32            %r241, t1, t2;
	}
	// inline asm
	not.b32 	%r243, %r26;
	and.b32  	%r244, %r416, %r243;
	and.b32  	%r245, %r26, %r27;
	xor.b32  	%r246, %r244, %r245;
	mul.wide.u32 	%rd58, %r19, 4;
	mov.u64 	%rd59, k;
	add.s64 	%rd60, %rd59, %rd58;
	add.s32 	%r247, %r19, %r5;
	shl.b32 	%r248, %r247, 2;
	add.s32 	%r29, %r139, %r248;
	add.s32 	%r250, %r246, %r31;
	add.s32 	%r251, %r250, %r241;
	ld.const.u32 	%r252, [%rd60];
	add.s32 	%r253, %r251, %r252;
	ld.shared.u32 	%r254, [%r29];
	add.s32 	%r30, %r253, %r254;
	add.s32 	%r31, %r30, %r257;
	add.s32 	%r255, %r31, 1541459225;
	setp.gt.u32	%p9, %r255, 65535;
	setp.eq.s32	%p10, %r19, 60;
	and.pred  	%p11, %p10, %p9;
	@%p11 bra 	BB0_5;

	xor.b32  	%r260, %r23, %r419;
	and.b32  	%r261, %r22, %r260;
	and.b32  	%r262, %r23, %r419;
	xor.b32  	%r263, %r261, %r262;
	add.s32 	%r264, %r239, %r263;
	add.s32 	%r257, %r264, %r30;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r257, %r257, 2;
	 shf.r.clamp.b32    t2, %r257, %r257, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r257, %r257, 22;
	 xor.b32            %r256, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r31, %r31, 6;
	 shf.r.clamp.b32    t2, %r31, %r31, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r31, %r31, 25;
	 xor.b32            %r258, t1, t2;
	}
	// inline asm
	not.b32 	%r265, %r31;
	and.b32  	%r266, %r27, %r265;
	and.b32  	%r267, %r31, %r26;
	xor.b32  	%r268, %r266, %r267;
	add.s32 	%r269, %r19, 1;
	mul.wide.u32 	%rd61, %r269, 4;
	add.s64 	%rd63, %rd59, %rd61;
	add.s32 	%r270, %r268, %r416;
	add.s32 	%r271, %r270, %r258;
	ld.const.u32 	%r272, [%rd63];
	add.s32 	%r273, %r271, %r272;
	ld.shared.u32 	%r274, [%r29+4];
	add.s32 	%r35, %r273, %r274;
	add.s32 	%r416, %r35, %r419;
	setp.eq.s32	%p12, %r269, 60;
	add.s32 	%r275, %r416, 1541459225;
	setp.gt.u32	%p13, %r275, 65535;
	and.pred  	%p14, %p12, %p13;
	@%p14 bra 	BB0_5;

	xor.b32  	%r280, %r22, %r23;
	and.b32  	%r281, %r257, %r280;
	and.b32  	%r282, %r22, %r23;
	xor.b32  	%r283, %r281, %r282;
	add.s32 	%r284, %r256, %r283;
	add.s32 	%r419, %r284, %r35;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r419, %r419, 2;
	 shf.r.clamp.b32    t2, %r419, %r419, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r419, %r419, 22;
	 xor.b32            %r276, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r416, %r416, 6;
	 shf.r.clamp.b32    t2, %r416, %r416, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r416, %r416, 25;
	 xor.b32            %r278, t1, t2;
	}
	// inline asm
	not.b32 	%r285, %r416;
	and.b32  	%r286, %r26, %r285;
	and.b32  	%r287, %r416, %r31;
	xor.b32  	%r288, %r286, %r287;
	add.s32 	%r289, %r19, 2;
	mul.wide.u32 	%rd64, %r289, 4;
	add.s64 	%rd66, %rd59, %rd64;
	add.s32 	%r290, %r288, %r27;
	add.s32 	%r291, %r290, %r278;
	ld.const.u32 	%r292, [%rd66];
	add.s32 	%r293, %r291, %r292;
	ld.shared.u32 	%r294, [%r29+8];
	add.s32 	%r39, %r293, %r294;
	add.s32 	%r27, %r39, %r23;
	setp.eq.s32	%p15, %r289, 60;
	add.s32 	%r295, %r27, 1541459225;
	setp.gt.u32	%p16, %r295, 65535;
	and.pred  	%p17, %p15, %p16;
	@%p17 bra 	BB0_5;

	xor.b32  	%r300, %r257, %r22;
	and.b32  	%r301, %r419, %r300;
	and.b32  	%r302, %r257, %r22;
	xor.b32  	%r303, %r301, %r302;
	add.s32 	%r304, %r276, %r303;
	add.s32 	%r23, %r304, %r39;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r23, %r23, 2;
	 shf.r.clamp.b32    t2, %r23, %r23, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r23, %r23, 22;
	 xor.b32            %r296, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r27, %r27, 6;
	 shf.r.clamp.b32    t2, %r27, %r27, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r27, %r27, 25;
	 xor.b32            %r298, t1, t2;
	}
	// inline asm
	not.b32 	%r305, %r27;
	and.b32  	%r306, %r31, %r305;
	and.b32  	%r307, %r27, %r416;
	xor.b32  	%r308, %r306, %r307;
	add.s32 	%r309, %r19, 3;
	mul.wide.u32 	%rd67, %r309, 4;
	add.s64 	%rd69, %rd59, %rd67;
	add.s32 	%r310, %r308, %r26;
	add.s32 	%r311, %r310, %r298;
	ld.const.u32 	%r312, [%rd69];
	add.s32 	%r313, %r311, %r312;
	ld.shared.u32 	%r314, [%r29+12];
	add.s32 	%r43, %r313, %r314;
	add.s32 	%r26, %r43, %r22;
	setp.eq.s32	%p18, %r309, 60;
	add.s32 	%r315, %r26, 1541459225;
	setp.gt.u32	%p19, %r315, 65535;
	and.pred  	%p20, %p18, %p19;
	@%p20 bra 	BB0_5;

	xor.b32  	%r316, %r419, %r257;
	and.b32  	%r317, %r23, %r316;
	and.b32  	%r318, %r419, %r257;
	xor.b32  	%r319, %r317, %r318;
	add.s32 	%r320, %r296, %r319;
	add.s32 	%r22, %r320, %r43;
	add.s32 	%r19, %r19, 4;
	setp.lt.u32	%p21, %r19, 64;
	@%p21 bra 	BB0_15;

	add.s32 	%r48, %r22, 1779033703;
	st.local.u32 	[%rd4+64], %r48;
	add.s32 	%r323, %r23, -1150833019;
	st.local.u32 	[%rd4+68], %r323;
	add.s32 	%r324, %r419, 1013904242;
	st.local.u32 	[%rd4+72], %r324;
	add.s32 	%r325, %r257, -1521486534;
	st.local.u32 	[%rd4+76], %r325;
	add.s32 	%r326, %r26, 1359893119;
	st.local.u32 	[%rd4+80], %r326;
	add.s32 	%r327, %r27, -1694144372;
	st.local.u32 	[%rd4+84], %r327;
	add.s32 	%r328, %r416, 528734635;
	st.local.u32 	[%rd4+88], %r328;
	add.s32 	%r408, %r31, 1541459225;
	st.local.u32 	[%rd4+92], %r408;
	mov.u32 	%r49, 0;
	mov.u32 	%r425, %r49;
	bra.uni 	BB0_21;

BB0_64:
	mul.wide.s32 	%rd74, %r425, 4;
	add.s64 	%rd75, %rd4, %rd74;
	ld.local.u32 	%r48, [%rd75+64];

BB0_21:
	mov.pred 	%p91, 0;
	mov.u32 	%r426, 1;
	setp.lt.s32	%p23, %r48, 0;
	@%p23 bra 	BB0_22;

	add.s32 	%r427, %r49, 1;
	and.b32  	%r332, %r48, 1073741824;
	mov.u32 	%r426, 2;
	setp.ne.s32	%p25, %r332, 0;
	@%p25 bra 	BB0_23;

	add.s32 	%r427, %r49, 2;
	and.b32  	%r334, %r48, 536870912;
	mov.u32 	%r426, 3;
	setp.ne.s32	%p27, %r334, 0;
	@%p27 bra 	BB0_23;

	add.s32 	%r427, %r49, 3;
	and.b32  	%r336, %r48, 268435456;
	mov.u32 	%r426, 4;
	setp.ne.s32	%p29, %r336, 0;
	@%p29 bra 	BB0_23;

	add.s32 	%r427, %r49, 4;
	and.b32  	%r338, %r48, 134217728;
	mov.u32 	%r426, 5;
	setp.ne.s32	%p31, %r338, 0;
	@%p31 bra 	BB0_23;

	add.s32 	%r427, %r49, 5;
	and.b32  	%r340, %r48, 67108864;
	mov.u32 	%r426, 6;
	setp.ne.s32	%p33, %r340, 0;
	@%p33 bra 	BB0_23;

	add.s32 	%r427, %r49, 6;
	and.b32  	%r342, %r48, 33554432;
	mov.u32 	%r426, 7;
	setp.ne.s32	%p35, %r342, 0;
	@%p35 bra 	BB0_23;

	add.s32 	%r427, %r49, 7;
	and.b32  	%r344, %r48, 16777216;
	mov.u32 	%r426, 8;
	setp.ne.s32	%p37, %r344, 0;
	@%p37 bra 	BB0_23;

	add.s32 	%r427, %r49, 8;
	and.b32  	%r346, %r48, 8388608;
	mov.u32 	%r426, 9;
	setp.ne.s32	%p39, %r346, 0;
	@%p39 bra 	BB0_23;

	add.s32 	%r427, %r49, 9;
	and.b32  	%r348, %r48, 4194304;
	mov.u32 	%r426, 10;
	setp.ne.s32	%p41, %r348, 0;
	@%p41 bra 	BB0_23;

	add.s32 	%r427, %r49, 10;
	and.b32  	%r350, %r48, 2097152;
	mov.u32 	%r426, 11;
	setp.ne.s32	%p43, %r350, 0;
	@%p43 bra 	BB0_23;

	add.s32 	%r427, %r49, 11;
	and.b32  	%r352, %r48, 1048576;
	mov.u32 	%r426, 12;
	setp.ne.s32	%p45, %r352, 0;
	@%p45 bra 	BB0_23;

	add.s32 	%r427, %r49, 12;
	and.b32  	%r354, %r48, 524288;
	mov.u32 	%r426, 13;
	setp.ne.s32	%p47, %r354, 0;
	@%p47 bra 	BB0_23;

	add.s32 	%r427, %r49, 13;
	and.b32  	%r356, %r48, 262144;
	mov.u32 	%r426, 14;
	setp.ne.s32	%p49, %r356, 0;
	@%p49 bra 	BB0_23;

	add.s32 	%r427, %r49, 14;
	and.b32  	%r358, %r48, 131072;
	mov.u32 	%r426, 15;
	setp.ne.s32	%p51, %r358, 0;
	@%p51 bra 	BB0_23;

	add.s32 	%r427, %r49, 15;
	and.b32  	%r360, %r48, 65536;
	mov.u32 	%r426, 16;
	setp.ne.s32	%p53, %r360, 0;
	@%p53 bra 	BB0_23;

	add.s32 	%r427, %r49, 16;
	and.b32  	%r362, %r48, 32768;
	mov.u32 	%r426, 17;
	setp.ne.s32	%p55, %r362, 0;
	@%p55 bra 	BB0_23;

	add.s32 	%r427, %r49, 17;
	and.b32  	%r364, %r48, 16384;
	mov.u32 	%r426, 18;
	setp.ne.s32	%p57, %r364, 0;
	@%p57 bra 	BB0_23;

	add.s32 	%r427, %r49, 18;
	and.b32  	%r366, %r48, 8192;
	mov.u32 	%r426, 19;
	setp.ne.s32	%p59, %r366, 0;
	@%p59 bra 	BB0_23;

	add.s32 	%r427, %r49, 19;
	and.b32  	%r368, %r48, 4096;
	mov.u32 	%r426, 20;
	setp.ne.s32	%p61, %r368, 0;
	@%p61 bra 	BB0_23;

	add.s32 	%r427, %r49, 20;
	and.b32  	%r370, %r48, 2048;
	mov.u32 	%r426, 21;
	setp.ne.s32	%p63, %r370, 0;
	@%p63 bra 	BB0_23;

	add.s32 	%r427, %r49, 21;
	and.b32  	%r372, %r48, 1024;
	mov.u32 	%r426, 22;
	setp.ne.s32	%p65, %r372, 0;
	@%p65 bra 	BB0_23;

	add.s32 	%r427, %r49, 22;
	and.b32  	%r374, %r48, 512;
	mov.u32 	%r426, 23;
	setp.ne.s32	%p67, %r374, 0;
	@%p67 bra 	BB0_23;

	add.s32 	%r427, %r49, 23;
	and.b32  	%r376, %r48, 256;
	mov.u32 	%r426, 24;
	setp.ne.s32	%p69, %r376, 0;
	@%p69 bra 	BB0_23;

	add.s32 	%r427, %r49, 24;
	and.b32  	%r378, %r48, 128;
	mov.u32 	%r426, 25;
	setp.ne.s32	%p71, %r378, 0;
	@%p71 bra 	BB0_23;

	add.s32 	%r427, %r49, 25;
	and.b32  	%r380, %r48, 64;
	mov.u32 	%r426, 26;
	setp.ne.s32	%p73, %r380, 0;
	@%p73 bra 	BB0_23;

	add.s32 	%r427, %r49, 26;
	and.b32  	%r382, %r48, 32;
	mov.u32 	%r426, 27;
	setp.ne.s32	%p75, %r382, 0;
	@%p75 bra 	BB0_23;

	add.s32 	%r427, %r49, 27;
	and.b32  	%r384, %r48, 16;
	mov.u32 	%r426, 28;
	setp.ne.s32	%p77, %r384, 0;
	@%p77 bra 	BB0_23;

	add.s32 	%r427, %r49, 28;
	and.b32  	%r386, %r48, 8;
	mov.u32 	%r426, 29;
	setp.ne.s32	%p79, %r386, 0;
	@%p79 bra 	BB0_23;

	add.s32 	%r427, %r49, 29;
	and.b32  	%r388, %r48, 4;
	mov.u32 	%r426, 30;
	setp.ne.s32	%p81, %r388, 0;
	@%p81 bra 	BB0_23;

	add.s32 	%r427, %r49, 30;
	and.b32  	%r390, %r48, 2;
	mov.u32 	%r426, 31;
	setp.ne.s32	%p83, %r390, 0;
	@%p83 bra 	BB0_23;

	add.s32 	%r427, %r49, 31;
	and.b32  	%r392, %r48, 1;
	setp.eq.b32	%p85, %r392, 1;
	mov.pred 	%p91, -1;
	mov.u32 	%r426, 32;
	@!%p85 bra 	BB0_58;
	bra.uni 	BB0_23;

BB0_58:
	add.s32 	%r49, %r49, 32;
	add.s32 	%r425, %r425, 1;
	setp.lt.s32	%p87, %r425, 8;
	@%p87 bra 	BB0_64;
	bra.uni 	BB0_59;

BB0_22:
	mov.u32 	%r427, %r49;

BB0_23:
	selp.u32	%r393, 1, 0, %p91;
	add.s32 	%r84, %r393, %r425;
	selp.b32	%r85, 0, %r426, %p91;
	mov.u32 	%r394, 24;
	sub.s32 	%r86, %r394, %r85;
	setp.gt.s32	%p86, %r86, -1;
	mul.wide.s32 	%rd70, %r84, 4;
	add.s64 	%rd71, %rd4, %rd70;
	ld.local.u32 	%r87, [%rd71+64];
	@%p86 bra 	BB0_56;
	bra.uni 	BB0_24;

BB0_56:
	shr.u32 	%r403, %r87, %r86;
	and.b32  	%r428, %r403, 255;
	bra.uni 	BB0_57;

BB0_24:
	add.s32 	%r395, %r85, -24;
	shl.b32 	%r396, %r87, %r395;
	add.s32 	%r397, %r84, 1;
	mul.wide.s32 	%rd72, %r397, 4;
	add.s64 	%rd73, %rd4, %rd72;
	ld.local.u32 	%r398, [%rd73+64];
	shr.u32 	%r399, %r398, 1;
	mov.u32 	%r400, 55;
	sub.s32 	%r401, %r400, %r85;
	shr.u32 	%r402, %r399, %r401;
	add.s32 	%r428, %r402, %r396;

BB0_57:
	shl.b32 	%r404, %r427, 8;
	add.s32 	%r49, %r428, %r404;

BB0_59:
	setp.le.u32	%p88, %r49, %r100;
	@%p88 bra 	BB0_5;

	mov.u16 	%rs5, 1;
	st.volatile.global.u8 	[%rd11], %rs5;
	st.volatile.global.u8 	[%rd2], %rs5;
	mov.u32 	%r430, 0;
	mov.u64 	%rd81, %rd4;
	mov.u64 	%rd82, %rd12;

BB0_61:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs6, %rs7, %rs8, %rs9}, [%rd81+32];
	st.global.u8 	[%rd82], %rs9;
	st.global.u8 	[%rd82+1], %rs8;
	st.global.u8 	[%rd82+2], %rs7;
	st.global.u8 	[%rd82+3], %rs6;
	add.s64 	%rd82, %rd82, 4;
	add.s64 	%rd81, %rd81, 4;
	add.s32 	%r430, %r430, 4;
	setp.lt.s32	%p89, %r430, 20;
	@%p89 bra 	BB0_61;

	ld.local.v2.u8 	{%rs14, %rs15}, [%rd4+54];
	ld.local.u8 	%rs18, [%rd4+53];
	st.global.u8 	[%rd12+20], %rs15;
	st.global.u8 	[%rd12+21], %rs14;
	st.global.u8 	[%rd12+22], %rs18;
	mov.u32 	%r431, 0;
	mov.u64 	%rd83, %rd4;
	mov.u64 	%rd84, %rd3;

BB0_63:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs19, %rs20, %rs21, %rs22}, [%rd83+64];
	st.global.u8 	[%rd84], %rs22;
	st.global.u8 	[%rd84+1], %rs21;
	st.global.u8 	[%rd84+2], %rs20;
	st.global.u8 	[%rd84+3], %rs19;
	add.s64 	%rd84, %rd84, 4;
	add.s64 	%rd83, %rd83, 4;
	add.s32 	%r431, %r431, 4;
	setp.lt.s32	%p90, %r431, 32;
	@%p90 bra 	BB0_63;

BB0_5:
	mul.hi.s64 	%rd41, %rd80, -6640827866535438581;
	add.s64 	%rd42, %rd41, %rd80;
	shr.u64 	%rd43, %rd42, 63;
	shr.s64 	%rd44, %rd42, 6;
	add.s64 	%rd45, %rd44, %rd43;
	mul.lo.s64 	%rd46, %rd45, 100;
	sub.s64 	%rd47, %rd80, %rd46;
	setp.ne.s64	%p4, %rd47, 99;
	@%p4 bra 	BB0_10;

	ld.volatile.global.u8 	%rs4, [%rd2];
	setp.eq.s16	%p5, %rs4, 0;
	@%p5 bra 	BB0_10;

	cvt.u32.u64	%r141, %rd5;
	shl.b64 	%rd48, %rd5, 3;
	add.s64 	%rd49, %rd1, %rd48;
	st.global.u64 	[%rd49], %rd80;
	setp.ne.s32	%p6, %r141, 1;
	@%p6 bra 	BB0_9;

	st.volatile.global.u64 	[%rd10], %rd80;

BB0_9:
	ret;
}


