//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};
.const .align 4 .u32 a0 = 1779033703;
.const .align 4 .u32 b0 = -1150833019;
.const .align 4 .u32 c0 = 1013904242;
.const .align 4 .u32 d0 = -1521486534;
.const .align 4 .u32 e0 = 1359893119;
.const .align 4 .u32 f0 = -1694144372;
.const .align 4 .u32 g0 = 528734635;
.const .align 4 .u32 h0 = 1541459225;

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 16 .b8 	__local_depot0[400];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<80>;
	.reg .b16 	%rs<28>;
	.reg .b32 	%r<732>;
	.reg .b64 	%rd<84>;


	mov.u64 	%rd83, __local_depot0;
	cvta.local.u64 	%SP, %rd83;
	ld.param.u64 	%rd39, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0]; // data
	ld.param.u64 	%rd36, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2]; // nonce
	ld.param.u64 	%rd37, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3]; // success
	ld.param.u64 	%rd40, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4]; // stop
	ld.param.u32 	%r119, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6]; // device id
	cvta.to.global.u64 	%rd2, %rd40;
	cvta.to.global.u64 	%rd3, %rd39;
	add.u64 	%rd42, %SP, 0;
	cvta.to.local.u64 	%rd4, %rd42;
	mov.u32 	%r121, %ntid.x;
	mov.u32 	%r122, %ctaid.x;
	mov.u32 	%r123, %tid.x;
	mad.lo.s32 	%r1, %r121, %r122, %r123;
	mov.u32 	%r709, -8; // counter
	mov.u64 	%rd74, %rd3;  // pointer to data[j] j = 0
	mov.u64 	%rd75, %rd4;  // pointer to ctx_data[i] i = 0

BB0_1:
	.pragma "nounroll"; // rd74 -> j, rd75 -> i
	ld.global.u8 	%r124, [%rd74];
	shl.b32 	%r125, %r124, 24;      // data[j] << 24
	ld.global.u8 	%r126, [%rd74+1];
	shl.b32 	%r127, %r126, 16;      // data[j + 1] << 16
	or.b32  	%r128, %r127, %r125;   // (data[j] << 24) | (data[j + 1] << 16)
	ld.global.u8 	%rs3, [%rd74+2];
	mul.wide.u16 	%r129, %rs3, 256;  // (data[j + 2] << 8)
	or.b32  	%r130, %r128, %r129;
	ld.global.u8 	%r131, [%rd74+3];
	or.b32  	%r132, %r130, %r131;
	st.local.u32 	[%rd75], %r132;    // ctx_data[i] =
	add.s64 	%rd75, %rd75, 4;
	add.s64 	%rd74, %rd74, 4;
	add.s32 	%r709, %r709, 1;
	setp.ne.s32	%p1, %r709, 0;
	@%p1 bra 	BB0_1;

	cvta.to.global.u64 	%rd10, %rd37;
	cvta.to.global.u64 	%rd11, %rd36;
	add.u64 	%rd44, %SP, 112;
	cvta.to.local.u64 	%rd12, %rd44;
	mov.u64 	%rd77, -8;  // i -?
	mov.u32 	%r710, -5; counter
	mov.u64 	%rd76, %rd11; // pointer to nonce[j] j = 0

BB0_3:
	.pragma "nounroll"; // 
	ld.global.u8 	%r134, [%rd76];
	shl.b32 	%r135, %r134, 24;      // (nonce[j] << 24)
	ld.global.u8 	%r136, [%rd76+1];
	shl.b32 	%r137, %r136, 16;      // (nonce[j + 1] << 16)
	or.b32  	%r138, %r137, %r135;
	ld.global.u8 	%rs4, [%rd76+2];
	mul.wide.u16 	%r139, %rs4, 256;  // (nonce[j + 2] << 8)
	or.b32  	%r140, %r138, %r139;
	ld.global.u8 	%r141, [%rd76+3];
	or.b32  	%r142, %r140, %r141;   //  
	shl.b64 	%rd46, %rd77, 2;
	sub.s64 	%rd47, %rd4, %rd46;    // pointer to stx_data - (-8 * 4)
	st.local.u32 	[%rd47], %r142;    // stx_data[i] =
	add.s64 	%rd77, %rd77, -1;      // i = i - 1
	add.s64 	%rd76, %rd76, 4;       // j = j + 4
	add.s32 	%r710, %r710, 1;       // counter++
	setp.ne.s32	%p2, %r710, 0;
	@%p2 bra 	BB0_3;

	ld.global.u8 	%r175, [%rd11+20];
	shl.b32 	%r176, %r175, 24;      // (nonce[20] << 24)
	ld.global.u8 	%r177, [%rd11+21];
	shl.b32 	%r178, %r177, 16;      // (nonce[21] << 16)
	ld.global.u8 	%rs5, [%rd11+22];
	mul.wide.u16 	%r179, %rs5, 256;  // (nonce[22] << 8)
	or.b32  	%r180, %r176, %r178;
	or.b32  	%r181, %r180, %r179;
	or.b32  	%r182, %r181, 128;
	st.local.u32 	[%rd4+52], %r182; // ctx_data[13] = 
	mov.u32 	%r146, 0;
	st.local.u32 	[%rd4+56], %r146; // ctx_data[14] =
	mov.u32 	%r150, 440;
	st.local.u32 	[%rd4+60], %r150; // ctx_data[15] =
	ld.local.u32 	%r183, [%rd4+40];
	xor.b32  	%r172, %r183, %r1;
	ld.local.u32 	%r184, [%rd4+44];
	ld.local.u32 	%r170, [%rd4+36]; = ctx_data[9]
	ld.local.u32 	%r185, [%rd4];
	st.local.u32 	[%rd4+40], %r172;  // ctx_data[10] =
	xor.b32  	%r174, %r184, %r119;
	st.local.u32 	[%rd4+44], %r174; // ctx_data[11] =
	ld.local.u32 	%r144, [%rd4+4];  //  = ctx_data[1]
//************************ i = 16 *********************************
	// inline asm sig0(ctx_data[i - 15], res0) :: i = 16
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r144, %r144, 7;
	 shf.r.clamp.b32    t2, %r144, %r144, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r144, 3;
	 xor.b32            %r143, t1, t2;
	}
	// inline asm
	// inline asm sig1(ctx_data[i - 2], res1)
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r146, %r146, 17;
	 shf.r.clamp.b32    t2, %r146, %r146, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r146, 10;
	 xor.b32            %r145, t1, t2;
	}
	// inline asm
	add.s32 	%r186, %r145, %r143; = res1 + res0
	add.s32 	%r187, %r186, %r170; =  + ctx_data[9]
	add.s32 	%r154, %r187, %r185; =  + ctx_data[0]
	st.local.u32 	[%rd4+64], %r154;  // ctx_data[16] = 
	ld.local.u32 	%r148, [%rd4+8]; = ctx_data[2]
//************************ i = 17 *********************************
	// inline asm  sig0(ctx_data[i - 15], res0)
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r148, %r148, 7;
	 shf.r.clamp.b32    t2, %r148, %r148, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r148, 3;
	 xor.b32            %r147, t1, t2;
	}
	// inline asm
	// inline asm  sig1(ctx_data[i - 2], res1)
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r150, %r150, 17;
	 shf.r.clamp.b32    t2, %r150, %r150, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r150, 10;
	 xor.b32            %r149, t1, t2;
	}
	// inline asm
	add.s32 	%r188, %r149, %r147;
	add.s32 	%r189, %r188, %r172;
	add.s32 	%r158, %r189, %r144;
	st.local.u32 	[%rd4+68], %r158;
	ld.local.u32 	%r152, [%rd4+12];
//************************ i = 18 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r152, %r152, 7;
	 shf.r.clamp.b32    t2, %r152, %r152, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r152, 3;
	 xor.b32            %r151, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r154, %r154, 17;
	 shf.r.clamp.b32    t2, %r154, %r154, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r154, 10;
	 xor.b32            %r153, t1, t2;
	}
	// inline asm
	add.s32 	%r190, %r153, %r151;
	add.s32 	%r191, %r190, %r174;
	add.s32 	%r162, %r191, %r148;
	ld.local.u32 	%r192, [%rd4+48];
	st.local.u32 	[%rd4+72], %r162;
	ld.local.u32 	%r156, [%rd4+16];
//************************ i = 19 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r156, %r156, 7;
	 shf.r.clamp.b32    t2, %r156, %r156, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r156, 3;
	 xor.b32            %r155, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r158, %r158, 17;
	 shf.r.clamp.b32    t2, %r158, %r158, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r158, 10;
	 xor.b32            %r157, t1, t2;
	}
	// inline asm
	add.s32 	%r193, %r157, %r155;
	add.s32 	%r194, %r193, %r192;
	add.s32 	%r195, %r194, %r152;
	st.local.u32 	[%rd4+76], %r195;
	ld.local.u32 	%r160, [%rd4+20];
//************************ i = 20 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r160, %r160, 7;
	 shf.r.clamp.b32    t2, %r160, %r160, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r160, 3;
	 xor.b32            %r159, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r162, %r162, 17;
	 shf.r.clamp.b32    t2, %r162, %r162, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r162, 10;
	 xor.b32            %r161, t1, t2;
	}
	// inline asm
	add.s32 	%r196, %r161, %r159;
	add.s32 	%r197, %r196, %r182;
	add.s32 	%r198, %r197, %r156;
	st.local.u32 	[%rd4+80], %r198;
	ld.local.u32 	%r164, [%rd4+24];
//************************ i = 21 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r164, %r164, 7;
	 shf.r.clamp.b32    t2, %r164, %r164, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r164, 3;
	 xor.b32            %r163, t1, t2;
	}
	// inline asm
	add.s32 	%r199, %r163, %r160;
	st.local.u32 	[%rd4+84], %r199;
	ld.local.u32 	%r166, [%rd4+28];
//************************ i = 22 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r166, %r166, 7;
	 shf.r.clamp.b32    t2, %r166, %r166, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r166, 3;
	 xor.b32            %r165, t1, t2;
	}
	// inline asm
	add.s32 	%r200, %r165, %r164;
	add.s32 	%r201, %r200, 440;
	st.local.u32 	[%rd4+88], %r201;
	ld.local.u32 	%r168, [%rd4+32];
//************************ i = 23 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r168, %r168, 7;
	 shf.r.clamp.b32    t2, %r168, %r168, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r168, 3;
	 xor.b32            %r167, t1, t2;
	}
	// inline asm
	add.s32 	%r202, %r154, %r167;
	add.s32 	%r203, %r202, %r166;
	st.local.u32 	[%rd4+92], %r203;
//************************ i = 24 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r170, %r170, 7;
	 shf.r.clamp.b32    t2, %r170, %r170, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r170, 3;
	 xor.b32            %r169, t1, t2;
	}
	// inline asm
	add.s32 	%r204, %r158, %r169;
	add.s32 	%r205, %r204, %r168;
	st.local.u32 	[%rd4+96], %r205;
//************************ i = 25 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r172, %r172, 7;
	 shf.r.clamp.b32    t2, %r172, %r172, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r172, 3;
	 xor.b32            %r171, t1, t2;
	}
	// inline asm
	add.s32 	%r206, %r162, %r171;
	add.s32 	%r207, %r206, %r170;
	st.local.u32 	[%rd4+100], %r207;
//************************ i = 26 *********************************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r174, %r174, 7;
	 shf.r.clamp.b32    t2, %r174, %r174, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r174, 3;
	 xor.b32            %r173, t1, t2;
	}
	// inline asm
	add.s32 	%r208, %r195, %r173;
	add.s32 	%r209, %r208, %r172;
	st.local.u32 	[%rd4+104], %r209;     // ctx_data[26] = 
	mov.u64 	%rd78, 0;                  // r = 0
	bra.uni 	BB0_5;

BB0_10:
	ld.local.u32 	%r210, [%rd4+48];      // ctx_data[12]
	add.s32 	%r6, %r210, 1;             // ++ctx_data[12]
	ld.local.u32 	%r712, [%rd4+52];      // ctx_data[13]
	st.local.u32 	[%rd4+48], %r6;        // ctx_data[12]
	setp.eq.s32	%p6, %r6, 0;
	@%p6 bra 	BB0_12;
	bra.uni 	BB0_11;

BB0_12:
	add.s32 	%r712, %r712, 16;
	st.local.u32 	[%rd4+52], %r712;      // stx_data[13]
	ld.local.u32 	%r211, [%rd4+80];      // stx_data[20]
	add.s32 	%r711, %r211, 16;
	st.local.u32 	[%rd4+80], %r711;      // stx_data[20]
	bra.uni 	BB0_13;

BB0_11:
	ld.local.u32 	%r711, [%rd4+80];      // stx_data[20]

BB0_13:
	mov.u32 	%r715, 0;
	ld.local.u32 	%r381, [%rd4+76];
	ld.local.u32 	%r382, [%rd4+104];
	ld.local.u32 	%r383, [%rd4+12];
	ld.local.u32 	%r384, [%rd4+8];
	ld.local.u32 	%r385, [%rd4+4];
	ld.local.u32 	%r713, [%rd4];
	ld.local.u32 	%r386, [%rd4+28];
	ld.local.u32 	%r387, [%rd4+24];
	ld.local.u32 	%r388, [%rd4+20];
	ld.local.u32 	%r389, [%rd4+16];
	ld.local.u32 	%r390, [%rd4+44];
	ld.local.u32 	%r391, [%rd4+40];
	ld.local.u32 	%r392, [%rd4+36];
	ld.local.u32 	%r393, [%rd4+32];
	ld.local.u32 	%r237, [%rd4+60];
	ld.local.u32 	%r233, [%rd4+56];
	ld.local.u32 	%r249, [%rd4+72];
	ld.local.u32 	%r245, [%rd4+68];
	ld.local.u32 	%r241, [%rd4+64];
	add.s32 	%r253, %r381, 1;
	st.local.u32 	[%rd4+76], %r253;   // ctx_data[19]++
	add.s32 	%r394, %r382, 1;
	st.local.u32 	[%rd4+104], %r394;  // ctx_data[26]++
	st.local.v4.u32 	[%rd12], {%r713, %r385, %r384, %r383};    // m[0..3] =
	st.local.v4.u32 	[%rd12+16], {%r389, %r388, %r387, %r386}; // m[4..7] =
	st.local.v4.u32 	[%rd12+32], {%r393, %r392, %r391, %r390}; // m[8..11] =
	add.s32 	%r700, %r210, 1;                                  // ctx_data[12] = calculate again
	st.local.v4.u32 	[%rd12+48], {%r700, %r712, %r233, %r237};  // m[12..15] =
	st.local.v4.u32 	[%rd12+64], {%r241, %r245, %r249, %r253};  // m[16..19]
	st.local.u32 	[%rd12+80], %r711; // m[20] = 
	ld.local.u32 	%r395, [%rd4+84];  // ctx_data[21]
	ld.local.u32 	%r396, [%rd4+88];  // ctx_data[22]
	ld.local.u32 	%r397, [%rd4+92];  // ctx_data[23]
	ld.local.u32 	%r398, [%rd4+96];  // ctx_data[24]
	ld.local.u32 	%r399, [%rd4+100]; // ctx_data[25]
//**************************** i = 21, m[19]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 17;
	 shf.r.clamp.b32    t2, %r253, %r253, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 10;
	 xor.b32            %r212, t1, t2;
	}
	// inline asm
	add.s32 	%r261, %r395, %r212;
	st.local.u32 	[%rd12+84], %r261; // m[21] =
//**************************** i = 22, m[20]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r711, %r711, 17;
	 shf.r.clamp.b32    t2, %r711, %r711, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r711, 10;
	 xor.b32            %r214, t1, t2;
	}
	// inline asm
	add.s32 	%r265, %r396, %r214;
	st.local.u32 	[%rd12+88], %r265; // m[22] =
//**************************** i = 23, m[21]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r261, %r261, 17;
	 shf.r.clamp.b32    t2, %r261, %r261, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r261, 10;
	 xor.b32            %r216, t1, t2;
	}
	// inline asm
	add.s32 	%r269, %r397, %r216;
	st.local.u32 	[%rd12+92], %r269; // m[23] = 
//**************************** i = 24, m[22]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 17;
	 shf.r.clamp.b32    t2, %r265, %r265, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 10;
	 xor.b32            %r218, t1, t2;
	}
	// inline asm
	add.s32 	%r273, %r398, %r218;
	st.local.u32 	[%rd12+96], %r273;  // m[24] = 
//**************************** i = 25, m[23]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r269, %r269, 17;
	 shf.r.clamp.b32    t2, %r269, %r269, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r269, 10;
	 xor.b32            %r220, t1, t2;
	}
	// inline asm
	add.s32 	%r277, %r399, %r220;
	st.local.u32 	[%rd12+100], %r277;  // m[25] = 
//**************************** i = 26, m[24]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r273, %r273, 17;
	 shf.r.clamp.b32    t2, %r273, %r273, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r273, 10;
	 xor.b32            %r222, t1, t2;
	}
	// inline asm
	add.s32 	%r281, %r394, %r222;
	st.local.u32 	[%rd12+104], %r281;  // m[26] =
//**************************** i = 27, m[12] & m[25]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r6, %r6, 7;
	 shf.r.clamp.b32    t2, %r6, %r6, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r6, 3;
	 xor.b32            %r224, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 17;
	 shf.r.clamp.b32    t2, %r277, %r277, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 10;
	 xor.b32            %r226, t1, t2;
	}
	// inline asm
	add.s32 	%r400, %r226, %r224;
	add.s32 	%r401, %r400, %r711;
	add.s32 	%r285, %r401, %r390;
	st.local.u32 	[%rd12+108], %r285;
//**************************** i = 28, m[13] & m[26]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r712, %r712, 7;
	 shf.r.clamp.b32    t2, %r712, %r712, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r712, 3;
	 xor.b32            %r228, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r281, %r281, 17;
	 shf.r.clamp.b32    t2, %r281, %r281, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r281, 10;
	 xor.b32            %r230, t1, t2;
	}
	// inline asm
	add.s32 	%r402, %r230, %r228;
	add.s32 	%r403, %r402, %r261;
	add.s32 	%r289, %r403, %r6;
	st.local.u32 	[%rd12+112], %r289;
//**************************** i = 29, m[14] & m[27]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r233, %r233, 7;
	 shf.r.clamp.b32    t2, %r233, %r233, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r233, 3;
	 xor.b32            %r232, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r285, %r285, 17;
	 shf.r.clamp.b32    t2, %r285, %r285, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r285, 10;
	 xor.b32            %r234, t1, t2;
	}
	// inline asm
	add.s32 	%r404, %r234, %r232;
	add.s32 	%r405, %r404, %r265;
	add.s32 	%r293, %r405, %r712;
	st.local.u32 	[%rd12+116], %r293;
//**************************** i = 30, m[15] & m[28]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r237, %r237, 7;
	 shf.r.clamp.b32    t2, %r237, %r237, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 3;
	 xor.b32            %r236, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 17;
	 shf.r.clamp.b32    t2, %r289, %r289, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 10;
	 xor.b32            %r238, t1, t2;
	}
	// inline asm
	add.s32 	%r406, %r238, %r236;
	add.s32 	%r407, %r406, %r269;
	add.s32 	%r297, %r407, %r233;
	st.local.u32 	[%rd12+120], %r297;
//**************************** i = 31, m[16] & m[29]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r241, %r241, 7;
	 shf.r.clamp.b32    t2, %r241, %r241, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 3;
	 xor.b32            %r240, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r293, %r293, 17;
	 shf.r.clamp.b32    t2, %r293, %r293, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r293, 10;
	 xor.b32            %r242, t1, t2;
	}
	// inline asm
	add.s32 	%r408, %r242, %r240;
	add.s32 	%r409, %r408, %r273;
	add.s32 	%r301, %r409, %r237;
	st.local.u32 	[%rd12+124], %r301;
//**************************** i = 32, m[17] & m[30]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r245, %r245, 7;
	 shf.r.clamp.b32    t2, %r245, %r245, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r245, 3;
	 xor.b32            %r244, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r297, %r297, 17;
	 shf.r.clamp.b32    t2, %r297, %r297, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r297, 10;
	 xor.b32            %r246, t1, t2;
	}
	// inline asm
	add.s32 	%r410, %r246, %r244;
	add.s32 	%r411, %r410, %r277;
	add.s32 	%r305, %r411, %r241;
	st.local.u32 	[%rd12+128], %r305;
//**************************** i = 33, m[18] & m[31]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r249, %r249, 7;
	 shf.r.clamp.b32    t2, %r249, %r249, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r249, 3;
	 xor.b32            %r248, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 17;
	 shf.r.clamp.b32    t2, %r301, %r301, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 10;
	 xor.b32            %r250, t1, t2;
	}
	// inline asm
	add.s32 	%r412, %r250, %r248;
	add.s32 	%r413, %r412, %r281;
	add.s32 	%r309, %r413, %r245;
	st.local.u32 	[%rd12+132], %r309;
//**************************** i = 34, m[19] & m[32]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 7;
	 shf.r.clamp.b32    t2, %r253, %r253, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 3;
	 xor.b32            %r252, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r305, %r305, 17;
	 shf.r.clamp.b32    t2, %r305, %r305, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r305, 10;
	 xor.b32            %r254, t1, t2;
	}
	// inline asm
	add.s32 	%r414, %r254, %r252;
	add.s32 	%r415, %r414, %r285;
	add.s32 	%r313, %r415, %r249;
	st.local.u32 	[%rd12+136], %r313;
//**************************** i = 35, m[20] & m[33]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r711, %r711, 7;
	 shf.r.clamp.b32    t2, %r711, %r711, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r711, 3;
	 xor.b32            %r256, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r309, %r309, 17;
	 shf.r.clamp.b32    t2, %r309, %r309, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r309, 10;
	 xor.b32            %r258, t1, t2;
	}
	// inline asm
	add.s32 	%r416, %r258, %r256;
	add.s32 	%r417, %r416, %r289;
	add.s32 	%r317, %r417, %r253;
	st.local.u32 	[%rd12+140], %r317;
//**************************** i = 36, m[21] & m[34]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r261, %r261, 7;
	 shf.r.clamp.b32    t2, %r261, %r261, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r261, 3;
	 xor.b32            %r260, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r313, %r313, 17;
	 shf.r.clamp.b32    t2, %r313, %r313, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r313, 10;
	 xor.b32            %r262, t1, t2;
	}
	// inline asm
	add.s32 	%r418, %r262, %r260;
	add.s32 	%r419, %r418, %r293;
	add.s32 	%r321, %r419, %r711;
	st.local.u32 	[%rd12+144], %r321;
//**************************** i = 37, m[22] & m[35]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 7;
	 shf.r.clamp.b32    t2, %r265, %r265, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 3;
	 xor.b32            %r264, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r317, %r317, 17;
	 shf.r.clamp.b32    t2, %r317, %r317, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r317, 10;
	 xor.b32            %r266, t1, t2;
	}
	// inline asm
	add.s32 	%r420, %r266, %r264;
	add.s32 	%r421, %r420, %r297;
	add.s32 	%r325, %r421, %r261;
	st.local.u32 	[%rd12+148], %r325;
//**************************** i = 38, m[23] & m[36]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r269, %r269, 7;
	 shf.r.clamp.b32    t2, %r269, %r269, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r269, 3;
	 xor.b32            %r268, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r321, %r321, 17;
	 shf.r.clamp.b32    t2, %r321, %r321, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r321, 10;
	 xor.b32            %r270, t1, t2;
	}
	// inline asm
	add.s32 	%r422, %r270, %r268;
	add.s32 	%r423, %r422, %r301;
	add.s32 	%r329, %r423, %r265;
	st.local.u32 	[%rd12+152], %r329;
//**************************** i = 39, m[24] & m[37]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r273, %r273, 7;
	 shf.r.clamp.b32    t2, %r273, %r273, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r273, 3;
	 xor.b32            %r272, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r325, %r325, 17;
	 shf.r.clamp.b32    t2, %r325, %r325, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r325, 10;
	 xor.b32            %r274, t1, t2;
	}
	// inline asm
	add.s32 	%r424, %r274, %r272;
	add.s32 	%r425, %r424, %r305;
	add.s32 	%r333, %r425, %r269;
	st.local.u32 	[%rd12+156], %r333;
//**************************** i = 40, m[25] & m[38]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 7;
	 shf.r.clamp.b32    t2, %r277, %r277, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 3;
	 xor.b32            %r276, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r329, %r329, 17;
	 shf.r.clamp.b32    t2, %r329, %r329, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r329, 10;
	 xor.b32            %r278, t1, t2;
	}
	// inline asm
	add.s32 	%r426, %r278, %r276;
	add.s32 	%r427, %r426, %r309;
	add.s32 	%r337, %r427, %r273;
	st.local.u32 	[%rd12+160], %r337;
//**************************** i = 41, m[26] & m[39]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r281, %r281, 7;
	 shf.r.clamp.b32    t2, %r281, %r281, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r281, 3;
	 xor.b32            %r280, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r333, %r333, 17;
	 shf.r.clamp.b32    t2, %r333, %r333, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r333, 10;
	 xor.b32            %r282, t1, t2;
	}
	// inline asm
	add.s32 	%r428, %r282, %r280;
	add.s32 	%r429, %r428, %r313;
	add.s32 	%r341, %r429, %r277;
	st.local.u32 	[%rd12+164], %r341;
//**************************** i = 42, m[27] & m[40]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r285, %r285, 7;
	 shf.r.clamp.b32    t2, %r285, %r285, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r285, 3;
	 xor.b32            %r284, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r337, %r337, 17;
	 shf.r.clamp.b32    t2, %r337, %r337, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r337, 10;
	 xor.b32            %r286, t1, t2;
	}
	// inline asm
	add.s32 	%r430, %r286, %r284;
	add.s32 	%r431, %r430, %r317;
	add.s32 	%r345, %r431, %r281;
	st.local.u32 	[%rd12+168], %r345;
//**************************** i = 43, m[28] & m[41]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 7;
	 shf.r.clamp.b32    t2, %r289, %r289, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 3;
	 xor.b32            %r288, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r341, %r341, 17;
	 shf.r.clamp.b32    t2, %r341, %r341, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r341, 10;
	 xor.b32            %r290, t1, t2;
	}
	// inline asm
	add.s32 	%r432, %r290, %r288;
	add.s32 	%r433, %r432, %r321;
	add.s32 	%r349, %r433, %r285;
	st.local.u32 	[%rd12+172], %r349;
//**************************** i = 44, m[29] & m[42]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r293, %r293, 7;
	 shf.r.clamp.b32    t2, %r293, %r293, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r293, 3;
	 xor.b32            %r292, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r345, %r345, 17;
	 shf.r.clamp.b32    t2, %r345, %r345, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r345, 10;
	 xor.b32            %r294, t1, t2;
	}
	// inline asm
	add.s32 	%r434, %r294, %r292;
	add.s32 	%r435, %r434, %r325;
	add.s32 	%r353, %r435, %r289;
	st.local.u32 	[%rd12+176], %r353;
//**************************** i = 45, m[30] & m[43]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r297, %r297, 7;
	 shf.r.clamp.b32    t2, %r297, %r297, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r297, 3;
	 xor.b32            %r296, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r349, %r349, 17;
	 shf.r.clamp.b32    t2, %r349, %r349, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r349, 10;
	 xor.b32            %r298, t1, t2;
	}
	// inline asm
	add.s32 	%r436, %r298, %r296;
	add.s32 	%r437, %r436, %r329;
	add.s32 	%r357, %r437, %r293;
	st.local.u32 	[%rd12+180], %r357;
//**************************** i = 46, m[31] & m[44]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 7;
	 shf.r.clamp.b32    t2, %r301, %r301, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 3;
	 xor.b32            %r300, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r353, %r353, 17;
	 shf.r.clamp.b32    t2, %r353, %r353, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r353, 10;
	 xor.b32            %r302, t1, t2;
	}
	// inline asm
	add.s32 	%r438, %r302, %r300;
	add.s32 	%r439, %r438, %r333;
	add.s32 	%r361, %r439, %r297;
	st.local.u32 	[%rd12+184], %r361;
//**************************** i = 47, m[32] & m[45]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r305, %r305, 7;
	 shf.r.clamp.b32    t2, %r305, %r305, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r305, 3;
	 xor.b32            %r304, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r357, %r357, 17;
	 shf.r.clamp.b32    t2, %r357, %r357, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r357, 10;
	 xor.b32            %r306, t1, t2;
	}
	// inline asm
	add.s32 	%r440, %r306, %r304;
	add.s32 	%r441, %r440, %r337;
	add.s32 	%r365, %r441, %r301;
	st.local.u32 	[%rd12+188], %r365;
//**************************** i = 48, m[33] & m[46]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r309, %r309, 7;
	 shf.r.clamp.b32    t2, %r309, %r309, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r309, 3;
	 xor.b32            %r308, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r361, %r361, 17;
	 shf.r.clamp.b32    t2, %r361, %r361, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r361, 10;
	 xor.b32            %r310, t1, t2;
	}
	// inline asm
	add.s32 	%r442, %r310, %r308;
	add.s32 	%r443, %r442, %r341;
	add.s32 	%r369, %r443, %r305;
	st.local.u32 	[%rd12+192], %r369;
//**************************** i = 49, m[34] & m[47]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r313, %r313, 7;
	 shf.r.clamp.b32    t2, %r313, %r313, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r313, 3;
	 xor.b32            %r312, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r365, %r365, 17;
	 shf.r.clamp.b32    t2, %r365, %r365, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r365, 10;
	 xor.b32            %r314, t1, t2;
	}
	// inline asm
	add.s32 	%r444, %r314, %r312;
	add.s32 	%r445, %r444, %r345;
	add.s32 	%r323, %r445, %r309;
	st.local.u32 	[%rd12+196], %r323;
//**************************** i = 50, m[35] & m[48]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r317, %r317, 7;
	 shf.r.clamp.b32    t2, %r317, %r317, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r317, 3;
	 xor.b32            %r316, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r369, %r369, 17;
	 shf.r.clamp.b32    t2, %r369, %r369, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r369, 10;
	 xor.b32            %r318, t1, t2;
	}
	// inline asm
	add.s32 	%r446, %r318, %r316;
	add.s32 	%r447, %r446, %r349;
	add.s32 	%r327, %r447, %r313;
	st.local.u32 	[%rd12+200], %r327;
//**************************** i = 51, m[36] & m[49]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r321, %r321, 7;
	 shf.r.clamp.b32    t2, %r321, %r321, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r321, 3;
	 xor.b32            %r320, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r323, %r323, 17;
	 shf.r.clamp.b32    t2, %r323, %r323, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r323, 10;
	 xor.b32            %r322, t1, t2;
	}
	// inline asm
	add.s32 	%r448, %r322, %r320;
	add.s32 	%r449, %r448, %r353;
	add.s32 	%r331, %r449, %r317;
	st.local.u32 	[%rd12+204], %r331;
//**************************** i = 52, m[37] & m[50]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r325, %r325, 7;
	 shf.r.clamp.b32    t2, %r325, %r325, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r325, 3;
	 xor.b32            %r324, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r327, %r327, 17;
	 shf.r.clamp.b32    t2, %r327, %r327, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r327, 10;
	 xor.b32            %r326, t1, t2;
	}
	// inline asm
	add.s32 	%r450, %r326, %r324;
	add.s32 	%r451, %r450, %r357;
	add.s32 	%r335, %r451, %r321;
	st.local.u32 	[%rd12+208], %r335;
//**************************** i = 53, m[38] & m[51]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r329, %r329, 7;
	 shf.r.clamp.b32    t2, %r329, %r329, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r329, 3;
	 xor.b32            %r328, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r331, %r331, 17;
	 shf.r.clamp.b32    t2, %r331, %r331, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r331, 10;
	 xor.b32            %r330, t1, t2;
	}
//**************************** i = 54, m[39] & m[52]  *********************
	// inline asm
	add.s32 	%r452, %r330, %r328;
	add.s32 	%r453, %r452, %r361;
	add.s32 	%r339, %r453, %r325;
	st.local.u32 	[%rd12+212], %r339;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r333, %r333, 7;
	 shf.r.clamp.b32    t2, %r333, %r333, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r333, 3;
	 xor.b32            %r332, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r335, %r335, 17;
	 shf.r.clamp.b32    t2, %r335, %r335, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r335, 10;
	 xor.b32            %r334, t1, t2;
	}
	// inline asm
	add.s32 	%r454, %r334, %r332;
	add.s32 	%r455, %r454, %r365;
	add.s32 	%r343, %r455, %r329;
	st.local.u32 	[%rd12+216], %r343;
//**************************** i = 55, m[40] & m[53]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r337, %r337, 7;
	 shf.r.clamp.b32    t2, %r337, %r337, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r337, 3;
	 xor.b32            %r336, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r339, %r339, 17;
	 shf.r.clamp.b32    t2, %r339, %r339, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r339, 10;
	 xor.b32            %r338, t1, t2;
	}
	// inline asm
	add.s32 	%r456, %r338, %r336;
	add.s32 	%r457, %r456, %r369;
	add.s32 	%r347, %r457, %r333;
	st.local.u32 	[%rd12+220], %r347;
//**************************** i = 56, m[41] & m[54]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r341, %r341, 7;
	 shf.r.clamp.b32    t2, %r341, %r341, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r341, 3;
	 xor.b32            %r340, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r343, %r343, 17;
	 shf.r.clamp.b32    t2, %r343, %r343, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r343, 10;
	 xor.b32            %r342, t1, t2;
	}
	// inline asm
	add.s32 	%r458, %r342, %r340;
	add.s32 	%r459, %r458, %r323;
	add.s32 	%r351, %r459, %r337;
	st.local.u32 	[%rd12+224], %r351;
//**************************** i = 57, m[42] & m[55]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r345, %r345, 7;
	 shf.r.clamp.b32    t2, %r345, %r345, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r345, 3;
	 xor.b32            %r344, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r347, %r347, 17;
	 shf.r.clamp.b32    t2, %r347, %r347, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r347, 10;
	 xor.b32            %r346, t1, t2;
	}
	// inline asm
	add.s32 	%r460, %r346, %r344;
	add.s32 	%r461, %r460, %r327;
	add.s32 	%r355, %r461, %r341;
	st.local.u32 	[%rd12+228], %r355;
//**************************** i = 58, m[43] & m[56]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r349, %r349, 7;
	 shf.r.clamp.b32    t2, %r349, %r349, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r349, 3;
	 xor.b32            %r348, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r351, %r351, 17;
	 shf.r.clamp.b32    t2, %r351, %r351, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r351, 10;
	 xor.b32            %r350, t1, t2;
	}
	// inline asm
	add.s32 	%r462, %r350, %r348;
	add.s32 	%r463, %r462, %r331;
	add.s32 	%r359, %r463, %r345;
	st.local.u32 	[%rd12+232], %r359;
//**************************** i = 59, m[44] & m[57]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r353, %r353, 7;
	 shf.r.clamp.b32    t2, %r353, %r353, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r353, 3;
	 xor.b32            %r352, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r355, %r355, 17;
	 shf.r.clamp.b32    t2, %r355, %r355, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r355, 10;
	 xor.b32            %r354, t1, t2;
	}
	// inline asm
	add.s32 	%r464, %r354, %r352;
	add.s32 	%r465, %r464, %r335;
	add.s32 	%r363, %r465, %r349;
	st.local.u32 	[%rd12+236], %r363;
//**************************** i = 60, m[45] & m[58]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r357, %r357, 7;
	 shf.r.clamp.b32    t2, %r357, %r357, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r357, 3;
	 xor.b32            %r356, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r359, %r359, 17;
	 shf.r.clamp.b32    t2, %r359, %r359, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r359, 10;
	 xor.b32            %r358, t1, t2;
	}
	// inline asm
	add.s32 	%r466, %r358, %r356;
	add.s32 	%r467, %r466, %r339;
	add.s32 	%r367, %r467, %r353;
	st.local.u32 	[%rd12+240], %r367;
//**************************** i = 61, m[46] & m[59]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r361, %r361, 7;
	 shf.r.clamp.b32    t2, %r361, %r361, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r361, 3;
	 xor.b32            %r360, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r363, %r363, 17;
	 shf.r.clamp.b32    t2, %r363, %r363, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r363, 10;
	 xor.b32            %r362, t1, t2;
	}
	// inline asm
	add.s32 	%r468, %r362, %r360;
	add.s32 	%r469, %r468, %r343;
	add.s32 	%r371, %r469, %r357;
	st.local.u32 	[%rd12+244], %r371;
//**************************** i = 62, m[47] & m[60]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r365, %r365, 7;
	 shf.r.clamp.b32    t2, %r365, %r365, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r365, 3;
	 xor.b32            %r364, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r367, %r367, 17;
	 shf.r.clamp.b32    t2, %r367, %r367, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r367, 10;
	 xor.b32            %r366, t1, t2;
	}
	// inline asm
	add.s32 	%r470, %r366, %r364;
	add.s32 	%r471, %r470, %r347;
	add.s32 	%r472, %r471, %r361;
	st.local.u32 	[%rd12+248], %r472;
//**************************** i = 63, m[48] & m[61]  *********************
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r369, %r369, 7;
	 shf.r.clamp.b32    t2, %r369, %r369, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r369, 3;
	 xor.b32            %r368, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r371, %r371, 17;
	 shf.r.clamp.b32    t2, %r371, %r371, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r371, 10;
	 xor.b32            %r370, t1, t2;
	}
	// inline asm
	add.s32 	%r473, %r370, %r368;
	add.s32 	%r474, %r473, %r351;
	add.s32 	%r475, %r474, %r365;
	st.local.u32 	[%rd12+252], %r475;

	mov.u32 	%ff, -1694144372;
	mov.u32 	%ee, 1359893119;
	mov.u32 	%dd, -1521486534;
	mov.u32 	%cc, 1013904242;
	mov.u32 	%bb, -1150833019;
	mov.u32 	%aa, 1779033703;
	mov.u32 	%gg, 528734635;
	mov.u32 	%hh, 1541459225;
	bra.uni 	BB0_14;

BB0_15:
	ld.local.u32 	%r713, [%rd26+12];  // rd26 = ???

BB0_14:
============================= i = 0  ============================
	// inline asm ep0
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %aa, %aa, 2;
	 shf.r.clamp.b32    t2, %aa, %aa, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %aa, %aa, 22;
	 xor.b32            %r476, t1, t2;
	}
	// inline asm
	// inline asm ep1
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %ee, %ee, 6;
	 shf.r.clamp.b32    t2, %ee, %ee, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %ee, %ee, 25;
	 xor.b32            %r478, t1, t2;
	}
	// inline asm
	not.b32 	%r492, %ee;
	and.b32  	%r493, %gg, %r492; // = (~e & g)
	and.b32  	%r494, %ee, %ff;   // = (e & f)
	xor.b32  	%r495, %r493, %r494;  // = ((e & f) ^ (~e & g))
	mul.wide.u32 	%rd59, %r715, 4;  // r715 -> i; rd59 -> 4*i
	mov.u64 	%rd60, k;
	add.s64 	%rd61, %rd60, %rd59;  //rd61 -> &k + 4*i
	add.s32 	%r496, %r495, %hh;    // = h + ((e & f) ^ (~e & g)) 
	add.s32 	%r497, %r496, %r478;  // = h + res1 + ((e & f) ^ (~e & g))
	ld.const.u32 	%r498, [%rd61];   // k[4*i]
	add.s32 	%r499, %r497, %r498;  // h + res1 + ((e & f) ^ (~e & g)) + k[i]
	add.s32 	%r500, %r499, %r713;  // t1 = h + res1 + ((e & f) ^ (~e & g)) + k[i] + m[i]
	add.s32 	%hh, %r500, %dd;
	xor.b32  	%r501, %bb, %cc;     // = (b ^ c)
	and.b32  	%r502, %aa, %r501;   // = a & (b ^ c)
	and.b32  	%r503, %bb, %cc;     // = (b & c)
	xor.b32  	%r504, %r502, %r503; // = (a & (b ^ c)) ^ (b & c)
	add.s32 	%r505, %r476, %r504; // = res0 + ((a & b) ^ (a & c) ^ (b & c))
	add.s32 	%dd, %r505, %r500;   // t1 += ...
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %dd, %dd, 2;
	 shf.r.clamp.b32    t2, %dd, %dd, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %dd, %dd, 22;
	 xor.b32            %r480, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %hh, %hh, 6;
	 shf.r.clamp.b32    t2, %hh, %hh, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %hh, %hh, 25;
	 xor.b32            %r482, t1, t2;
	}
	// inline asm
	and.b32  	%r506, %hh, %ee;
	not.b32 	%r507, %hh;
	and.b32  	%r508, %ff, %r507;
	xor.b32  	%r509, %r508, %r506;
	add.s32 	%r510, %r715, 1;
	mul.wide.u32 	%rd62, %r510, 4;
	add.s64 	%rd26, %rd12, %rd62;
	add.s32 	%r511, %r509, %gg;
	add.s32 	%r512, %r511, %r482;
	ld.const.u32 	%r513, [%rd61+4];
	add.s32 	%r514, %r512, %r513;
	ld.local.u32 	%r515, [%rd26];
	add.s32 	%r516, %r514, %r515;
	add.s32 	%gg, %r516, %cc;
	xor.b32  	%r517, %aa, %bb;
	and.b32  	%r518, %dd, %r517;
	and.b32  	%r519, %aa, %bb;
	xor.b32  	%r520, %r518, %r519;
	add.s32 	%r521, %r480, %r520;
	add.s32 	%cc, %r521, %r516;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %cc, %cc, 2;
	 shf.r.clamp.b32    t2, %cc, %cc, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %cc, %cc, 22;
	 xor.b32            %r484, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %gg, %gg, 6;
	 shf.r.clamp.b32    t2, %gg, %gg, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %gg, %gg, 25;
	 xor.b32            %r486, t1, t2;
	}
	// inline asm
	and.b32  	%r522, %gg, %hh;
	not.b32 	%r523, %gg;
	and.b32  	%r524, %ee, %r523;
	xor.b32  	%r525, %r524, %r522;
	add.s32 	%r526, %r525, %ff;
	add.s32 	%r527, %r526, %r486;
	ld.const.u32 	%r528, [%rd61+8];
	add.s32 	%r529, %r527, %r528;
	ld.local.v2.u32 	{%r530, %r531}, [%rd26+4];
	add.s32 	%r533, %r529, %r530;
	add.s32 	%ff, %r533, %bb;
	xor.b32  	%r534, %dd, %aa;
	and.b32  	%r535, %cc, %r534;
	and.b32  	%r536, %dd, %aa;
	xor.b32  	%r537, %r535, %r536;
	add.s32 	%r538, %r484, %r537;
	add.s32 	%bb, %r538, %r533;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %bb, %bb, 2;
	 shf.r.clamp.b32    t2, %bb, %bb, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %bb, %bb, 22;
	 xor.b32            %r488, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %ff, %ff, 6;
	 shf.r.clamp.b32    t2, %ff, %ff, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %ff, %ff, 25;
	 xor.b32            %r490, t1, t2;
	}
	// inline asm
	and.b32  	%r539, %ff, %gg;
	not.b32 	%r540, %ff;
	and.b32  	%r541, %hh, %r540;
	xor.b32  	%r542, %r541, %r539;
	add.s32 	%r543, %r542, %ee;
	add.s32 	%r544, %r543, %r490;
	ld.const.u32 	%r545, [%rd61+12];
	add.s32 	%r546, %r544, %r545;
	add.s32 	%r548, %r546, %r531;
	add.s32 	%ee, %r548, %aa;
	xor.b32  	%r549, %cc, %dd;
	and.b32  	%r550, %bb, %r549;
	and.b32  	%r551, %cc, %dd;
	xor.b32  	%r552, %r550, %r551;
	add.s32 	%r553, %r488, %r552;
	add.s32 	%aa, %r553, %r548;
	add.s32 	%r715, %r715, 4;
	setp.eq.s32	%p7, %r715, 64;
	@%p7 bra 	BB0_16;
	bra.uni 	BB0_15;

BB0_16:
	add.s64 	%rd78, %rd78, 1;
	add.s32 	%r34, %bb, -1150833019;
	add.s32 	%r35, %cc, 1013904242;
	add.s32 	%r36, %dd, -1521486534;
	setp.ne.s32	%p8, %aa, -1779033703;
	@%p8 bra 	BB0_5;

	mov.u32 	%r725, 0;
	mov.u32 	%r723, 8192;
	setp.lt.s32	%p9, %r34, 0;
	@%p9 bra 	BB0_18;

	shl.b32 	%r724, %r34, 1;
	mov.u32 	%r725, 1;
	mov.u32 	%r723, 8448;
	setp.lt.s32	%p10, %r724, 0;
	@%p10 bra 	BB0_50;

	shl.b32 	%r724, %r34, 2;
	mov.u32 	%r725, 2;
	mov.u32 	%r723, 8704;
	setp.lt.s32	%p11, %r724, 0;
	@%p11 bra 	BB0_50;

	shl.b32 	%r724, %r34, 3;
	mov.u32 	%r725, 3;
	mov.u32 	%r723, 8960;
	setp.lt.s32	%p12, %r724, 0;
	@%p12 bra 	BB0_50;

	shl.b32 	%r724, %r34, 4;
	mov.u32 	%r725, 4;
	mov.u32 	%r723, 9216;
	setp.lt.s32	%p13, %r724, 0;
	@%p13 bra 	BB0_50;

	shl.b32 	%r724, %r34, 5;
	mov.u32 	%r725, 5;
	mov.u32 	%r723, 9472;
	setp.lt.s32	%p14, %r724, 0;
	@%p14 bra 	BB0_50;

	shl.b32 	%r724, %r34, 6;
	mov.u32 	%r725, 6;
	mov.u32 	%r723, 9728;
	setp.lt.s32	%p15, %r724, 0;
	@%p15 bra 	BB0_50;

	shl.b32 	%r724, %r34, 7;
	mov.u32 	%r725, 7;
	mov.u32 	%r723, 9984;
	setp.lt.s32	%p16, %r724, 0;
	@%p16 bra 	BB0_50;

	shl.b32 	%r724, %r34, 8;
	mov.u32 	%r725, 8;
	mov.u32 	%r723, 10240;
	setp.lt.s32	%p17, %r724, 0;
	@%p17 bra 	BB0_50;

	shl.b32 	%r724, %r34, 9;
	mov.u32 	%r725, 9;
	mov.u32 	%r723, 10496;
	setp.lt.s32	%p18, %r724, 0;
	@%p18 bra 	BB0_50;

	shl.b32 	%r724, %r34, 10;
	mov.u32 	%r725, 10;
	mov.u32 	%r723, 10752;
	setp.lt.s32	%p19, %r724, 0;
	@%p19 bra 	BB0_50;

	shl.b32 	%r724, %r34, 11;
	mov.u32 	%r725, 11;
	mov.u32 	%r723, 11008;
	setp.lt.s32	%p20, %r724, 0;
	@%p20 bra 	BB0_50;

	shl.b32 	%r724, %r34, 12;
	mov.u32 	%r725, 12;
	mov.u32 	%r723, 11264;
	setp.lt.s32	%p21, %r724, 0;
	@%p21 bra 	BB0_50;

	shl.b32 	%r724, %r34, 13;
	mov.u32 	%r725, 13;
	mov.u32 	%r723, 11520;
	setp.lt.s32	%p22, %r724, 0;
	@%p22 bra 	BB0_50;

	shl.b32 	%r724, %r34, 14;
	mov.u32 	%r725, 14;
	mov.u32 	%r723, 11776;
	setp.lt.s32	%p23, %r724, 0;
	@%p23 bra 	BB0_50;

	shl.b32 	%r724, %r34, 15;
	mov.u32 	%r725, 15;
	mov.u32 	%r723, 12032;
	setp.lt.s32	%p24, %r724, 0;
	@%p24 bra 	BB0_50;

	shl.b32 	%r724, %r34, 16;
	mov.u32 	%r725, 16;
	mov.u32 	%r723, 12288;
	setp.lt.s32	%p25, %r724, 0;
	@%p25 bra 	BB0_50;

	shl.b32 	%r724, %r34, 17;
	mov.u32 	%r725, 17;
	mov.u32 	%r723, 12544;
	setp.lt.s32	%p26, %r724, 0;
	@%p26 bra 	BB0_50;

	shl.b32 	%r724, %r34, 18;
	mov.u32 	%r725, 18;
	mov.u32 	%r723, 12800;
	setp.lt.s32	%p27, %r724, 0;
	@%p27 bra 	BB0_50;

	shl.b32 	%r724, %r34, 19;
	mov.u32 	%r725, 19;
	mov.u32 	%r723, 13056;
	setp.lt.s32	%p28, %r724, 0;
	@%p28 bra 	BB0_50;

	shl.b32 	%r724, %r34, 20;
	mov.u32 	%r725, 20;
	mov.u32 	%r723, 13312;
	setp.lt.s32	%p29, %r724, 0;
	@%p29 bra 	BB0_50;

	shl.b32 	%r724, %r34, 21;
	mov.u32 	%r725, 21;
	mov.u32 	%r723, 13568;
	setp.lt.s32	%p30, %r724, 0;
	@%p30 bra 	BB0_50;

	shl.b32 	%r724, %r34, 22;
	mov.u32 	%r725, 22;
	mov.u32 	%r723, 13824;
	setp.lt.s32	%p31, %r724, 0;
	@%p31 bra 	BB0_50;

	shl.b32 	%r724, %r34, 23;
	mov.u32 	%r725, 23;
	mov.u32 	%r723, 14080;
	setp.lt.s32	%p32, %r724, 0;
	@%p32 bra 	BB0_50;

	shl.b32 	%r724, %r34, 24;
	mov.u32 	%r725, 24;
	mov.u32 	%r723, 14336;
	setp.lt.s32	%p33, %r724, 0;
	@%p33 bra 	BB0_50;

	shl.b32 	%r724, %r34, 25;
	mov.u32 	%r725, 25;
	mov.u32 	%r723, 14592;
	setp.lt.s32	%p34, %r724, 0;
	@%p34 bra 	BB0_50;

	shl.b32 	%r724, %r34, 26;
	mov.u32 	%r725, 26;
	mov.u32 	%r723, 14848;
	setp.lt.s32	%p35, %r724, 0;
	@%p35 bra 	BB0_50;

	shl.b32 	%r724, %r34, 27;
	mov.u32 	%r725, 27;
	mov.u32 	%r723, 15104;
	setp.lt.s32	%p36, %r724, 0;
	@%p36 bra 	BB0_50;

	shl.b32 	%r724, %r34, 28;
	mov.u32 	%r725, 28;
	mov.u32 	%r723, 15360;
	setp.lt.s32	%p37, %r724, 0;
	@%p37 bra 	BB0_50;

	shl.b32 	%r724, %r34, 29;
	mov.u32 	%r725, 29;
	mov.u32 	%r723, 15616;
	setp.lt.s32	%p38, %r724, 0;
	@%p38 bra 	BB0_50;

	shl.b32 	%r724, %r34, 30;
	mov.u32 	%r725, 30;
	mov.u32 	%r723, 15872;
	setp.lt.s32	%p39, %r724, 0;
	@%p39 bra 	BB0_50;

	shl.b32 	%r724, %r34, 31;
	setp.eq.s32	%p40, %r724, 0;
	mov.u32 	%r725, 31;
	mov.u32 	%r723, 16128;
	@%p40 bra 	BB0_53;
	bra.uni 	BB0_50;

BB0_53:
	mov.u32 	%r728, 0;
	mov.u32 	%r726, 16384;
	setp.lt.s32	%p43, %r35, 0;
	mov.u32 	%r727, %r35;
	@%p43 bra 	BB0_85;

	shl.b32 	%r727, %r35, 1;
	mov.u32 	%r728, 1;
	mov.u32 	%r726, 16640;
	setp.lt.s32	%p44, %r727, 0;
	@%p44 bra 	BB0_85;

	shl.b32 	%r727, %r35, 2;
	mov.u32 	%r728, 2;
	mov.u32 	%r726, 16896;
	setp.lt.s32	%p45, %r727, 0;
	@%p45 bra 	BB0_85;

	shl.b32 	%r727, %r35, 3;
	mov.u32 	%r728, 3;
	mov.u32 	%r726, 17152;
	setp.lt.s32	%p46, %r727, 0;
	@%p46 bra 	BB0_85;

	shl.b32 	%r727, %r35, 4;
	mov.u32 	%r728, 4;
	mov.u32 	%r726, 17408;
	setp.lt.s32	%p47, %r727, 0;
	@%p47 bra 	BB0_85;

	shl.b32 	%r727, %r35, 5;
	mov.u32 	%r728, 5;
	mov.u32 	%r726, 17664;
	setp.lt.s32	%p48, %r727, 0;
	@%p48 bra 	BB0_85;

	shl.b32 	%r727, %r35, 6;
	mov.u32 	%r728, 6;
	mov.u32 	%r726, 17920;
	setp.lt.s32	%p49, %r727, 0;
	@%p49 bra 	BB0_85;

	shl.b32 	%r727, %r35, 7;
	mov.u32 	%r728, 7;
	mov.u32 	%r726, 18176;
	setp.lt.s32	%p50, %r727, 0;
	@%p50 bra 	BB0_85;

	shl.b32 	%r727, %r35, 8;
	mov.u32 	%r728, 8;
	mov.u32 	%r726, 18432;
	setp.lt.s32	%p51, %r727, 0;
	@%p51 bra 	BB0_85;

	shl.b32 	%r727, %r35, 9;
	mov.u32 	%r728, 9;
	mov.u32 	%r726, 18688;
	setp.lt.s32	%p52, %r727, 0;
	@%p52 bra 	BB0_85;

	shl.b32 	%r727, %r35, 10;
	mov.u32 	%r728, 10;
	mov.u32 	%r726, 18944;
	setp.lt.s32	%p53, %r727, 0;
	@%p53 bra 	BB0_85;

	shl.b32 	%r727, %r35, 11;
	mov.u32 	%r728, 11;
	mov.u32 	%r726, 19200;
	setp.lt.s32	%p54, %r727, 0;
	@%p54 bra 	BB0_85;

	shl.b32 	%r727, %r35, 12;
	mov.u32 	%r728, 12;
	mov.u32 	%r726, 19456;
	setp.lt.s32	%p55, %r727, 0;
	@%p55 bra 	BB0_85;

	shl.b32 	%r727, %r35, 13;
	mov.u32 	%r728, 13;
	mov.u32 	%r726, 19712;
	setp.lt.s32	%p56, %r727, 0;
	@%p56 bra 	BB0_85;

	shl.b32 	%r727, %r35, 14;
	mov.u32 	%r728, 14;
	mov.u32 	%r726, 19968;
	setp.lt.s32	%p57, %r727, 0;
	@%p57 bra 	BB0_85;

	shl.b32 	%r727, %r35, 15;
	mov.u32 	%r728, 15;
	mov.u32 	%r726, 20224;
	setp.lt.s32	%p58, %r727, 0;
	@%p58 bra 	BB0_85;

	shl.b32 	%r727, %r35, 16;
	mov.u32 	%r728, 16;
	mov.u32 	%r726, 20480;
	setp.lt.s32	%p59, %r727, 0;
	@%p59 bra 	BB0_85;

	shl.b32 	%r727, %r35, 17;
	mov.u32 	%r728, 17;
	mov.u32 	%r726, 20736;
	setp.lt.s32	%p60, %r727, 0;
	@%p60 bra 	BB0_85;

	shl.b32 	%r727, %r35, 18;
	mov.u32 	%r728, 18;
	mov.u32 	%r726, 20992;
	setp.lt.s32	%p61, %r727, 0;
	@%p61 bra 	BB0_85;

	shl.b32 	%r727, %r35, 19;
	mov.u32 	%r728, 19;
	mov.u32 	%r726, 21248;
	setp.lt.s32	%p62, %r727, 0;
	@%p62 bra 	BB0_85;

	shl.b32 	%r727, %r35, 20;
	mov.u32 	%r728, 20;
	mov.u32 	%r726, 21504;
	setp.lt.s32	%p63, %r727, 0;
	@%p63 bra 	BB0_85;

	shl.b32 	%r727, %r35, 21;
	mov.u32 	%r728, 21;
	mov.u32 	%r726, 21760;
	setp.lt.s32	%p64, %r727, 0;
	@%p64 bra 	BB0_85;

	shl.b32 	%r727, %r35, 22;
	mov.u32 	%r728, 22;
	mov.u32 	%r726, 22016;
	setp.lt.s32	%p65, %r727, 0;
	@%p65 bra 	BB0_85;

	shl.b32 	%r727, %r35, 23;
	mov.u32 	%r728, 23;
	mov.u32 	%r726, 22272;
	setp.lt.s32	%p66, %r727, 0;
	@%p66 bra 	BB0_85;

	shl.b32 	%r727, %r35, 24;
	mov.u32 	%r728, 24;
	mov.u32 	%r726, 22528;
	setp.lt.s32	%p67, %r727, 0;
	@%p67 bra 	BB0_85;

	shl.b32 	%r727, %r35, 25;
	mov.u32 	%r728, 25;
	mov.u32 	%r726, 22784;
	setp.lt.s32	%p68, %r727, 0;
	@%p68 bra 	BB0_85;

	shl.b32 	%r727, %r35, 26;
	mov.u32 	%r728, 26;
	mov.u32 	%r726, 23040;
	setp.lt.s32	%p69, %r727, 0;
	@%p69 bra 	BB0_85;

	shl.b32 	%r727, %r35, 27;
	mov.u32 	%r728, 27;
	mov.u32 	%r726, 23296;
	setp.lt.s32	%p70, %r727, 0;
	@%p70 bra 	BB0_85;

	shl.b32 	%r727, %r35, 28;
	mov.u32 	%r728, 28;
	mov.u32 	%r726, 23552;
	setp.lt.s32	%p71, %r727, 0;
	@%p71 bra 	BB0_85;

	shl.b32 	%r727, %r35, 29;
	mov.u32 	%r728, 29;
	mov.u32 	%r726, 23808;
	setp.lt.s32	%p72, %r727, 0;
	@%p72 bra 	BB0_85;

	shl.b32 	%r727, %r35, 30;
	mov.u32 	%r728, 30;
	mov.u32 	%r726, 24064;
	setp.lt.s32	%p73, %r727, 0;
	@%p73 bra 	BB0_85;

	shl.b32 	%r727, %r35, 31;
	setp.eq.s32	%p74, %r727, 0;
	mov.u32 	%r728, 31;
	mov.u32 	%r726, 24320;
	mov.u32 	%r729, 96;
	@%p74 bra 	BB0_88;

BB0_85:
	setp.eq.s32	%p75, %r728, 31;
	@%p75 bra 	BB0_87;
	bra.uni 	BB0_86;

BB0_87:
	shr.u32 	%r696, %r36, 24;
	add.s32 	%r729, %r726, %r696;
	bra.uni 	BB0_88;

BB0_18:
	mov.u32 	%r724, %r34;

BB0_50:
	setp.eq.s32	%p41, %r725, 31;
	@%p41 bra 	BB0_52;
	bra.uni 	BB0_51;

BB0_52:
	shr.u32 	%r624, %r35, 24;
	add.s32 	%r729, %r723, %r624;
	bra.uni 	BB0_88;

BB0_51:
	bfe.u32 	%r618, %r724, 23, 8;
	add.s32 	%r619, %r618, %r723;
	mov.u32 	%r620, 56;
	sub.s32 	%r621, %r620, %r725;
	shr.u32 	%r622, %r35, %r621;
	setp.gt.u32	%p42, %r725, 23;
	selp.b32	%r623, %r622, 0, %p42;
	add.s32 	%r729, %r619, %r623;

BB0_88:
	ld.param.u32 	%r708, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	setp.le.u32	%p77, %r729, %r708;
	@%p77 bra 	BB0_5;

	add.s64 	%rd80, %rd4, 35;
	mov.u16 	%rs7, 1;
	st.volatile.global.u8 	[%rd10], %rs7;
	st.volatile.global.u8 	[%rd2], %rs7;
	add.s32 	%r110, %hh, 1541459225;
	add.s32 	%r111, %gg, 528734635;
	add.s32 	%r112, %ff, -1694144372;
	mov.u32 	%r730, 0;
	mov.u64 	%rd79, %rd11;

BB0_90:
	.pragma "nounroll";
	add.s64 	%rd71, %rd4, 32;
	ld.local.v4.u8 	{%rs8, %rs9, %rs10, %rs11}, [%rd80+-3];
	st.global.u8 	[%rd79], %rs11;
	st.global.u8 	[%rd79+1], %rs10;
	st.global.u8 	[%rd79+2], %rs9;
	st.global.u8 	[%rd79+3], %rs8;
	add.s32 	%r730, %r730, 4;
	setp.lt.u32	%p78, %r730, 20;
	cvt.u64.u32	%rd63, %r730;
	add.s64 	%rd64, %rd63, %rd71;
	add.s64 	%rd80, %rd64, 3;
	add.s64 	%rd79, %rd11, %rd63;
	@%p78 bra 	BB0_90;

	add.u64 	%rd73, %SP, 368;
	cvta.to.local.u64 	%rd82, %rd73;
	ld.local.v2.u8 	{%rs17, %rs18}, [%rd4+54];
	ld.local.u8 	%rs21, [%rd4+53];
	st.global.u8 	[%rd11+20], %rs18;
	st.global.u8 	[%rd11+21], %rs17;
	st.global.u8 	[%rd11+22], %rs21;
	mov.u32 	%r699, 0;
	add.s32 	%r701, %bb, -1150833019;
	st.local.v4.u32 	[%rd82], {%r699, %r701, %r35, %r36};
	add.s32 	%r702, %ee, 1359893119;
	st.local.v4.u32 	[%rd82+16], {%r702, %r112, %r111, %r110};
	mov.u16 	%rs27, 0;
	mov.u32 	%r731, 4;
	mov.u64 	%rd81, %rd3;
	bra.uni 	BB0_92;

BB0_93:
	add.s64 	%rd34, %rd82, 4;
	ld.local.u8 	%rs27, [%rd82+4];
	add.s64 	%rd81, %rd81, 4;
	add.s32 	%r731, %r731, 4;
	mov.u64 	%rd82, %rd34;

BB0_92:
	ld.local.v2.u8 	{%rs22, %rs23}, [%rd82+2];
	ld.local.u8 	%rs26, [%rd82+1];
	st.global.u8 	[%rd81], %rs23;
	st.global.u8 	[%rd81+1], %rs22;
	st.global.u8 	[%rd81+2], %rs26;
	st.global.u8 	[%rd81+3], %rs27;
	setp.gt.u32	%p79, %r731, 31;
	@%p79 bra 	BB0_5;
	bra.uni 	BB0_93;

BB0_86:
	bfe.u32 	%r690, %r727, 23, 8;
	add.s32 	%r691, %r690, %r726;
	mov.u32 	%r692, 56;
	sub.s32 	%r693, %r692, %r728;
	shr.u32 	%r694, %r36, %r693;
	setp.gt.u32	%p76, %r728, 23;
	selp.b32	%r695, %r694, 0, %p76;
	add.s32 	%r729, %r691, %r695;
	bra.uni 	BB0_88;

BB0_5:
	mul.hi.s64 	%rd50, %rd78, -6640827866535438581;
	add.s64 	%rd51, %rd50, %rd78;
	shr.u64 	%rd52, %rd51, 63;
	shr.s64 	%rd53, %rd51, 6;
	add.s64 	%rd54, %rd53, %rd52;
	mul.lo.s64 	%rd55, %rd54, 100;
	sub.s64 	%rd56, %rd78, %rd55;
	setp.ne.s64	%p3, %rd56, 9;
	@%p3 bra 	BB0_10;

	ld.volatile.global.u8 	%rs6, [%rd2];
	setp.eq.s16	%p4, %rs6, 0;
	@%p4 bra 	BB0_10;

	mov.u32 	%r707, %tid.x;
	mov.u32 	%r706, %ctaid.x;
	mov.u32 	%r705, %ntid.x;
	mad.lo.s32 	%r704, %r705, %r706, %r707;
	ld.param.u64 	%rd67, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd66, %rd67;
	cvt.u64.u32	%rd65, %r704;
	shl.b64 	%rd57, %rd65, 3;
	add.s64 	%rd58, %rd66, %rd57;
	st.global.u64 	[%rd58], %rd78;
	setp.ne.s32	%p5, %r704, 1;
	@%p5 bra 	BB0_9;

	ld.param.u64 	%rd69, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	cvta.to.global.u64 	%rd68, %rd69;
	st.volatile.global.u64 	[%rd68], %rd78;

BB0_9:
	ret;
}


