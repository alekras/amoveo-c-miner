//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};
.extern .shared .align 4 .b8 shared_memory[];

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 4 .b8 	__local_depot0[96];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<84>;
	.reg .b16 	%rs<27>;
	.reg .b32 	%r<287>;
	.reg .b64 	%rd<78>;


	mov.u64 	%rd77, __local_depot0;
	cvta.local.u64 	%SP, %rd77;
	ld.param.u64 	%rd35, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0];
	ld.param.u32 	%r83, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	ld.param.u64 	%rd32, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2];
	ld.param.u64 	%rd33, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3];
	ld.param.u64 	%rd36, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4];
	ld.param.u64 	%rd34, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	ld.param.u32 	%r84, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6];
	ld.param.u64 	%rd37, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd1, %rd37;
	cvta.to.global.u64 	%rd2, %rd36;
	cvta.to.global.u64 	%rd3, %rd35;
	add.u64 	%rd38, %SP, 0;
	cvta.to.local.u64 	%rd4, %rd38;
	mov.u32 	%r86, %ntid.x;
	mov.u32 	%r87, %ctaid.x;
	mov.u32 	%r1, %tid.x;
	mad.lo.s32 	%r2, %r86, %r87, %r1;
	cvt.u64.u32	%rd5, %r2;
	mov.u32 	%r264, -8;
	mov.u64 	%rd67, %rd4;
	mov.u64 	%rd68, %rd3;

BB0_1:
	.pragma "nounroll";
	ld.global.u8 	%r88, [%rd68];
	shl.b32 	%r89, %r88, 24;
	ld.global.u8 	%r90, [%rd68+1];
	shl.b32 	%r91, %r90, 16;
	or.b32  	%r92, %r91, %r89;
	ld.global.u8 	%rs1, [%rd68+2];
	mul.wide.u16 	%r93, %rs1, 256;
	or.b32  	%r94, %r92, %r93;
	ld.global.u8 	%r95, [%rd68+3];
	or.b32  	%r96, %r94, %r95;
	st.local.u32 	[%rd67], %r96;
	add.s64 	%rd68, %rd68, 4;
	add.s64 	%rd67, %rd67, 4;
	add.s32 	%r264, %r264, 1;
	setp.ne.s32	%p2, %r264, 0;
	@%p2 bra 	BB0_1;

	cvta.to.global.u64 	%rd10, %rd34;
	cvta.to.global.u64 	%rd11, %rd33;
	cvta.to.global.u64 	%rd12, %rd32;
	shl.b32 	%r5, %r1, 6;
	mov.u32 	%r265, -5;
	mov.u64 	%rd69, -8;
	mov.u64 	%rd70, %rd12;

BB0_3:
	.pragma "nounroll";
	ld.global.u8 	%r98, [%rd70];
	shl.b32 	%r99, %r98, 24;
	ld.global.u8 	%r100, [%rd70+1];
	shl.b32 	%r101, %r100, 16;
	or.b32  	%r102, %r101, %r99;
	ld.global.u8 	%rs2, [%rd70+2];
	mul.wide.u16 	%r103, %rs2, 256;
	or.b32  	%r104, %r102, %r103;
	ld.global.u8 	%r105, [%rd70+3];
	or.b32  	%r106, %r104, %r105;
	shl.b64 	%rd40, %rd69, 2;
	sub.s64 	%rd41, %rd4, %rd40;
	st.local.u32 	[%rd41], %r106;
	add.s64 	%rd70, %rd70, 4;
	add.s64 	%rd69, %rd69, -1;
	add.s32 	%r265, %r265, 1;
	setp.ne.s32	%p3, %r265, 0;
	@%p3 bra 	BB0_3;

	ld.global.u8 	%r107, [%rd12+20];
	shl.b32 	%r108, %r107, 24;
	ld.global.u8 	%r109, [%rd12+21];
	shl.b32 	%r110, %r109, 16;
	ld.global.u8 	%rs3, [%rd12+22];
	mul.wide.u16 	%r111, %rs3, 256;
	or.b32  	%r112, %r108, %r110;
	or.b32  	%r113, %r112, %r111;
	or.b32  	%r114, %r113, 128;
	st.local.u32 	[%rd4+52], %r114;
	mov.u32 	%r115, 0;
	st.local.u32 	[%rd4+56], %r115;
	mov.u32 	%r116, 440;
	st.local.u32 	[%rd4+60], %r116;
	ld.local.u32 	%r117, [%rd4+36];
	xor.b32  	%r118, %r117, %r2;
	ld.local.u32 	%r119, [%rd4+40];
	st.local.u32 	[%rd4+36], %r118;
	xor.b32  	%r120, %r119, %r84;
	st.local.u32 	[%rd4+40], %r120;
	shl.b32 	%r121, %r1, 8;
	mov.u32 	%r122, shared_memory;
	add.s32 	%r8, %r122, %r121;
	mov.u64 	%rd71, 0;
	bra.uni 	BB0_5;

BB0_10:
	ld.local.u32 	%r124, [%rd4+44];
	add.s32 	%r125, %r124, 1;
	st.local.u32 	[%rd4+44], %r125;
	setp.ne.s32	%p7, %r125, 0;
	@%p7 bra 	BB0_12;

	ld.local.u32 	%r126, [%rd4+48];
	add.s32 	%r127, %r126, 1;
	st.local.u32 	[%rd4+48], %r127;

BB0_12:
	mov.u32 	%r270, 1779033703;
	st.local.u32 	[%rd4+64], %r270;
	mov.u32 	%r277, 1541459225;
	st.local.u32 	[%rd4+92], %r277;
	mov.u64 	%rd52, 1013904242;
	st.local.u32 	[%rd4+72], %rd52;
	mov.u64 	%rd53, 3144134277;
	st.local.u32 	[%rd4+68], %rd53;
	mov.u64 	%rd54, 1359893119;
	st.local.u32 	[%rd4+80], %rd54;
	mov.u64 	%rd55, 2773480762;
	st.local.u32 	[%rd4+76], %rd55;
	mov.u64 	%rd56, 528734635;
	st.local.u32 	[%rd4+88], %rd56;
	mov.u64 	%rd57, 2600822924;
	st.local.u32 	[%rd4+84], %rd57;
	mov.u32 	%r267, -16;
	mov.u32 	%r266, %r8;
	mov.u64 	%rd72, %rd4;

BB0_13:
	.pragma "nounroll";
	ld.local.u32 	%r131, [%rd72];
	st.shared.u32 	[%r266], %r131;
	add.s64 	%rd72, %rd72, 4;
	add.s32 	%r266, %r266, 4;
	add.s32 	%r267, %r267, 1;
	setp.ne.s32	%p8, %r267, 0;
	@%p8 bra 	BB0_13;

	add.s64 	%rd71, %rd71, 1;
	mov.u32 	%r268, 16;

BB0_15:
	.pragma "nounroll";
	add.s32 	%r146, %r5, %r268;
	shl.b32 	%r147, %r146, 2;
	add.s32 	%r149, %r122, %r147;
	ld.shared.u32 	%r134, [%r149+-60];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r134, %r134, 7;
	 shf.r.clamp.b32    t2, %r134, %r134, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r134, 3;
	 xor.b32            %r133, t1, t2;
	}
	// inline asm
	ld.shared.u32 	%r136, [%r149+-8];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r136, %r136, 17;
	 shf.r.clamp.b32    t2, %r136, %r136, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r136, 10;
	 xor.b32            %r135, t1, t2;
	}
	// inline asm
	add.s32 	%r150, %r135, %r133;
	ld.shared.u32 	%r151, [%r149+-28];
	add.s32 	%r152, %r150, %r151;
	ld.shared.u32 	%r153, [%r149+-64];
	add.s32 	%r154, %r152, %r153;
	st.shared.u32 	[%r149], %r154;
	add.s32 	%r268, %r268, 1;
	mov.u32 	%r275, -1694144372;
	mov.u32 	%r274, 1359893119;
	mov.u32 	%r273, -1521486534;
	mov.u32 	%r272, 1013904242;
	mov.u32 	%r271, -1150833019;
	mov.u32 	%r276, 528734635;
	setp.ne.s32	%p9, %r268, 64;
	@%p9 bra 	BB0_15;

	mov.u32 	%r269, %r115;

BB0_17:
	.pragma "nounroll";
	mov.u32 	%r18, %r276;
	mov.u32 	%r276, %r275;
	mov.u32 	%r275, %r274;
	mov.u32 	%r21, %r272;
	mov.u32 	%r272, %r271;
	mov.u32 	%r271, %r270;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r271, %r271, 2;
	 shf.r.clamp.b32    t2, %r271, %r271, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r271, %r271, 22;
	 xor.b32            %r155, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r275, %r275, 6;
	 shf.r.clamp.b32    t2, %r275, %r275, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r275, %r275, 25;
	 xor.b32            %r157, t1, t2;
	}
	// inline asm
	not.b32 	%r159, %r275;
	and.b32  	%r160, %r18, %r159;
	and.b32  	%r161, %r275, %r276;
	xor.b32  	%r162, %r160, %r161;
	mul.wide.u32 	%rd58, %r269, 4;
	mov.u64 	%rd59, k;
	add.s64 	%rd60, %rd59, %rd58;
	add.s32 	%r163, %r269, %r5;
	shl.b32 	%r164, %r163, 2;
	add.s32 	%r166, %r122, %r164;
	add.s32 	%r167, %r162, %r277;
	add.s32 	%r168, %r167, %r157;
	ld.const.u32 	%r169, [%rd60];
	add.s32 	%r170, %r168, %r169;
	ld.shared.u32 	%r171, [%r166];
	add.s32 	%r26, %r170, %r171;
	add.s32 	%r274, %r26, %r273;
	add.s32 	%r172, %r274, 1541459225;
	setp.gt.u32	%p10, %r172, 16777215;
	setp.eq.s32	%p11, %r269, 60;
	and.pred  	%p12, %p11, %p10;
	@%p12 bra 	BB0_5;

	xor.b32  	%r173, %r272, %r21;
	and.b32  	%r174, %r271, %r173;
	and.b32  	%r175, %r272, %r21;
	xor.b32  	%r176, %r174, %r175;
	add.s32 	%r177, %r155, %r176;
	add.s32 	%r270, %r177, %r26;
	add.s32 	%r269, %r269, 1;
	setp.lt.u32	%p13, %r269, 6;
	mov.u32 	%r273, %r21;
	mov.u32 	%r277, %r18;
	@%p13 bra 	BB0_17;

	add.s32 	%r31, %r270, 1779033703;
	st.local.u32 	[%rd4+64], %r31;
	add.s32 	%r180, %r271, -1150833019;
	st.local.u32 	[%rd4+68], %r180;
	add.s32 	%r181, %r272, 1013904242;
	st.local.u32 	[%rd4+72], %r181;
	add.s32 	%r182, %r21, -1521486534;
	st.local.u32 	[%rd4+76], %r182;
	add.s32 	%r183, %r274, 1359893119;
	st.local.u32 	[%rd4+80], %r183;
	add.s32 	%r184, %r275, -1694144372;
	st.local.u32 	[%rd4+84], %r184;
	add.s32 	%r185, %r276, 528734635;
	st.local.u32 	[%rd4+88], %r185;
	add.s32 	%r186, %r18, 1541459225;
	st.local.u32 	[%rd4+92], %r186;
	mov.u32 	%r32, 0;
	mov.u32 	%r280, %r32;
	bra.uni 	BB0_20;

BB0_63:
	mul.wide.s32 	%rd65, %r280, 4;
	add.s64 	%rd66, %rd4, %rd65;
	ld.local.u32 	%r31, [%rd66+64];

BB0_20:
	mov.pred 	%p83, 0;
	mov.u32 	%r281, 1;
	setp.lt.s32	%p15, %r31, 0;
	@%p15 bra 	BB0_21;

	add.s32 	%r282, %r32, 1;
	and.b32  	%r189, %r31, 1073741824;
	mov.u32 	%r281, 2;
	setp.ne.s32	%p17, %r189, 0;
	@%p17 bra 	BB0_22;

	add.s32 	%r282, %r32, 2;
	and.b32  	%r191, %r31, 536870912;
	mov.u32 	%r281, 3;
	setp.ne.s32	%p19, %r191, 0;
	@%p19 bra 	BB0_22;

	add.s32 	%r282, %r32, 3;
	and.b32  	%r193, %r31, 268435456;
	mov.u32 	%r281, 4;
	setp.ne.s32	%p21, %r193, 0;
	@%p21 bra 	BB0_22;

	add.s32 	%r282, %r32, 4;
	and.b32  	%r195, %r31, 134217728;
	mov.u32 	%r281, 5;
	setp.ne.s32	%p23, %r195, 0;
	@%p23 bra 	BB0_22;

	add.s32 	%r282, %r32, 5;
	and.b32  	%r197, %r31, 67108864;
	mov.u32 	%r281, 6;
	setp.ne.s32	%p25, %r197, 0;
	@%p25 bra 	BB0_22;

	add.s32 	%r282, %r32, 6;
	and.b32  	%r199, %r31, 33554432;
	mov.u32 	%r281, 7;
	setp.ne.s32	%p27, %r199, 0;
	@%p27 bra 	BB0_22;

	add.s32 	%r282, %r32, 7;
	and.b32  	%r201, %r31, 16777216;
	mov.u32 	%r281, 8;
	setp.ne.s32	%p29, %r201, 0;
	@%p29 bra 	BB0_22;

	add.s32 	%r282, %r32, 8;
	and.b32  	%r203, %r31, 8388608;
	mov.u32 	%r281, 9;
	setp.ne.s32	%p31, %r203, 0;
	@%p31 bra 	BB0_22;

	add.s32 	%r282, %r32, 9;
	and.b32  	%r205, %r31, 4194304;
	mov.u32 	%r281, 10;
	setp.ne.s32	%p33, %r205, 0;
	@%p33 bra 	BB0_22;

	add.s32 	%r282, %r32, 10;
	and.b32  	%r207, %r31, 2097152;
	mov.u32 	%r281, 11;
	setp.ne.s32	%p35, %r207, 0;
	@%p35 bra 	BB0_22;

	add.s32 	%r282, %r32, 11;
	and.b32  	%r209, %r31, 1048576;
	mov.u32 	%r281, 12;
	setp.ne.s32	%p37, %r209, 0;
	@%p37 bra 	BB0_22;

	add.s32 	%r282, %r32, 12;
	and.b32  	%r211, %r31, 524288;
	mov.u32 	%r281, 13;
	setp.ne.s32	%p39, %r211, 0;
	@%p39 bra 	BB0_22;

	add.s32 	%r282, %r32, 13;
	and.b32  	%r213, %r31, 262144;
	mov.u32 	%r281, 14;
	setp.ne.s32	%p41, %r213, 0;
	@%p41 bra 	BB0_22;

	add.s32 	%r282, %r32, 14;
	and.b32  	%r215, %r31, 131072;
	mov.u32 	%r281, 15;
	setp.ne.s32	%p43, %r215, 0;
	@%p43 bra 	BB0_22;

	add.s32 	%r282, %r32, 15;
	and.b32  	%r217, %r31, 65536;
	mov.u32 	%r281, 16;
	setp.ne.s32	%p45, %r217, 0;
	@%p45 bra 	BB0_22;

	add.s32 	%r282, %r32, 16;
	and.b32  	%r219, %r31, 32768;
	mov.u32 	%r281, 17;
	setp.ne.s32	%p47, %r219, 0;
	@%p47 bra 	BB0_22;

	add.s32 	%r282, %r32, 17;
	and.b32  	%r221, %r31, 16384;
	mov.u32 	%r281, 18;
	setp.ne.s32	%p49, %r221, 0;
	@%p49 bra 	BB0_22;

	add.s32 	%r282, %r32, 18;
	and.b32  	%r223, %r31, 8192;
	mov.u32 	%r281, 19;
	setp.ne.s32	%p51, %r223, 0;
	@%p51 bra 	BB0_22;

	add.s32 	%r282, %r32, 19;
	and.b32  	%r225, %r31, 4096;
	mov.u32 	%r281, 20;
	setp.ne.s32	%p53, %r225, 0;
	@%p53 bra 	BB0_22;

	add.s32 	%r282, %r32, 20;
	and.b32  	%r227, %r31, 2048;
	mov.u32 	%r281, 21;
	setp.ne.s32	%p55, %r227, 0;
	@%p55 bra 	BB0_22;

	add.s32 	%r282, %r32, 21;
	and.b32  	%r229, %r31, 1024;
	mov.u32 	%r281, 22;
	setp.ne.s32	%p57, %r229, 0;
	@%p57 bra 	BB0_22;

	add.s32 	%r282, %r32, 22;
	and.b32  	%r231, %r31, 512;
	mov.u32 	%r281, 23;
	setp.ne.s32	%p59, %r231, 0;
	@%p59 bra 	BB0_22;

	add.s32 	%r282, %r32, 23;
	and.b32  	%r233, %r31, 256;
	mov.u32 	%r281, 24;
	setp.ne.s32	%p61, %r233, 0;
	@%p61 bra 	BB0_22;

	add.s32 	%r282, %r32, 24;
	and.b32  	%r235, %r31, 128;
	mov.u32 	%r281, 25;
	setp.ne.s32	%p63, %r235, 0;
	@%p63 bra 	BB0_22;

	add.s32 	%r282, %r32, 25;
	and.b32  	%r237, %r31, 64;
	mov.u32 	%r281, 26;
	setp.ne.s32	%p65, %r237, 0;
	@%p65 bra 	BB0_22;

	add.s32 	%r282, %r32, 26;
	and.b32  	%r239, %r31, 32;
	mov.u32 	%r281, 27;
	setp.ne.s32	%p67, %r239, 0;
	@%p67 bra 	BB0_22;

	add.s32 	%r282, %r32, 27;
	and.b32  	%r241, %r31, 16;
	mov.u32 	%r281, 28;
	setp.ne.s32	%p69, %r241, 0;
	@%p69 bra 	BB0_22;

	add.s32 	%r282, %r32, 28;
	and.b32  	%r243, %r31, 8;
	mov.u32 	%r281, 29;
	setp.ne.s32	%p71, %r243, 0;
	@%p71 bra 	BB0_22;

	add.s32 	%r282, %r32, 29;
	and.b32  	%r245, %r31, 4;
	mov.u32 	%r281, 30;
	setp.ne.s32	%p73, %r245, 0;
	@%p73 bra 	BB0_22;

	add.s32 	%r282, %r32, 30;
	and.b32  	%r247, %r31, 2;
	mov.u32 	%r281, 31;
	setp.ne.s32	%p75, %r247, 0;
	@%p75 bra 	BB0_22;

	add.s32 	%r282, %r32, 31;
	and.b32  	%r249, %r31, 1;
	setp.eq.b32	%p77, %r249, 1;
	mov.pred 	%p83, -1;
	mov.u32 	%r281, 32;
	@!%p77 bra 	BB0_57;
	bra.uni 	BB0_22;

BB0_57:
	add.s32 	%r32, %r32, 32;
	add.s32 	%r280, %r280, 1;
	setp.lt.s32	%p79, %r280, 8;
	@%p79 bra 	BB0_63;
	bra.uni 	BB0_58;

BB0_21:
	mov.u32 	%r282, %r32;

BB0_22:
	selp.u32	%r250, 1, 0, %p83;
	add.s32 	%r67, %r250, %r280;
	selp.b32	%r68, 0, %r281, %p83;
	mov.u32 	%r251, 24;
	sub.s32 	%r69, %r251, %r68;
	setp.gt.s32	%p78, %r69, -1;
	mul.wide.s32 	%rd61, %r67, 4;
	add.s64 	%rd62, %rd4, %rd61;
	ld.local.u32 	%r70, [%rd62+64];
	@%p78 bra 	BB0_55;
	bra.uni 	BB0_23;

BB0_55:
	shr.u32 	%r260, %r70, %r69;
	and.b32  	%r283, %r260, 255;
	bra.uni 	BB0_56;

BB0_23:
	add.s32 	%r252, %r68, -24;
	shl.b32 	%r253, %r70, %r252;
	add.s32 	%r254, %r67, 1;
	mul.wide.s32 	%rd63, %r254, 4;
	add.s64 	%rd64, %rd4, %rd63;
	ld.local.u32 	%r255, [%rd64+64];
	shr.u32 	%r256, %r255, 1;
	mov.u32 	%r257, 55;
	sub.s32 	%r258, %r257, %r68;
	shr.u32 	%r259, %r256, %r258;
	add.s32 	%r283, %r259, %r253;

BB0_56:
	shl.b32 	%r261, %r282, 8;
	add.s32 	%r32, %r283, %r261;

BB0_58:
	setp.le.u32	%p80, %r32, %r83;
	@%p80 bra 	BB0_5;

	mov.u16 	%rs5, 1;
	st.volatile.global.u8 	[%rd11], %rs5;
	st.volatile.global.u8 	[%rd2], %rs5;
	mov.u32 	%r285, 0;
	mov.u64 	%rd73, %rd4;
	mov.u64 	%rd74, %rd12;

BB0_60:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs6, %rs7, %rs8, %rs9}, [%rd73+32];
	st.global.u8 	[%rd74], %rs9;
	st.global.u8 	[%rd74+1], %rs8;
	st.global.u8 	[%rd74+2], %rs7;
	st.global.u8 	[%rd74+3], %rs6;
	add.s64 	%rd74, %rd74, 4;
	add.s64 	%rd73, %rd73, 4;
	add.s32 	%r285, %r285, 4;
	setp.lt.s32	%p81, %r285, 20;
	@%p81 bra 	BB0_60;

	ld.local.v2.u8 	{%rs14, %rs15}, [%rd4+54];
	ld.local.u8 	%rs18, [%rd4+53];
	st.global.u8 	[%rd12+20], %rs15;
	st.global.u8 	[%rd12+21], %rs14;
	st.global.u8 	[%rd12+22], %rs18;
	mov.u32 	%r286, 0;
	mov.u64 	%rd75, %rd4;
	mov.u64 	%rd76, %rd3;

BB0_62:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs19, %rs20, %rs21, %rs22}, [%rd75+64];
	st.global.u8 	[%rd76], %rs22;
	st.global.u8 	[%rd76+1], %rs21;
	st.global.u8 	[%rd76+2], %rs20;
	st.global.u8 	[%rd76+3], %rs19;
	add.s64 	%rd76, %rd76, 4;
	add.s64 	%rd75, %rd75, 4;
	add.s32 	%r286, %r286, 4;
	setp.lt.s32	%p82, %r286, 32;
	@%p82 bra 	BB0_62;

BB0_5:
	mul.hi.s64 	%rd43, %rd71, -6640827866535438581;
	add.s64 	%rd44, %rd43, %rd71;
	shr.u64 	%rd45, %rd44, 63;
	shr.s64 	%rd46, %rd44, 6;
	add.s64 	%rd47, %rd46, %rd45;
	mul.lo.s64 	%rd48, %rd47, 100;
	sub.s64 	%rd49, %rd71, %rd48;
	setp.ne.s64	%p4, %rd49, 99;
	@%p4 bra 	BB0_10;

	ld.volatile.global.u8 	%rs4, [%rd2];
	setp.eq.s16	%p5, %rs4, 0;
	@%p5 bra 	BB0_10;

	cvt.u32.u64	%r123, %rd5;
	shl.b64 	%rd50, %rd5, 3;
	add.s64 	%rd51, %rd1, %rd50;
	st.global.u64 	[%rd51], %rd71;
	setp.ne.s32	%p6, %r123, 1;
	@%p6 bra 	BB0_9;

	st.volatile.global.u64 	[%rd10], %rd71;

BB0_9:
	ret;
}


