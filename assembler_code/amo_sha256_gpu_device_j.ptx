//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};
.const .align 4 .u32 a0 = 1779033703;
.const .align 4 .u32 b0 = -1150833019;
.const .align 4 .u32 c0 = 1013904242;
.const .align 4 .u32 d0 = -1521486534;
.const .align 4 .u32 e0 = 1359893119;
.const .align 4 .u32 f0 = -1694144372;
.const .align 4 .u32 g0 = 528734635;
.const .align 4 .u32 h0 = 1541459225;

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 16 .b8 	__local_depot0[400];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<82>;
	.reg .b16 	%rs<28>;
	.reg .b32 	%r<656>;
	.reg .b64 	%rd<78>;


	mov.u64 	%rd77, __local_depot0;
	cvta.local.u64 	%SP, %rd77;
	ld.param.u64 	%rd40, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0];
	ld.param.u32 	%r128, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	ld.param.u64 	%rd37, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2];
	ld.param.u64 	%rd38, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3];
	ld.param.u64 	%rd41, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4];
	ld.param.u64 	%rd39, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	ld.param.u32 	%r129, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6];
	ld.param.u64 	%rd42, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd1, %rd42;
	cvta.to.global.u64 	%rd2, %rd41;
	cvta.to.global.u64 	%rd3, %rd40;
	add.u64 	%rd43, %SP, 0;
	cvta.to.local.u64 	%rd4, %rd43;
	mov.u32 	%r131, %ntid.x;
	mov.u32 	%r132, %ctaid.x;
	mov.u32 	%r133, %tid.x;
	mad.lo.s32 	%r1, %r131, %r132, %r133;
	mov.u32 	%r629, -8;
	mov.u64 	%rd68, %rd3;
	mov.u64 	%rd69, %rd4;

BB0_1:
	.pragma "nounroll";
	ld.global.u8 	%r134, [%rd68];
	shl.b32 	%r135, %r134, 24;
	ld.global.u8 	%r136, [%rd68+1];
	shl.b32 	%r137, %r136, 16;
	or.b32  	%r138, %r137, %r135;
	ld.global.u8 	%rs3, [%rd68+2];
	mul.wide.u16 	%r139, %rs3, 256;
	or.b32  	%r140, %r138, %r139;
	ld.global.u8 	%r141, [%rd68+3];
	or.b32  	%r142, %r140, %r141;
	st.local.u32 	[%rd69], %r142;
	add.s64 	%rd69, %rd69, 4;
	add.s64 	%rd68, %rd68, 4;
	add.s32 	%r629, %r629, 1;
	setp.ne.s32	%p1, %r629, 0;
	@%p1 bra 	BB0_1;

	cvta.to.global.u64 	%rd9, %rd39;
	cvta.to.global.u64 	%rd10, %rd38;
	cvta.to.global.u64 	%rd11, %rd37;
	add.u64 	%rd45, %SP, 112;
	cvta.to.local.u64 	%rd12, %rd45;
	add.u64 	%rd46, %SP, 368;
	cvta.to.local.u64 	%rd13, %rd46;
	cvt.u64.u32	%rd14, %r1;
	mov.u64 	%rd71, -8;
	mov.u32 	%r630, -5;
	mov.u64 	%rd70, %rd11;

BB0_3:
	.pragma "nounroll";
	ld.global.u8 	%r144, [%rd70];
	shl.b32 	%r145, %r144, 24;
	ld.global.u8 	%r146, [%rd70+1];
	shl.b32 	%r147, %r146, 16;
	or.b32  	%r148, %r147, %r145;
	ld.global.u8 	%rs4, [%rd70+2];
	mul.wide.u16 	%r149, %rs4, 256;
	or.b32  	%r150, %r148, %r149;
	ld.global.u8 	%r151, [%rd70+3];
	or.b32  	%r152, %r150, %r151;
	shl.b64 	%rd47, %rd71, 2;
	sub.s64 	%rd48, %rd4, %rd47;
	st.local.u32 	[%rd48], %r152;
	add.s64 	%rd71, %rd71, -1;
	add.s64 	%rd70, %rd70, 4;
	add.s32 	%r630, %r630, 1;
	setp.ne.s32	%p2, %r630, 0;
	@%p2 bra 	BB0_3;

	ld.global.u8 	%r154, [%rd11+20];
	shl.b32 	%r155, %r154, 24;
	ld.global.u8 	%r156, [%rd11+21];
	shl.b32 	%r157, %r156, 16;
	ld.global.u8 	%rs5, [%rd11+22];
	mul.wide.u16 	%r158, %rs5, 256;
	or.b32  	%r159, %r155, %r157;
	or.b32  	%r160, %r159, %r158;
	or.b32  	%r161, %r160, 128;
	st.local.u32 	[%rd4+52], %r161;
	mov.u32 	%r162, 0;
	st.local.u32 	[%rd4+56], %r162;
	mov.u32 	%r163, 440;
	st.local.u32 	[%rd4+60], %r163;
	ld.local.u32 	%r164, [%rd4+40];
	xor.b32  	%r165, %r164, %r1;
	ld.local.u32 	%r166, [%rd4+44];
	ld.local.u32 	%r632, [%rd4];
	st.local.u32 	[%rd4+40], %r165;
	xor.b32  	%r167, %r166, %r129;
	st.local.u32 	[%rd4+44], %r167;
	mov.u32 	%r631, -5;
	mov.u64 	%rd72, %rd4;

BB0_5:
	.pragma "nounroll";
	add.s64 	%rd22, %rd72, 4;
	ld.local.u32 	%r9, [%rd72+4];
	ld.local.u32 	%r170, [%rd72+56];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r9, %r9, 7;
	 shf.r.clamp.b32    t2, %r9, %r9, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r9, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r170, %r170, 17;
	 shf.r.clamp.b32    t2, %r170, %r170, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r170, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r168, t3, t1;
	}
	// inline asm
	ld.local.u32 	%r171, [%rd72+36];
	add.s32 	%r172, %r171, %r168;
	add.s32 	%r173, %r172, %r632;
	st.local.u32 	[%rd72+64], %r173;
	add.s32 	%r631, %r631, 1;
	setp.ne.s32	%p3, %r631, 0;
	mov.u64 	%rd72, %rd22;
	mov.u32 	%r632, %r9;
	@%p3 bra 	BB0_5;

	ld.local.u32 	%r634, [%rd4+20];
	mov.u32 	%r633, -6;
	mov.u64 	%rd73, %rd4;

BB0_7:
	.pragma "nounroll";
	ld.local.u32 	%r14, [%rd73+24];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r14, %r14, 7;
	 shf.r.clamp.b32    t2, %r14, %r14, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r14, 3;
	 xor.b32            %r175, t1, t2;
	}
	// inline asm
	ld.local.u32 	%r177, [%rd73+56];
	add.s32 	%r178, %r177, %r175;
	add.s32 	%r179, %r178, %r634;
	st.local.u32 	[%rd73+84], %r179;
	add.s64 	%rd73, %rd73, 4;
	add.s32 	%r633, %r633, 1;
	setp.ne.s32	%p4, %r633, 0;
	mov.u32 	%r634, %r14;
	@%p4 bra 	BB0_7;

	add.s64 	%rd25, %rd4, 32;
	add.s64 	%rd26, %rd4, 35;
	mov.u64 	%rd74, 0;
	bra.uni 	BB0_9;

BB0_14:
	ld.local.u32 	%r180, [%rd4+48];
	add.s32 	%r16, %r180, 1;
	ld.local.u32 	%r636, [%rd4+52];
	st.local.u32 	[%rd4+48], %r16;
	setp.eq.s32	%p8, %r16, 0;
	@%p8 bra 	BB0_16;
	bra.uni 	BB0_15;

BB0_16:
	add.s32 	%r636, %r636, 16;
	st.local.u32 	[%rd4+52], %r636;
	ld.local.u32 	%r181, [%rd4+80];
	add.s32 	%r635, %r181, 16;
	st.local.u32 	[%rd4+80], %r635;
	bra.uni 	BB0_17;

BB0_15:
	ld.local.u32 	%r635, [%rd4+80];

BB0_17:
	ld.local.u32 	%r314, [%rd4+76];
	ld.local.u32 	%r315, [%rd4+104];
	ld.local.u32 	%r316, [%rd4+12];
	ld.local.u32 	%r317, [%rd4+8];
	ld.local.u32 	%r318, [%rd4+4];
	ld.local.u32 	%r637, [%rd4];
	ld.local.u32 	%r319, [%rd4+28];
	ld.local.u32 	%r320, [%rd4+24];
	ld.local.u32 	%r321, [%rd4+20];
	ld.local.u32 	%r322, [%rd4+16];
	ld.local.u32 	%r323, [%rd4+44];
	ld.local.u32 	%r324, [%rd4+40];
	ld.local.u32 	%r325, [%rd4+36];
	ld.local.u32 	%r326, [%rd4+32];
	ld.local.u32 	%r204, [%rd4+60];
	ld.local.u32 	%r201, [%rd4+56];
	ld.local.u32 	%r213, [%rd4+72];
	ld.local.u32 	%r210, [%rd4+68];
	ld.local.u32 	%r207, [%rd4+64];
	add.s32 	%r216, %r314, 1;
	st.local.u32 	[%rd4+76], %r216;
	add.s32 	%r237, %r315, 1;
	st.local.u32 	[%rd4+104], %r237;
	st.local.v4.u32 	[%rd12], {%r637, %r318, %r317, %r316};
	st.local.v4.u32 	[%rd12+16], {%r322, %r321, %r320, %r319};
	st.local.v4.u32 	[%rd12+32], {%r326, %r325, %r324, %r323};
	add.s32 	%r626, %r180, 1;
	st.local.v4.u32 	[%rd12+48], {%r626, %r636, %r201, %r204};
	st.local.v4.u32 	[%rd12+64], {%r207, %r210, %r213, %r216};
	st.local.u32 	[%rd12+80], %r635;
	ld.local.u32 	%r222, [%rd4+84];
	ld.local.u32 	%r225, [%rd4+88];
	ld.local.u32 	%r228, [%rd4+92];
	ld.local.u32 	%r231, [%rd4+96];
	ld.local.u32 	%r196, [%rd4+100];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r216, %r216, 17;
	 shf.r.clamp.b32    t2, %r216, %r216, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r216, 10;
	 xor.b32            %r182, t1, t2;
	}
	// inline asm
	add.s32 	%r187, %r222, %r182;
	st.local.u32 	[%rd12+84], %r187;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r635, %r635, 17;
	 shf.r.clamp.b32    t2, %r635, %r635, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r635, 10;
	 xor.b32            %r184, t1, t2;
	}
	// inline asm
	add.s32 	%r189, %r225, %r184;
	st.local.u32 	[%rd12+88], %r189;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r187, %r187, 17;
	 shf.r.clamp.b32    t2, %r187, %r187, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r187, 10;
	 xor.b32            %r186, t1, t2;
	}
	// inline asm
	add.s32 	%r191, %r228, %r186;
	st.local.u32 	[%rd12+92], %r191;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r189, %r189, 17;
	 shf.r.clamp.b32    t2, %r189, %r189, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r189, 10;
	 xor.b32            %r188, t1, t2;
	}
	// inline asm
	add.s32 	%r193, %r231, %r188;
	st.local.u32 	[%rd12+96], %r193;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r191, %r191, 17;
	 shf.r.clamp.b32    t2, %r191, %r191, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r191, 10;
	 xor.b32            %r190, t1, t2;
	}
	// inline asm
	add.s32 	%r327, %r196, %r190;
	st.local.u32 	[%rd12+100], %r327;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r193, %r193, 17;
	 shf.r.clamp.b32    t2, %r193, %r193, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r193, 10;
	 xor.b32            %r192, t1, t2;
	}
	// inline asm
	add.s32 	%r328, %r237, %r192;
	st.local.u32 	[%rd12+104], %r328;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r16, %r16, 7;
	 shf.r.clamp.b32    t2, %r16, %r16, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r16, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r196, %r196, 17;
	 shf.r.clamp.b32    t2, %r196, %r196, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r196, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r194, t3, t1;
	}
	// inline asm
	add.s32 	%r329, %r635, %r194;
	add.s32 	%r330, %r329, %r323;
	st.local.u32 	[%rd12+108], %r330;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r636, %r636, 7;
	 shf.r.clamp.b32    t2, %r636, %r636, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r636, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r237, %r237, 17;
	 shf.r.clamp.b32    t2, %r237, %r237, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r197, t3, t1;
	}
	// inline asm
	add.s32 	%r331, %r187, %r197;
	add.s32 	%r332, %r331, %r16;
	st.local.u32 	[%rd12+112], %r332;
	ld.local.u32 	%r202, [%rd4+108];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r201, %r201, 7;
	 shf.r.clamp.b32    t2, %r201, %r201, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r201, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r202, %r202, 17;
	 shf.r.clamp.b32    t2, %r202, %r202, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r202, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r200, t3, t1;
	}
	// inline asm
	add.s32 	%r333, %r189, %r200;
	add.s32 	%r334, %r333, %r636;
	st.local.u32 	[%rd12+116], %r334;
	ld.local.u32 	%r205, [%rd4+112];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r204, %r204, 7;
	 shf.r.clamp.b32    t2, %r204, %r204, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r204, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r205, %r205, 17;
	 shf.r.clamp.b32    t2, %r205, %r205, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r205, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r203, t3, t1;
	}
	// inline asm
	add.s32 	%r335, %r191, %r203;
	add.s32 	%r336, %r335, %r201;
	st.local.u32 	[%rd12+120], %r336;
	ld.local.u32 	%r208, [%rd4+116];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r207, %r207, 7;
	 shf.r.clamp.b32    t2, %r207, %r207, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r207, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r208, %r208, 17;
	 shf.r.clamp.b32    t2, %r208, %r208, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r208, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r206, t3, t1;
	}
	// inline asm
	add.s32 	%r337, %r193, %r206;
	add.s32 	%r338, %r337, %r204;
	st.local.u32 	[%rd12+124], %r338;
	ld.local.u32 	%r211, [%rd4+120];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r210, %r210, 7;
	 shf.r.clamp.b32    t2, %r210, %r210, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r210, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r211, %r211, 17;
	 shf.r.clamp.b32    t2, %r211, %r211, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r211, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r209, t3, t1;
	}
	// inline asm
	add.s32 	%r339, %r327, %r209;
	add.s32 	%r340, %r339, %r207;
	st.local.u32 	[%rd12+128], %r340;
	ld.local.u32 	%r214, [%rd4+124];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r213, %r213, 7;
	 shf.r.clamp.b32    t2, %r213, %r213, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r213, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r214, %r214, 17;
	 shf.r.clamp.b32    t2, %r214, %r214, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r214, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r212, t3, t1;
	}
	// inline asm
	add.s32 	%r341, %r328, %r212;
	add.s32 	%r342, %r341, %r210;
	st.local.u32 	[%rd12+132], %r342;
	ld.local.u32 	%r217, [%rd4+128];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r216, %r216, 7;
	 shf.r.clamp.b32    t2, %r216, %r216, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r216, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r217, %r217, 17;
	 shf.r.clamp.b32    t2, %r217, %r217, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r217, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r215, t3, t1;
	}
	// inline asm
	add.s32 	%r343, %r330, %r215;
	add.s32 	%r344, %r343, %r213;
	st.local.u32 	[%rd12+136], %r344;
	ld.local.u32 	%r220, [%rd4+132];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r635, %r635, 7;
	 shf.r.clamp.b32    t2, %r635, %r635, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r635, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r220, %r220, 17;
	 shf.r.clamp.b32    t2, %r220, %r220, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r220, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r218, t3, t1;
	}
	// inline asm
	add.s32 	%r345, %r332, %r218;
	add.s32 	%r346, %r345, %r216;
	st.local.u32 	[%rd12+140], %r346;
	ld.local.u32 	%r223, [%rd4+136];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r222, %r222, 7;
	 shf.r.clamp.b32    t2, %r222, %r222, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r222, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r223, %r223, 17;
	 shf.r.clamp.b32    t2, %r223, %r223, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r223, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r221, t3, t1;
	}
	// inline asm
	add.s32 	%r347, %r334, %r221;
	add.s32 	%r348, %r347, %r635;
	st.local.u32 	[%rd12+144], %r348;
	ld.local.u32 	%r226, [%rd4+140];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r225, %r225, 7;
	 shf.r.clamp.b32    t2, %r225, %r225, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r225, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r226, %r226, 17;
	 shf.r.clamp.b32    t2, %r226, %r226, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r226, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r224, t3, t1;
	}
	// inline asm
	add.s32 	%r349, %r336, %r224;
	add.s32 	%r350, %r349, %r187;
	st.local.u32 	[%rd12+148], %r350;
	ld.local.u32 	%r229, [%rd4+144];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r228, %r228, 7;
	 shf.r.clamp.b32    t2, %r228, %r228, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r228, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r229, %r229, 17;
	 shf.r.clamp.b32    t2, %r229, %r229, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r229, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r227, t3, t1;
	}
	// inline asm
	add.s32 	%r351, %r338, %r227;
	add.s32 	%r352, %r351, %r189;
	st.local.u32 	[%rd12+152], %r352;
	ld.local.u32 	%r232, [%rd4+148];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r231, %r231, 7;
	 shf.r.clamp.b32    t2, %r231, %r231, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r231, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r232, %r232, 17;
	 shf.r.clamp.b32    t2, %r232, %r232, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r232, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r230, t3, t1;
	}
	// inline asm
	add.s32 	%r353, %r340, %r230;
	add.s32 	%r354, %r353, %r191;
	st.local.u32 	[%rd12+156], %r354;
	ld.local.u32 	%r235, [%rd4+152];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r196, %r196, 7;
	 shf.r.clamp.b32    t2, %r196, %r196, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r196, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r235, %r235, 17;
	 shf.r.clamp.b32    t2, %r235, %r235, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r235, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r233, t3, t1;
	}
	// inline asm
	add.s32 	%r355, %r342, %r233;
	add.s32 	%r356, %r355, %r193;
	st.local.u32 	[%rd12+160], %r356;
	ld.local.u32 	%r238, [%rd4+156];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r237, %r237, 7;
	 shf.r.clamp.b32    t2, %r237, %r237, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r238, %r238, 17;
	 shf.r.clamp.b32    t2, %r238, %r238, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r238, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r236, t3, t1;
	}
	// inline asm
	add.s32 	%r357, %r344, %r236;
	add.s32 	%r358, %r357, %r327;
	st.local.u32 	[%rd12+164], %r358;
	ld.local.u32 	%r241, [%rd4+160];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r202, %r202, 7;
	 shf.r.clamp.b32    t2, %r202, %r202, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r202, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r241, %r241, 17;
	 shf.r.clamp.b32    t2, %r241, %r241, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r239, t3, t1;
	}
	// inline asm
	add.s32 	%r359, %r346, %r239;
	add.s32 	%r360, %r359, %r328;
	st.local.u32 	[%rd12+168], %r360;
	ld.local.u32 	%r244, [%rd4+164];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r205, %r205, 7;
	 shf.r.clamp.b32    t2, %r205, %r205, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r205, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r244, %r244, 17;
	 shf.r.clamp.b32    t2, %r244, %r244, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r244, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r242, t3, t1;
	}
	// inline asm
	add.s32 	%r361, %r348, %r242;
	add.s32 	%r362, %r361, %r330;
	st.local.u32 	[%rd12+172], %r362;
	ld.local.u32 	%r247, [%rd4+168];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r208, %r208, 7;
	 shf.r.clamp.b32    t2, %r208, %r208, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r208, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r247, %r247, 17;
	 shf.r.clamp.b32    t2, %r247, %r247, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r247, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r245, t3, t1;
	}
	// inline asm
	add.s32 	%r363, %r350, %r245;
	add.s32 	%r364, %r363, %r332;
	st.local.u32 	[%rd12+176], %r364;
	ld.local.u32 	%r250, [%rd4+172];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r211, %r211, 7;
	 shf.r.clamp.b32    t2, %r211, %r211, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r211, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r250, %r250, 17;
	 shf.r.clamp.b32    t2, %r250, %r250, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r250, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r248, t3, t1;
	}
	// inline asm
	add.s32 	%r365, %r352, %r248;
	add.s32 	%r366, %r365, %r334;
	st.local.u32 	[%rd12+180], %r366;
	ld.local.u32 	%r253, [%rd4+176];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r214, %r214, 7;
	 shf.r.clamp.b32    t2, %r214, %r214, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r214, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 17;
	 shf.r.clamp.b32    t2, %r253, %r253, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r251, t3, t1;
	}
	// inline asm
	add.s32 	%r367, %r354, %r251;
	add.s32 	%r368, %r367, %r336;
	st.local.u32 	[%rd12+184], %r368;
	ld.local.u32 	%r256, [%rd4+180];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r217, %r217, 7;
	 shf.r.clamp.b32    t2, %r217, %r217, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r217, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r256, %r256, 17;
	 shf.r.clamp.b32    t2, %r256, %r256, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r256, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r254, t3, t1;
	}
	// inline asm
	add.s32 	%r369, %r356, %r254;
	add.s32 	%r370, %r369, %r338;
	st.local.u32 	[%rd12+188], %r370;
	ld.local.u32 	%r259, [%rd4+184];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r220, %r220, 7;
	 shf.r.clamp.b32    t2, %r220, %r220, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r220, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r259, %r259, 17;
	 shf.r.clamp.b32    t2, %r259, %r259, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r259, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r257, t3, t1;
	}
	// inline asm
	add.s32 	%r371, %r358, %r257;
	add.s32 	%r372, %r371, %r340;
	st.local.u32 	[%rd12+192], %r372;
	ld.local.u32 	%r262, [%rd4+188];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r223, %r223, 7;
	 shf.r.clamp.b32    t2, %r223, %r223, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r223, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r262, %r262, 17;
	 shf.r.clamp.b32    t2, %r262, %r262, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r262, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r260, t3, t1;
	}
	// inline asm
	add.s32 	%r373, %r360, %r260;
	add.s32 	%r374, %r373, %r342;
	st.local.u32 	[%rd12+196], %r374;
	ld.local.u32 	%r265, [%rd4+192];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r226, %r226, 7;
	 shf.r.clamp.b32    t2, %r226, %r226, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r226, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 17;
	 shf.r.clamp.b32    t2, %r265, %r265, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r263, t3, t1;
	}
	// inline asm
	add.s32 	%r375, %r362, %r263;
	add.s32 	%r376, %r375, %r344;
	st.local.u32 	[%rd12+200], %r376;
	ld.local.u32 	%r268, [%rd4+196];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r229, %r229, 7;
	 shf.r.clamp.b32    t2, %r229, %r229, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r229, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r268, %r268, 17;
	 shf.r.clamp.b32    t2, %r268, %r268, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r268, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r266, t3, t1;
	}
	// inline asm
	add.s32 	%r377, %r364, %r266;
	add.s32 	%r378, %r377, %r346;
	st.local.u32 	[%rd12+204], %r378;
	ld.local.u32 	%r271, [%rd4+200];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r232, %r232, 7;
	 shf.r.clamp.b32    t2, %r232, %r232, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r232, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r271, %r271, 17;
	 shf.r.clamp.b32    t2, %r271, %r271, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r271, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r269, t3, t1;
	}
	// inline asm
	add.s32 	%r379, %r366, %r269;
	add.s32 	%r380, %r379, %r348;
	st.local.u32 	[%rd12+208], %r380;
	ld.local.u32 	%r274, [%rd4+204];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r235, %r235, 7;
	 shf.r.clamp.b32    t2, %r235, %r235, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r235, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r274, %r274, 17;
	 shf.r.clamp.b32    t2, %r274, %r274, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r274, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r272, t3, t1;
	}
	// inline asm
	add.s32 	%r381, %r368, %r272;
	add.s32 	%r382, %r381, %r350;
	st.local.u32 	[%rd12+212], %r382;
	ld.local.u32 	%r277, [%rd4+208];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r238, %r238, 7;
	 shf.r.clamp.b32    t2, %r238, %r238, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r238, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 17;
	 shf.r.clamp.b32    t2, %r277, %r277, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r275, t3, t1;
	}
	// inline asm
	add.s32 	%r383, %r370, %r275;
	add.s32 	%r384, %r383, %r352;
	st.local.u32 	[%rd12+216], %r384;
	ld.local.u32 	%r280, [%rd4+212];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r241, %r241, 7;
	 shf.r.clamp.b32    t2, %r241, %r241, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r280, %r280, 17;
	 shf.r.clamp.b32    t2, %r280, %r280, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r280, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r278, t3, t1;
	}
	// inline asm
	add.s32 	%r385, %r372, %r278;
	add.s32 	%r386, %r385, %r354;
	st.local.u32 	[%rd12+220], %r386;
	ld.local.u32 	%r283, [%rd4+216];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r244, %r244, 7;
	 shf.r.clamp.b32    t2, %r244, %r244, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r244, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r283, %r283, 17;
	 shf.r.clamp.b32    t2, %r283, %r283, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r283, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r281, t3, t1;
	}
	// inline asm
	add.s32 	%r387, %r374, %r281;
	add.s32 	%r388, %r387, %r356;
	st.local.u32 	[%rd12+224], %r388;
	ld.local.u32 	%r286, [%rd4+220];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r247, %r247, 7;
	 shf.r.clamp.b32    t2, %r247, %r247, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r247, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r286, %r286, 17;
	 shf.r.clamp.b32    t2, %r286, %r286, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r286, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r284, t3, t1;
	}
	// inline asm
	add.s32 	%r389, %r376, %r284;
	add.s32 	%r390, %r389, %r358;
	st.local.u32 	[%rd12+228], %r390;
	ld.local.u32 	%r289, [%rd4+224];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r250, %r250, 7;
	 shf.r.clamp.b32    t2, %r250, %r250, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r250, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 17;
	 shf.r.clamp.b32    t2, %r289, %r289, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r287, t3, t1;
	}
	// inline asm
	add.s32 	%r391, %r378, %r287;
	add.s32 	%r392, %r391, %r360;
	st.local.u32 	[%rd12+232], %r392;
	ld.local.u32 	%r292, [%rd4+228];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r253, %r253, 7;
	 shf.r.clamp.b32    t2, %r253, %r253, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r292, %r292, 17;
	 shf.r.clamp.b32    t2, %r292, %r292, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r292, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r290, t3, t1;
	}
	// inline asm
	add.s32 	%r393, %r380, %r290;
	add.s32 	%r394, %r393, %r362;
	st.local.u32 	[%rd12+236], %r394;
	ld.local.u32 	%r295, [%rd4+232];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r256, %r256, 7;
	 shf.r.clamp.b32    t2, %r256, %r256, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r256, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r295, %r295, 17;
	 shf.r.clamp.b32    t2, %r295, %r295, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r295, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r293, t3, t1;
	}
	// inline asm
	add.s32 	%r395, %r382, %r293;
	add.s32 	%r396, %r395, %r364;
	st.local.u32 	[%rd12+240], %r396;
	ld.local.u32 	%r298, [%rd4+236];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r259, %r259, 7;
	 shf.r.clamp.b32    t2, %r259, %r259, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r259, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r298, %r298, 17;
	 shf.r.clamp.b32    t2, %r298, %r298, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r298, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r296, t3, t1;
	}
	// inline asm
	add.s32 	%r397, %r384, %r296;
	add.s32 	%r398, %r397, %r366;
	st.local.u32 	[%rd12+244], %r398;
	ld.local.u32 	%r301, [%rd4+240];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r262, %r262, 7;
	 shf.r.clamp.b32    t2, %r262, %r262, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r262, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 17;
	 shf.r.clamp.b32    t2, %r301, %r301, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r299, t3, t1;
	}
	// inline asm
	add.s32 	%r399, %r386, %r299;
	add.s32 	%r400, %r399, %r368;
	st.local.u32 	[%rd12+248], %r400;
	ld.local.u32 	%r304, [%rd4+244];
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 .reg .u32 t3;
	 shf.r.clamp.b32    t1, %r265, %r265, 7;
	 shf.r.clamp.b32    t2, %r265, %r265, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 3;
	 xor.b32            t3, t1, t2;
	 shf.r.clamp.b32    t1, %r304, %r304, 17;
	 shf.r.clamp.b32    t2, %r304, %r304, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r304, 10;
	 xor.b32            t1, t1, t2;
	 add.s32            %r302, t3, t1;
	}
	// inline asm
	add.s32 	%r401, %r388, %r302;
	add.s32 	%r402, %r401, %r370;
	st.local.u32 	[%rd12+252], %r402;
	mov.u32 	%r646, -1694144372;
	mov.u32 	%r645, 1359893119;
	mov.u32 	%r644, -1521486534;
	mov.u32 	%r643, 1013904242;
	mov.u32 	%r642, -1150833019;
	mov.u32 	%r641, 1779033703;
	mov.u32 	%r640, 528734635;
	mov.u32 	%r638, 1541459225;
	mov.u32 	%r639, %r162;
	bra.uni 	BB0_18;

BB0_19:
	ld.local.u32 	%r637, [%rd30+12];

BB0_18:
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r641, %r641, 2;
	 shf.r.clamp.b32    t2, %r641, %r641, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r641, %r641, 22;
	 xor.b32            %r403, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r645, %r645, 6;
	 shf.r.clamp.b32    t2, %r645, %r645, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r645, %r645, 25;
	 xor.b32            %r405, t1, t2;
	}
	// inline asm
	not.b32 	%r419, %r645;
	and.b32  	%r420, %r640, %r419;
	and.b32  	%r421, %r645, %r646;
	xor.b32  	%r422, %r420, %r421;
	mul.wide.u32 	%rd60, %r639, 4;
	mov.u64 	%rd61, k;
	add.s64 	%rd62, %rd61, %rd60;
	add.s32 	%r423, %r422, %r638;
	add.s32 	%r424, %r423, %r405;
	ld.const.u32 	%r425, [%rd62];
	add.s32 	%r426, %r424, %r425;
	add.s32 	%r427, %r426, %r637;
	add.s32 	%r638, %r427, %r644;
	xor.b32  	%r428, %r642, %r643;
	and.b32  	%r429, %r641, %r428;
	and.b32  	%r430, %r642, %r643;
	xor.b32  	%r431, %r429, %r430;
	add.s32 	%r432, %r403, %r431;
	add.s32 	%r644, %r432, %r427;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r644, %r644, 2;
	 shf.r.clamp.b32    t2, %r644, %r644, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r644, %r644, 22;
	 xor.b32            %r407, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r638, %r638, 6;
	 shf.r.clamp.b32    t2, %r638, %r638, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r638, %r638, 25;
	 xor.b32            %r409, t1, t2;
	}
	// inline asm
	and.b32  	%r433, %r638, %r645;
	not.b32 	%r434, %r638;
	and.b32  	%r435, %r646, %r434;
	xor.b32  	%r436, %r435, %r433;
	add.s32 	%r437, %r639, 1;
	mul.wide.u32 	%rd63, %r437, 4;
	add.s64 	%rd30, %rd12, %rd63;
	add.s32 	%r438, %r436, %r640;
	add.s32 	%r439, %r438, %r409;
	ld.const.u32 	%r440, [%rd62+4];
	add.s32 	%r441, %r439, %r440;
	ld.local.u32 	%r442, [%rd30];
	add.s32 	%r443, %r441, %r442;
	add.s32 	%r640, %r443, %r643;
	xor.b32  	%r444, %r641, %r642;
	and.b32  	%r445, %r644, %r444;
	and.b32  	%r446, %r641, %r642;
	xor.b32  	%r447, %r445, %r446;
	add.s32 	%r448, %r407, %r447;
	add.s32 	%r643, %r448, %r443;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r643, %r643, 2;
	 shf.r.clamp.b32    t2, %r643, %r643, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r643, %r643, 22;
	 xor.b32            %r411, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r640, %r640, 6;
	 shf.r.clamp.b32    t2, %r640, %r640, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r640, %r640, 25;
	 xor.b32            %r413, t1, t2;
	}
	// inline asm
	and.b32  	%r449, %r640, %r638;
	not.b32 	%r450, %r640;
	and.b32  	%r451, %r645, %r450;
	xor.b32  	%r452, %r451, %r449;
	add.s32 	%r453, %r452, %r646;
	add.s32 	%r454, %r453, %r413;
	ld.const.u32 	%r455, [%rd62+8];
	add.s32 	%r456, %r454, %r455;
	ld.local.v2.u32 	{%r457, %r458}, [%rd30+4];
	add.s32 	%r460, %r456, %r457;
	add.s32 	%r646, %r460, %r642;
	xor.b32  	%r461, %r644, %r641;
	and.b32  	%r462, %r643, %r461;
	and.b32  	%r463, %r644, %r641;
	xor.b32  	%r464, %r462, %r463;
	add.s32 	%r465, %r411, %r464;
	add.s32 	%r642, %r465, %r460;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r642, %r642, 2;
	 shf.r.clamp.b32    t2, %r642, %r642, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r642, %r642, 22;
	 xor.b32            %r415, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r646, %r646, 6;
	 shf.r.clamp.b32    t2, %r646, %r646, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r646, %r646, 25;
	 xor.b32            %r417, t1, t2;
	}
	// inline asm
	and.b32  	%r466, %r646, %r640;
	not.b32 	%r467, %r646;
	and.b32  	%r468, %r638, %r467;
	xor.b32  	%r469, %r468, %r466;
	add.s32 	%r470, %r469, %r645;
	add.s32 	%r471, %r470, %r417;
	ld.const.u32 	%r472, [%rd62+12];
	add.s32 	%r473, %r471, %r472;
	add.s32 	%r475, %r473, %r458;
	add.s32 	%r645, %r475, %r641;
	xor.b32  	%r476, %r643, %r644;
	and.b32  	%r477, %r642, %r476;
	and.b32  	%r478, %r643, %r644;
	xor.b32  	%r479, %r477, %r478;
	add.s32 	%r480, %r415, %r479;
	add.s32 	%r641, %r480, %r475;
	add.s32 	%r639, %r639, 4;
	setp.eq.s32	%p9, %r639, 64;
	@%p9 bra 	BB0_20;
	bra.uni 	BB0_19;

BB0_20:
	add.s64 	%rd74, %rd74, 1;
	add.s32 	%r44, %r642, -1150833019;
	add.s32 	%r45, %r643, 1013904242;
	add.s32 	%r46, %r644, -1521486534;
	setp.ne.s32	%p10, %r641, -1779033703;
	@%p10 bra 	BB0_9;

	mov.u32 	%r649, 0;
	mov.u32 	%r647, 8192;
	setp.lt.s32	%p11, %r44, 0;
	@%p11 bra 	BB0_22;

	shl.b32 	%r648, %r44, 1;
	mov.u32 	%r649, 1;
	mov.u32 	%r647, 8448;
	setp.lt.s32	%p12, %r648, 0;
	@%p12 bra 	BB0_54;

	shl.b32 	%r648, %r44, 2;
	mov.u32 	%r649, 2;
	mov.u32 	%r647, 8704;
	setp.lt.s32	%p13, %r648, 0;
	@%p13 bra 	BB0_54;

	shl.b32 	%r648, %r44, 3;
	mov.u32 	%r649, 3;
	mov.u32 	%r647, 8960;
	setp.lt.s32	%p14, %r648, 0;
	@%p14 bra 	BB0_54;

	shl.b32 	%r648, %r44, 4;
	mov.u32 	%r649, 4;
	mov.u32 	%r647, 9216;
	setp.lt.s32	%p15, %r648, 0;
	@%p15 bra 	BB0_54;

	shl.b32 	%r648, %r44, 5;
	mov.u32 	%r649, 5;
	mov.u32 	%r647, 9472;
	setp.lt.s32	%p16, %r648, 0;
	@%p16 bra 	BB0_54;

	shl.b32 	%r648, %r44, 6;
	mov.u32 	%r649, 6;
	mov.u32 	%r647, 9728;
	setp.lt.s32	%p17, %r648, 0;
	@%p17 bra 	BB0_54;

	shl.b32 	%r648, %r44, 7;
	mov.u32 	%r649, 7;
	mov.u32 	%r647, 9984;
	setp.lt.s32	%p18, %r648, 0;
	@%p18 bra 	BB0_54;

	shl.b32 	%r648, %r44, 8;
	mov.u32 	%r649, 8;
	mov.u32 	%r647, 10240;
	setp.lt.s32	%p19, %r648, 0;
	@%p19 bra 	BB0_54;

	shl.b32 	%r648, %r44, 9;
	mov.u32 	%r649, 9;
	mov.u32 	%r647, 10496;
	setp.lt.s32	%p20, %r648, 0;
	@%p20 bra 	BB0_54;

	shl.b32 	%r648, %r44, 10;
	mov.u32 	%r649, 10;
	mov.u32 	%r647, 10752;
	setp.lt.s32	%p21, %r648, 0;
	@%p21 bra 	BB0_54;

	shl.b32 	%r648, %r44, 11;
	mov.u32 	%r649, 11;
	mov.u32 	%r647, 11008;
	setp.lt.s32	%p22, %r648, 0;
	@%p22 bra 	BB0_54;

	shl.b32 	%r648, %r44, 12;
	mov.u32 	%r649, 12;
	mov.u32 	%r647, 11264;
	setp.lt.s32	%p23, %r648, 0;
	@%p23 bra 	BB0_54;

	shl.b32 	%r648, %r44, 13;
	mov.u32 	%r649, 13;
	mov.u32 	%r647, 11520;
	setp.lt.s32	%p24, %r648, 0;
	@%p24 bra 	BB0_54;

	shl.b32 	%r648, %r44, 14;
	mov.u32 	%r649, 14;
	mov.u32 	%r647, 11776;
	setp.lt.s32	%p25, %r648, 0;
	@%p25 bra 	BB0_54;

	shl.b32 	%r648, %r44, 15;
	mov.u32 	%r649, 15;
	mov.u32 	%r647, 12032;
	setp.lt.s32	%p26, %r648, 0;
	@%p26 bra 	BB0_54;

	shl.b32 	%r648, %r44, 16;
	mov.u32 	%r649, 16;
	mov.u32 	%r647, 12288;
	setp.lt.s32	%p27, %r648, 0;
	@%p27 bra 	BB0_54;

	shl.b32 	%r648, %r44, 17;
	mov.u32 	%r649, 17;
	mov.u32 	%r647, 12544;
	setp.lt.s32	%p28, %r648, 0;
	@%p28 bra 	BB0_54;

	shl.b32 	%r648, %r44, 18;
	mov.u32 	%r649, 18;
	mov.u32 	%r647, 12800;
	setp.lt.s32	%p29, %r648, 0;
	@%p29 bra 	BB0_54;

	shl.b32 	%r648, %r44, 19;
	mov.u32 	%r649, 19;
	mov.u32 	%r647, 13056;
	setp.lt.s32	%p30, %r648, 0;
	@%p30 bra 	BB0_54;

	shl.b32 	%r648, %r44, 20;
	mov.u32 	%r649, 20;
	mov.u32 	%r647, 13312;
	setp.lt.s32	%p31, %r648, 0;
	@%p31 bra 	BB0_54;

	shl.b32 	%r648, %r44, 21;
	mov.u32 	%r649, 21;
	mov.u32 	%r647, 13568;
	setp.lt.s32	%p32, %r648, 0;
	@%p32 bra 	BB0_54;

	shl.b32 	%r648, %r44, 22;
	mov.u32 	%r649, 22;
	mov.u32 	%r647, 13824;
	setp.lt.s32	%p33, %r648, 0;
	@%p33 bra 	BB0_54;

	shl.b32 	%r648, %r44, 23;
	mov.u32 	%r649, 23;
	mov.u32 	%r647, 14080;
	setp.lt.s32	%p34, %r648, 0;
	@%p34 bra 	BB0_54;

	shl.b32 	%r648, %r44, 24;
	mov.u32 	%r649, 24;
	mov.u32 	%r647, 14336;
	setp.lt.s32	%p35, %r648, 0;
	@%p35 bra 	BB0_54;

	shl.b32 	%r648, %r44, 25;
	mov.u32 	%r649, 25;
	mov.u32 	%r647, 14592;
	setp.lt.s32	%p36, %r648, 0;
	@%p36 bra 	BB0_54;

	shl.b32 	%r648, %r44, 26;
	mov.u32 	%r649, 26;
	mov.u32 	%r647, 14848;
	setp.lt.s32	%p37, %r648, 0;
	@%p37 bra 	BB0_54;

	shl.b32 	%r648, %r44, 27;
	mov.u32 	%r649, 27;
	mov.u32 	%r647, 15104;
	setp.lt.s32	%p38, %r648, 0;
	@%p38 bra 	BB0_54;

	shl.b32 	%r648, %r44, 28;
	mov.u32 	%r649, 28;
	mov.u32 	%r647, 15360;
	setp.lt.s32	%p39, %r648, 0;
	@%p39 bra 	BB0_54;

	shl.b32 	%r648, %r44, 29;
	mov.u32 	%r649, 29;
	mov.u32 	%r647, 15616;
	setp.lt.s32	%p40, %r648, 0;
	@%p40 bra 	BB0_54;

	shl.b32 	%r648, %r44, 30;
	mov.u32 	%r649, 30;
	mov.u32 	%r647, 15872;
	setp.lt.s32	%p41, %r648, 0;
	@%p41 bra 	BB0_54;

	shl.b32 	%r648, %r44, 31;
	setp.eq.s32	%p42, %r648, 0;
	mov.u32 	%r649, 31;
	mov.u32 	%r647, 16128;
	@%p42 bra 	BB0_57;
	bra.uni 	BB0_54;

BB0_57:
	mov.u32 	%r652, 0;
	mov.u32 	%r650, 16384;
	setp.lt.s32	%p45, %r45, 0;
	mov.u32 	%r651, %r45;
	@%p45 bra 	BB0_89;

	shl.b32 	%r651, %r45, 1;
	mov.u32 	%r652, 1;
	mov.u32 	%r650, 16640;
	setp.lt.s32	%p46, %r651, 0;
	@%p46 bra 	BB0_89;

	shl.b32 	%r651, %r45, 2;
	mov.u32 	%r652, 2;
	mov.u32 	%r650, 16896;
	setp.lt.s32	%p47, %r651, 0;
	@%p47 bra 	BB0_89;

	shl.b32 	%r651, %r45, 3;
	mov.u32 	%r652, 3;
	mov.u32 	%r650, 17152;
	setp.lt.s32	%p48, %r651, 0;
	@%p48 bra 	BB0_89;

	shl.b32 	%r651, %r45, 4;
	mov.u32 	%r652, 4;
	mov.u32 	%r650, 17408;
	setp.lt.s32	%p49, %r651, 0;
	@%p49 bra 	BB0_89;

	shl.b32 	%r651, %r45, 5;
	mov.u32 	%r652, 5;
	mov.u32 	%r650, 17664;
	setp.lt.s32	%p50, %r651, 0;
	@%p50 bra 	BB0_89;

	shl.b32 	%r651, %r45, 6;
	mov.u32 	%r652, 6;
	mov.u32 	%r650, 17920;
	setp.lt.s32	%p51, %r651, 0;
	@%p51 bra 	BB0_89;

	shl.b32 	%r651, %r45, 7;
	mov.u32 	%r652, 7;
	mov.u32 	%r650, 18176;
	setp.lt.s32	%p52, %r651, 0;
	@%p52 bra 	BB0_89;

	shl.b32 	%r651, %r45, 8;
	mov.u32 	%r652, 8;
	mov.u32 	%r650, 18432;
	setp.lt.s32	%p53, %r651, 0;
	@%p53 bra 	BB0_89;

	shl.b32 	%r651, %r45, 9;
	mov.u32 	%r652, 9;
	mov.u32 	%r650, 18688;
	setp.lt.s32	%p54, %r651, 0;
	@%p54 bra 	BB0_89;

	shl.b32 	%r651, %r45, 10;
	mov.u32 	%r652, 10;
	mov.u32 	%r650, 18944;
	setp.lt.s32	%p55, %r651, 0;
	@%p55 bra 	BB0_89;

	shl.b32 	%r651, %r45, 11;
	mov.u32 	%r652, 11;
	mov.u32 	%r650, 19200;
	setp.lt.s32	%p56, %r651, 0;
	@%p56 bra 	BB0_89;

	shl.b32 	%r651, %r45, 12;
	mov.u32 	%r652, 12;
	mov.u32 	%r650, 19456;
	setp.lt.s32	%p57, %r651, 0;
	@%p57 bra 	BB0_89;

	shl.b32 	%r651, %r45, 13;
	mov.u32 	%r652, 13;
	mov.u32 	%r650, 19712;
	setp.lt.s32	%p58, %r651, 0;
	@%p58 bra 	BB0_89;

	shl.b32 	%r651, %r45, 14;
	mov.u32 	%r652, 14;
	mov.u32 	%r650, 19968;
	setp.lt.s32	%p59, %r651, 0;
	@%p59 bra 	BB0_89;

	shl.b32 	%r651, %r45, 15;
	mov.u32 	%r652, 15;
	mov.u32 	%r650, 20224;
	setp.lt.s32	%p60, %r651, 0;
	@%p60 bra 	BB0_89;

	shl.b32 	%r651, %r45, 16;
	mov.u32 	%r652, 16;
	mov.u32 	%r650, 20480;
	setp.lt.s32	%p61, %r651, 0;
	@%p61 bra 	BB0_89;

	shl.b32 	%r651, %r45, 17;
	mov.u32 	%r652, 17;
	mov.u32 	%r650, 20736;
	setp.lt.s32	%p62, %r651, 0;
	@%p62 bra 	BB0_89;

	shl.b32 	%r651, %r45, 18;
	mov.u32 	%r652, 18;
	mov.u32 	%r650, 20992;
	setp.lt.s32	%p63, %r651, 0;
	@%p63 bra 	BB0_89;

	shl.b32 	%r651, %r45, 19;
	mov.u32 	%r652, 19;
	mov.u32 	%r650, 21248;
	setp.lt.s32	%p64, %r651, 0;
	@%p64 bra 	BB0_89;

	shl.b32 	%r651, %r45, 20;
	mov.u32 	%r652, 20;
	mov.u32 	%r650, 21504;
	setp.lt.s32	%p65, %r651, 0;
	@%p65 bra 	BB0_89;

	shl.b32 	%r651, %r45, 21;
	mov.u32 	%r652, 21;
	mov.u32 	%r650, 21760;
	setp.lt.s32	%p66, %r651, 0;
	@%p66 bra 	BB0_89;

	shl.b32 	%r651, %r45, 22;
	mov.u32 	%r652, 22;
	mov.u32 	%r650, 22016;
	setp.lt.s32	%p67, %r651, 0;
	@%p67 bra 	BB0_89;

	shl.b32 	%r651, %r45, 23;
	mov.u32 	%r652, 23;
	mov.u32 	%r650, 22272;
	setp.lt.s32	%p68, %r651, 0;
	@%p68 bra 	BB0_89;

	shl.b32 	%r651, %r45, 24;
	mov.u32 	%r652, 24;
	mov.u32 	%r650, 22528;
	setp.lt.s32	%p69, %r651, 0;
	@%p69 bra 	BB0_89;

	shl.b32 	%r651, %r45, 25;
	mov.u32 	%r652, 25;
	mov.u32 	%r650, 22784;
	setp.lt.s32	%p70, %r651, 0;
	@%p70 bra 	BB0_89;

	shl.b32 	%r651, %r45, 26;
	mov.u32 	%r652, 26;
	mov.u32 	%r650, 23040;
	setp.lt.s32	%p71, %r651, 0;
	@%p71 bra 	BB0_89;

	shl.b32 	%r651, %r45, 27;
	mov.u32 	%r652, 27;
	mov.u32 	%r650, 23296;
	setp.lt.s32	%p72, %r651, 0;
	@%p72 bra 	BB0_89;

	shl.b32 	%r651, %r45, 28;
	mov.u32 	%r652, 28;
	mov.u32 	%r650, 23552;
	setp.lt.s32	%p73, %r651, 0;
	@%p73 bra 	BB0_89;

	shl.b32 	%r651, %r45, 29;
	mov.u32 	%r652, 29;
	mov.u32 	%r650, 23808;
	setp.lt.s32	%p74, %r651, 0;
	@%p74 bra 	BB0_89;

	shl.b32 	%r651, %r45, 30;
	mov.u32 	%r652, 30;
	mov.u32 	%r650, 24064;
	setp.lt.s32	%p75, %r651, 0;
	@%p75 bra 	BB0_89;

	shl.b32 	%r651, %r45, 31;
	setp.eq.s32	%p76, %r651, 0;
	mov.u32 	%r652, 31;
	mov.u32 	%r650, 24320;
	mov.u32 	%r653, 96;
	@%p76 bra 	BB0_92;

BB0_89:
	setp.eq.s32	%p77, %r652, 31;
	@%p77 bra 	BB0_91;
	bra.uni 	BB0_90;

BB0_91:
	shr.u32 	%r623, %r46, 24;
	add.s32 	%r653, %r650, %r623;
	bra.uni 	BB0_92;

BB0_22:
	mov.u32 	%r648, %r44;

BB0_54:
	setp.eq.s32	%p43, %r649, 31;
	@%p43 bra 	BB0_56;
	bra.uni 	BB0_55;

BB0_56:
	shr.u32 	%r551, %r45, 24;
	add.s32 	%r653, %r647, %r551;
	bra.uni 	BB0_92;

BB0_55:
	bfe.u32 	%r545, %r648, 23, 8;
	add.s32 	%r546, %r545, %r647;
	mov.u32 	%r547, 56;
	sub.s32 	%r548, %r547, %r649;
	shr.u32 	%r549, %r45, %r548;
	setp.gt.u32	%p44, %r649, 23;
	selp.b32	%r550, %r549, 0, %p44;
	add.s32 	%r653, %r546, %r550;

BB0_92:
	setp.le.u32	%p79, %r653, %r128;
	@%p79 bra 	BB0_9;

	mov.u16 	%rs7, 1;
	st.volatile.global.u8 	[%rd10], %rs7;
	st.volatile.global.u8 	[%rd2], %rs7;
	add.s32 	%r120, %r638, 1541459225;
	add.s32 	%r121, %r640, 528734635;
	add.s32 	%r122, %r646, -1694144372;
	mov.u32 	%r654, 0;
	mov.u64 	%rd75, %rd11;
	mov.u64 	%rd76, %rd26;

BB0_94:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs8, %rs9, %rs10, %rs11}, [%rd76+-3];
	st.global.u8 	[%rd75], %rs11;
	st.global.u8 	[%rd75+1], %rs10;
	st.global.u8 	[%rd75+2], %rs9;
	st.global.u8 	[%rd75+3], %rs8;
	add.s32 	%r654, %r654, 4;
	setp.lt.u32	%p80, %r654, 20;
	cvt.u64.u32	%rd64, %r654;
	add.s64 	%rd65, %rd64, %rd25;
	add.s64 	%rd76, %rd65, 3;
	add.s64 	%rd75, %rd11, %rd64;
	@%p80 bra 	BB0_94;

	ld.local.v2.u8 	{%rs17, %rs18}, [%rd4+54];
	ld.local.u8 	%rs21, [%rd4+53];
	st.global.u8 	[%rd11+20], %rs18;
	st.global.u8 	[%rd11+21], %rs17;
	st.global.u8 	[%rd11+22], %rs21;
	mov.u32 	%r655, 0;
	add.s32 	%r627, %r642, -1150833019;
	st.local.v4.u32 	[%rd13], {%r655, %r627, %r45, %r46};
	add.s32 	%r628, %r645, 1359893119;
	st.local.v4.u32 	[%rd13+16], {%r628, %r122, %r121, %r120};
	mov.u16 	%rs27, 0;
	bra.uni 	BB0_96;

BB0_97:
	ld.local.u8 	%rs27, [%rd36+4];

BB0_96:
	cvt.u64.u32	%rd66, %r655;
	add.s64 	%rd36, %rd13, %rd66;
	ld.local.v2.u8 	{%rs22, %rs23}, [%rd36+2];
	add.s64 	%rd67, %rd3, %rd66;
	ld.local.u8 	%rs26, [%rd36+1];
	st.global.u8 	[%rd67], %rs23;
	st.global.u8 	[%rd67+1], %rs22;
	st.global.u8 	[%rd67+2], %rs26;
	st.global.u8 	[%rd67+3], %rs27;
	add.s32 	%r655, %r655, 4;
	setp.gt.u32	%p81, %r655, 31;
	@%p81 bra 	BB0_9;
	bra.uni 	BB0_97;

BB0_90:
	bfe.u32 	%r617, %r651, 23, 8;
	add.s32 	%r618, %r617, %r650;
	mov.u32 	%r619, 56;
	sub.s32 	%r620, %r619, %r652;
	shr.u32 	%r621, %r46, %r620;
	setp.gt.u32	%p78, %r652, 23;
	selp.b32	%r622, %r621, 0, %p78;
	add.s32 	%r653, %r618, %r622;
	bra.uni 	BB0_92;

BB0_9:
	mul.hi.s64 	%rd51, %rd74, -6640827866535438581;
	add.s64 	%rd52, %rd51, %rd74;
	shr.u64 	%rd53, %rd52, 63;
	shr.s64 	%rd54, %rd52, 6;
	add.s64 	%rd55, %rd54, %rd53;
	mul.lo.s64 	%rd56, %rd55, 100;
	sub.s64 	%rd57, %rd74, %rd56;
	setp.ne.s64	%p5, %rd57, 9;
	@%p5 bra 	BB0_14;

	ld.volatile.global.u8 	%rs6, [%rd2];
	setp.eq.s16	%p6, %rs6, 0;
	@%p6 bra 	BB0_14;

	shl.b64 	%rd58, %rd14, 3;
	add.s64 	%rd59, %rd1, %rd58;
	st.global.u64 	[%rd59], %rd74;
	setp.ne.s32	%p7, %r1, 1;
	@%p7 bra 	BB0_13;

	st.volatile.global.u64 	[%rd9], %rd74;

BB0_13:
	ret;
}


