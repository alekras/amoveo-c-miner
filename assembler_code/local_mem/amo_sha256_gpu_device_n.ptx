//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-23083092
// Cuda compilation tools, release 9.1, V9.1.85
// Based on LLVM 3.4svn
//

.version 6.1
.target sm_61
.address_size 64

	// .globl	_Z13kernel_sha256PhjS_PVbS1_PVljPl
.const .align 4 .b8 k[256] = {152, 47, 138, 66, 145, 68, 55, 113, 207, 251, 192, 181, 165, 219, 181, 233, 91, 194, 86, 57, 241, 17, 241, 89, 164, 130, 63, 146, 213, 94, 28, 171, 152, 170, 7, 216, 1, 91, 131, 18, 190, 133, 49, 36, 195, 125, 12, 85, 116, 93, 190, 114, 254, 177, 222, 128, 167, 6, 220, 155, 116, 241, 155, 193, 193, 105, 155, 228, 134, 71, 190, 239, 198, 157, 193, 15, 204, 161, 12, 36, 111, 44, 233, 45, 170, 132, 116, 74, 220, 169, 176, 92, 218, 136, 249, 118, 82, 81, 62, 152, 109, 198, 49, 168, 200, 39, 3, 176, 199, 127, 89, 191, 243, 11, 224, 198, 71, 145, 167, 213, 81, 99, 202, 6, 103, 41, 41, 20, 133, 10, 183, 39, 56, 33, 27, 46, 252, 109, 44, 77, 19, 13, 56, 83, 84, 115, 10, 101, 187, 10, 106, 118, 46, 201, 194, 129, 133, 44, 114, 146, 161, 232, 191, 162, 75, 102, 26, 168, 112, 139, 75, 194, 163, 81, 108, 199, 25, 232, 146, 209, 36, 6, 153, 214, 133, 53, 14, 244, 112, 160, 106, 16, 22, 193, 164, 25, 8, 108, 55, 30, 76, 119, 72, 39, 181, 188, 176, 52, 179, 12, 28, 57, 74, 170, 216, 78, 79, 202, 156, 91, 243, 111, 46, 104, 238, 130, 143, 116, 111, 99, 165, 120, 20, 120, 200, 132, 8, 2, 199, 140, 250, 255, 190, 144, 235, 108, 80, 164, 247, 163, 249, 190, 242, 120, 113, 198};

.visible .entry _Z13kernel_sha256PhjS_PVbS1_PVljPl(
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5,
	.param .u32 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6,
	.param .u64 _Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7
)
{
	.local .align 16 .b8 	__local_depot0[352];
	.reg .b64 	%SP;
	.reg .b64 	%SPL;
	.reg .pred 	%p<91>;
	.reg .b16 	%rs<27>;
	.reg .b32 	%r<625>;
	.reg .b64 	%rd<104>;


	mov.u64 	%rd103, __local_depot0;
	cvta.local.u64 	%SP, %rd103;
	ld.param.u64 	%rd34, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_0];
	ld.param.u64 	%rd31, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_2];
	ld.param.u64 	%rd32, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_3];
	ld.param.u64 	%rd35, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_4];
	ld.param.u32 	%r93, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_6];
	cvta.to.global.u64 	%rd2, %rd35;
	cvta.to.global.u64 	%rd3, %rd34;
	add.u64 	%rd37, %SP, 256;
	cvta.to.local.u64 	%rd4, %rd37;
	mov.u32 	%r95, %ntid.x;
	mov.u32 	%r96, %ctaid.x;
	mov.u32 	%r97, %tid.x;
	mad.lo.s32 	%r1, %r95, %r96, %r97;
	mov.u32 	%r603, -8;
	mov.u64 	%rd94, %rd4;
	mov.u64 	%rd95, %rd3;

BB0_1:
	.pragma "nounroll";
	ld.global.u8 	%r98, [%rd95];
	shl.b32 	%r99, %r98, 24;
	ld.global.u8 	%r100, [%rd95+1];
	shl.b32 	%r101, %r100, 16;
	or.b32  	%r102, %r101, %r99;
	ld.global.u8 	%rs1, [%rd95+2];
	mul.wide.u16 	%r103, %rs1, 256;
	or.b32  	%r104, %r102, %r103;
	ld.global.u8 	%r105, [%rd95+3];
	or.b32  	%r106, %r104, %r105;
	st.local.u32 	[%rd94], %r106;
	add.s64 	%rd95, %rd95, 4;
	add.s64 	%rd94, %rd94, 4;
	add.s32 	%r603, %r603, 1;
	setp.ne.s32	%p2, %r603, 0;
	@%p2 bra 	BB0_1;

	cvta.to.global.u64 	%rd10, %rd32;
	cvta.to.global.u64 	%rd11, %rd31;
	add.u64 	%rd39, %SP, 0;
	cvta.to.local.u64 	%rd12, %rd39;
	mov.u32 	%r604, -5;
	mov.u64 	%rd96, -8;
	mov.u64 	%rd97, %rd11;

BB0_3:
	.pragma "nounroll";
	ld.global.u8 	%r108, [%rd97];
	shl.b32 	%r109, %r108, 24;
	ld.global.u8 	%r110, [%rd97+1];
	shl.b32 	%r111, %r110, 16;
	or.b32  	%r112, %r111, %r109;
	ld.global.u8 	%rs2, [%rd97+2];
	mul.wide.u16 	%r113, %rs2, 256;
	or.b32  	%r114, %r112, %r113;
	ld.global.u8 	%r115, [%rd97+3];
	or.b32  	%r116, %r114, %r115;
	shl.b64 	%rd40, %rd96, 2;
	sub.s64 	%rd41, %rd4, %rd40;
	st.local.u32 	[%rd41], %r116;
	add.s64 	%rd97, %rd97, 4;
	add.s64 	%rd96, %rd96, -1;
	add.s32 	%r604, %r604, 1;
	setp.ne.s32	%p3, %r604, 0;
	@%p3 bra 	BB0_3;

	ld.global.u8 	%r117, [%rd11+20];
	shl.b32 	%r118, %r117, 24;
	ld.global.u8 	%r119, [%rd11+21];
	shl.b32 	%r120, %r119, 16;
	ld.global.u8 	%rs3, [%rd11+22];
	mul.wide.u16 	%r121, %rs3, 256;
	or.b32  	%r122, %r118, %r120;
	or.b32  	%r123, %r122, %r121;
	or.b32  	%r124, %r123, 128;
	st.local.u32 	[%rd4+52], %r124;
	mov.u32 	%r125, 0;
	st.local.u32 	[%rd4+56], %r125;
	mov.u32 	%r126, 440;
	st.local.u32 	[%rd4+60], %r126;
	ld.local.u32 	%r127, [%rd4+36];
	xor.b32  	%r128, %r127, %r1;
	ld.local.u32 	%r129, [%rd4+40];
	st.local.u32 	[%rd4+36], %r128;
	xor.b32  	%r130, %r129, %r93;
	st.local.u32 	[%rd4+40], %r130;
	mov.u64 	%rd98, 0;
	bra.uni 	BB0_5;

BB0_10:
	ld.local.u32 	%r131, [%rd4+44];
	add.s32 	%r6, %r131, 1;
	ld.local.u32 	%r605, [%rd4+48];
	st.local.u32 	[%rd4+44], %r6;
	setp.ne.s32	%p7, %r6, 0;
	@%p7 bra 	BB0_12;

	add.s32 	%r605, %r605, 1;
	st.local.u32 	[%rd4+48], %r605;

BB0_12:
	mov.u32 	%r607, 0;
	mov.u64 	%rd52, 3144134277;
	st.local.u32 	[%rd4+68], %rd52;
	mov.u64 	%rd53, 1779033703;
	st.local.u32 	[%rd4+64], %rd53;
	mov.u64 	%rd54, 2773480762;
	st.local.u32 	[%rd4+76], %rd54;
	mov.u64 	%rd55, 1013904242;
	st.local.u32 	[%rd4+72], %rd55;
	mov.u64 	%rd56, 2600822924;
	st.local.u32 	[%rd4+84], %rd56;
	mov.u64 	%rd57, 1359893119;
	st.local.u32 	[%rd4+80], %rd57;
	mov.u64 	%rd58, 1541459225;
	st.local.u32 	[%rd4+92], %rd58;
	mov.u64 	%rd59, 528734635;
	st.local.u32 	[%rd4+88], %rd59;
	ld.local.u32 	%r606, [%rd4];
	ld.local.u32 	%r141, [%rd4+12];
	ld.local.u32 	%r137, [%rd4+8];
	ld.local.u32 	%r133, [%rd4+4];
	ld.local.u32 	%r157, [%rd4+28];
	ld.local.u32 	%r153, [%rd4+24];
	ld.local.u32 	%r149, [%rd4+20];
	ld.local.u32 	%r145, [%rd4+16];
	ld.local.u32 	%r169, [%rd4+40];
	ld.local.u32 	%r165, [%rd4+36];
	ld.local.u32 	%r161, [%rd4+32];
	ld.local.u32 	%r139, [%rd4+60];
	ld.local.u32 	%r135, [%rd4+56];
	ld.local.u32 	%r181, [%rd4+52];
	st.local.v4.u32 	[%rd12], {%r606, %r133, %r137, %r141};
	st.local.v4.u32 	[%rd12+16], {%r145, %r149, %r153, %r157};
	add.s32 	%r595, %r131, 1;
	st.local.v4.u32 	[%rd12+32], {%r161, %r165, %r169, %r595};
	st.local.v4.u32 	[%rd12+48], {%r605, %r181, %r135, %r139};
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r133, %r133, 7;
	 shf.r.clamp.b32    t2, %r133, %r133, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r133, 3;
	 xor.b32            %r132, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r135, %r135, 17;
	 shf.r.clamp.b32    t2, %r135, %r135, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r135, 10;
	 xor.b32            %r134, t1, t2;
	}
	// inline asm
	add.s32 	%r333, %r134, %r132;
	add.s32 	%r334, %r333, %r165;
	add.s32 	%r193, %r334, %r606;
	st.local.u32 	[%rd12+64], %r193;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r137, %r137, 7;
	 shf.r.clamp.b32    t2, %r137, %r137, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r137, 3;
	 xor.b32            %r136, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r139, %r139, 17;
	 shf.r.clamp.b32    t2, %r139, %r139, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r139, 10;
	 xor.b32            %r138, t1, t2;
	}
	// inline asm
	add.s32 	%r335, %r138, %r136;
	add.s32 	%r336, %r335, %r169;
	add.s32 	%r197, %r336, %r133;
	st.local.u32 	[%rd12+68], %r197;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r141, %r141, 7;
	 shf.r.clamp.b32    t2, %r141, %r141, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r141, 3;
	 xor.b32            %r140, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r193, %r193, 17;
	 shf.r.clamp.b32    t2, %r193, %r193, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r193, 10;
	 xor.b32            %r142, t1, t2;
	}
	// inline asm
	add.s32 	%r337, %r142, %r140;
	add.s32 	%r338, %r337, %r6;
	add.s32 	%r201, %r338, %r137;
	st.local.u32 	[%rd12+72], %r201;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r145, %r145, 7;
	 shf.r.clamp.b32    t2, %r145, %r145, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r145, 3;
	 xor.b32            %r144, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r197, %r197, 17;
	 shf.r.clamp.b32    t2, %r197, %r197, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r197, 10;
	 xor.b32            %r146, t1, t2;
	}
	// inline asm
	add.s32 	%r339, %r146, %r144;
	add.s32 	%r340, %r339, %r605;
	add.s32 	%r205, %r340, %r141;
	st.local.u32 	[%rd12+76], %r205;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r149, %r149, 7;
	 shf.r.clamp.b32    t2, %r149, %r149, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r149, 3;
	 xor.b32            %r148, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r201, %r201, 17;
	 shf.r.clamp.b32    t2, %r201, %r201, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r201, 10;
	 xor.b32            %r150, t1, t2;
	}
	// inline asm
	add.s32 	%r341, %r150, %r148;
	add.s32 	%r342, %r341, %r181;
	add.s32 	%r209, %r342, %r145;
	st.local.u32 	[%rd12+80], %r209;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r153, %r153, 7;
	 shf.r.clamp.b32    t2, %r153, %r153, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r153, 3;
	 xor.b32            %r152, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r205, %r205, 17;
	 shf.r.clamp.b32    t2, %r205, %r205, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r205, 10;
	 xor.b32            %r154, t1, t2;
	}
	// inline asm
	add.s32 	%r343, %r154, %r152;
	add.s32 	%r344, %r343, %r135;
	add.s32 	%r213, %r344, %r149;
	st.local.u32 	[%rd12+84], %r213;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r157, %r157, 7;
	 shf.r.clamp.b32    t2, %r157, %r157, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r157, 3;
	 xor.b32            %r156, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r209, %r209, 17;
	 shf.r.clamp.b32    t2, %r209, %r209, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r209, 10;
	 xor.b32            %r158, t1, t2;
	}
	// inline asm
	add.s32 	%r345, %r158, %r156;
	add.s32 	%r346, %r345, %r139;
	add.s32 	%r217, %r346, %r153;
	st.local.u32 	[%rd12+88], %r217;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r161, %r161, 7;
	 shf.r.clamp.b32    t2, %r161, %r161, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r161, 3;
	 xor.b32            %r160, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r213, %r213, 17;
	 shf.r.clamp.b32    t2, %r213, %r213, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r213, 10;
	 xor.b32            %r162, t1, t2;
	}
	// inline asm
	add.s32 	%r347, %r162, %r160;
	add.s32 	%r348, %r347, %r193;
	add.s32 	%r221, %r348, %r157;
	st.local.u32 	[%rd12+92], %r221;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r165, %r165, 7;
	 shf.r.clamp.b32    t2, %r165, %r165, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r165, 3;
	 xor.b32            %r164, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r217, %r217, 17;
	 shf.r.clamp.b32    t2, %r217, %r217, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r217, 10;
	 xor.b32            %r166, t1, t2;
	}
	// inline asm
	add.s32 	%r349, %r166, %r164;
	add.s32 	%r350, %r349, %r197;
	add.s32 	%r225, %r350, %r161;
	st.local.u32 	[%rd12+96], %r225;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r169, %r169, 7;
	 shf.r.clamp.b32    t2, %r169, %r169, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r169, 3;
	 xor.b32            %r168, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r221, %r221, 17;
	 shf.r.clamp.b32    t2, %r221, %r221, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r221, 10;
	 xor.b32            %r170, t1, t2;
	}
	// inline asm
	add.s32 	%r351, %r170, %r168;
	add.s32 	%r352, %r351, %r201;
	add.s32 	%r229, %r352, %r165;
	st.local.u32 	[%rd12+100], %r229;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r6, %r6, 7;
	 shf.r.clamp.b32    t2, %r6, %r6, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r6, 3;
	 xor.b32            %r172, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r225, %r225, 17;
	 shf.r.clamp.b32    t2, %r225, %r225, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r225, 10;
	 xor.b32            %r174, t1, t2;
	}
	// inline asm
	add.s32 	%r353, %r174, %r172;
	add.s32 	%r354, %r353, %r205;
	add.s32 	%r233, %r354, %r169;
	st.local.u32 	[%rd12+104], %r233;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r605, %r605, 7;
	 shf.r.clamp.b32    t2, %r605, %r605, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r605, 3;
	 xor.b32            %r176, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r229, %r229, 17;
	 shf.r.clamp.b32    t2, %r229, %r229, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r229, 10;
	 xor.b32            %r178, t1, t2;
	}
	// inline asm
	add.s32 	%r355, %r178, %r176;
	add.s32 	%r356, %r355, %r209;
	add.s32 	%r237, %r356, %r6;
	st.local.u32 	[%rd12+108], %r237;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r181, %r181, 7;
	 shf.r.clamp.b32    t2, %r181, %r181, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r181, 3;
	 xor.b32            %r180, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r233, %r233, 17;
	 shf.r.clamp.b32    t2, %r233, %r233, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r233, 10;
	 xor.b32            %r182, t1, t2;
	}
	// inline asm
	add.s32 	%r357, %r182, %r180;
	add.s32 	%r358, %r357, %r213;
	add.s32 	%r241, %r358, %r605;
	st.local.u32 	[%rd12+112], %r241;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r135, %r135, 7;
	 shf.r.clamp.b32    t2, %r135, %r135, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r135, 3;
	 xor.b32            %r184, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r237, %r237, 17;
	 shf.r.clamp.b32    t2, %r237, %r237, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 10;
	 xor.b32            %r186, t1, t2;
	}
	// inline asm
	add.s32 	%r359, %r186, %r184;
	add.s32 	%r360, %r359, %r217;
	add.s32 	%r245, %r360, %r181;
	st.local.u32 	[%rd12+116], %r245;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r139, %r139, 7;
	 shf.r.clamp.b32    t2, %r139, %r139, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r139, 3;
	 xor.b32            %r188, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r241, %r241, 17;
	 shf.r.clamp.b32    t2, %r241, %r241, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 10;
	 xor.b32            %r190, t1, t2;
	}
	// inline asm
	add.s32 	%r361, %r190, %r188;
	add.s32 	%r362, %r361, %r221;
	add.s32 	%r249, %r362, %r135;
	st.local.u32 	[%rd12+120], %r249;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r193, %r193, 7;
	 shf.r.clamp.b32    t2, %r193, %r193, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r193, 3;
	 xor.b32            %r192, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r245, %r245, 17;
	 shf.r.clamp.b32    t2, %r245, %r245, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r245, 10;
	 xor.b32            %r194, t1, t2;
	}
	// inline asm
	add.s32 	%r363, %r194, %r192;
	add.s32 	%r364, %r363, %r225;
	add.s32 	%r253, %r364, %r139;
	st.local.u32 	[%rd12+124], %r253;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r197, %r197, 7;
	 shf.r.clamp.b32    t2, %r197, %r197, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r197, 3;
	 xor.b32            %r196, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r249, %r249, 17;
	 shf.r.clamp.b32    t2, %r249, %r249, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r249, 10;
	 xor.b32            %r198, t1, t2;
	}
	// inline asm
	add.s32 	%r365, %r198, %r196;
	add.s32 	%r366, %r365, %r229;
	add.s32 	%r257, %r366, %r193;
	st.local.u32 	[%rd12+128], %r257;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r201, %r201, 7;
	 shf.r.clamp.b32    t2, %r201, %r201, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r201, 3;
	 xor.b32            %r200, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 17;
	 shf.r.clamp.b32    t2, %r253, %r253, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 10;
	 xor.b32            %r202, t1, t2;
	}
	// inline asm
	add.s32 	%r367, %r202, %r200;
	add.s32 	%r368, %r367, %r233;
	add.s32 	%r261, %r368, %r197;
	st.local.u32 	[%rd12+132], %r261;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r205, %r205, 7;
	 shf.r.clamp.b32    t2, %r205, %r205, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r205, 3;
	 xor.b32            %r204, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r257, %r257, 17;
	 shf.r.clamp.b32    t2, %r257, %r257, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r257, 10;
	 xor.b32            %r206, t1, t2;
	}
	// inline asm
	add.s32 	%r369, %r206, %r204;
	add.s32 	%r370, %r369, %r237;
	add.s32 	%r265, %r370, %r201;
	st.local.u32 	[%rd12+136], %r265;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r209, %r209, 7;
	 shf.r.clamp.b32    t2, %r209, %r209, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r209, 3;
	 xor.b32            %r208, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r261, %r261, 17;
	 shf.r.clamp.b32    t2, %r261, %r261, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r261, 10;
	 xor.b32            %r210, t1, t2;
	}
	// inline asm
	add.s32 	%r371, %r210, %r208;
	add.s32 	%r372, %r371, %r241;
	add.s32 	%r269, %r372, %r205;
	st.local.u32 	[%rd12+140], %r269;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r213, %r213, 7;
	 shf.r.clamp.b32    t2, %r213, %r213, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r213, 3;
	 xor.b32            %r212, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 17;
	 shf.r.clamp.b32    t2, %r265, %r265, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 10;
	 xor.b32            %r214, t1, t2;
	}
	// inline asm
	add.s32 	%r373, %r214, %r212;
	add.s32 	%r374, %r373, %r245;
	add.s32 	%r273, %r374, %r209;
	st.local.u32 	[%rd12+144], %r273;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r217, %r217, 7;
	 shf.r.clamp.b32    t2, %r217, %r217, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r217, 3;
	 xor.b32            %r216, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r269, %r269, 17;
	 shf.r.clamp.b32    t2, %r269, %r269, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r269, 10;
	 xor.b32            %r218, t1, t2;
	}
	// inline asm
	add.s32 	%r375, %r218, %r216;
	add.s32 	%r376, %r375, %r249;
	add.s32 	%r277, %r376, %r213;
	st.local.u32 	[%rd12+148], %r277;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r221, %r221, 7;
	 shf.r.clamp.b32    t2, %r221, %r221, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r221, 3;
	 xor.b32            %r220, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r273, %r273, 17;
	 shf.r.clamp.b32    t2, %r273, %r273, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r273, 10;
	 xor.b32            %r222, t1, t2;
	}
	// inline asm
	add.s32 	%r377, %r222, %r220;
	add.s32 	%r378, %r377, %r253;
	add.s32 	%r281, %r378, %r217;
	st.local.u32 	[%rd12+152], %r281;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r225, %r225, 7;
	 shf.r.clamp.b32    t2, %r225, %r225, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r225, 3;
	 xor.b32            %r224, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 17;
	 shf.r.clamp.b32    t2, %r277, %r277, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 10;
	 xor.b32            %r226, t1, t2;
	}
	// inline asm
	add.s32 	%r379, %r226, %r224;
	add.s32 	%r380, %r379, %r257;
	add.s32 	%r285, %r380, %r221;
	st.local.u32 	[%rd12+156], %r285;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r229, %r229, 7;
	 shf.r.clamp.b32    t2, %r229, %r229, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r229, 3;
	 xor.b32            %r228, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r281, %r281, 17;
	 shf.r.clamp.b32    t2, %r281, %r281, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r281, 10;
	 xor.b32            %r230, t1, t2;
	}
	// inline asm
	add.s32 	%r381, %r230, %r228;
	add.s32 	%r382, %r381, %r261;
	add.s32 	%r289, %r382, %r225;
	st.local.u32 	[%rd12+160], %r289;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r233, %r233, 7;
	 shf.r.clamp.b32    t2, %r233, %r233, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r233, 3;
	 xor.b32            %r232, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r285, %r285, 17;
	 shf.r.clamp.b32    t2, %r285, %r285, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r285, 10;
	 xor.b32            %r234, t1, t2;
	}
	// inline asm
	add.s32 	%r383, %r234, %r232;
	add.s32 	%r384, %r383, %r265;
	add.s32 	%r293, %r384, %r229;
	st.local.u32 	[%rd12+164], %r293;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r237, %r237, 7;
	 shf.r.clamp.b32    t2, %r237, %r237, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r237, 3;
	 xor.b32            %r236, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 17;
	 shf.r.clamp.b32    t2, %r289, %r289, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 10;
	 xor.b32            %r238, t1, t2;
	}
	// inline asm
	add.s32 	%r385, %r238, %r236;
	add.s32 	%r386, %r385, %r269;
	add.s32 	%r297, %r386, %r233;
	st.local.u32 	[%rd12+168], %r297;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r241, %r241, 7;
	 shf.r.clamp.b32    t2, %r241, %r241, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r241, 3;
	 xor.b32            %r240, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r293, %r293, 17;
	 shf.r.clamp.b32    t2, %r293, %r293, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r293, 10;
	 xor.b32            %r242, t1, t2;
	}
	// inline asm
	add.s32 	%r387, %r242, %r240;
	add.s32 	%r388, %r387, %r273;
	add.s32 	%r301, %r388, %r237;
	st.local.u32 	[%rd12+172], %r301;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r245, %r245, 7;
	 shf.r.clamp.b32    t2, %r245, %r245, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r245, 3;
	 xor.b32            %r244, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r297, %r297, 17;
	 shf.r.clamp.b32    t2, %r297, %r297, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r297, 10;
	 xor.b32            %r246, t1, t2;
	}
	// inline asm
	add.s32 	%r389, %r246, %r244;
	add.s32 	%r390, %r389, %r277;
	add.s32 	%r305, %r390, %r241;
	st.local.u32 	[%rd12+176], %r305;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r249, %r249, 7;
	 shf.r.clamp.b32    t2, %r249, %r249, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r249, 3;
	 xor.b32            %r248, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 17;
	 shf.r.clamp.b32    t2, %r301, %r301, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 10;
	 xor.b32            %r250, t1, t2;
	}
	// inline asm
	add.s32 	%r391, %r250, %r248;
	add.s32 	%r392, %r391, %r281;
	add.s32 	%r309, %r392, %r245;
	st.local.u32 	[%rd12+180], %r309;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r253, %r253, 7;
	 shf.r.clamp.b32    t2, %r253, %r253, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r253, 3;
	 xor.b32            %r252, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r305, %r305, 17;
	 shf.r.clamp.b32    t2, %r305, %r305, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r305, 10;
	 xor.b32            %r254, t1, t2;
	}
	// inline asm
	add.s32 	%r393, %r254, %r252;
	add.s32 	%r394, %r393, %r285;
	add.s32 	%r313, %r394, %r249;
	st.local.u32 	[%rd12+184], %r313;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r257, %r257, 7;
	 shf.r.clamp.b32    t2, %r257, %r257, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r257, 3;
	 xor.b32            %r256, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r309, %r309, 17;
	 shf.r.clamp.b32    t2, %r309, %r309, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r309, 10;
	 xor.b32            %r258, t1, t2;
	}
	// inline asm
	add.s32 	%r395, %r258, %r256;
	add.s32 	%r396, %r395, %r289;
	add.s32 	%r317, %r396, %r253;
	st.local.u32 	[%rd12+188], %r317;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r261, %r261, 7;
	 shf.r.clamp.b32    t2, %r261, %r261, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r261, 3;
	 xor.b32            %r260, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r313, %r313, 17;
	 shf.r.clamp.b32    t2, %r313, %r313, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r313, 10;
	 xor.b32            %r262, t1, t2;
	}
	// inline asm
	add.s32 	%r397, %r262, %r260;
	add.s32 	%r398, %r397, %r293;
	add.s32 	%r321, %r398, %r257;
	st.local.u32 	[%rd12+192], %r321;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r265, %r265, 7;
	 shf.r.clamp.b32    t2, %r265, %r265, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r265, 3;
	 xor.b32            %r264, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r317, %r317, 17;
	 shf.r.clamp.b32    t2, %r317, %r317, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r317, 10;
	 xor.b32            %r266, t1, t2;
	}
	// inline asm
	add.s32 	%r399, %r266, %r264;
	add.s32 	%r400, %r399, %r297;
	add.s32 	%r275, %r400, %r261;
	st.local.u32 	[%rd12+196], %r275;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r269, %r269, 7;
	 shf.r.clamp.b32    t2, %r269, %r269, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r269, 3;
	 xor.b32            %r268, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r321, %r321, 17;
	 shf.r.clamp.b32    t2, %r321, %r321, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r321, 10;
	 xor.b32            %r270, t1, t2;
	}
	// inline asm
	add.s32 	%r401, %r270, %r268;
	add.s32 	%r402, %r401, %r301;
	add.s32 	%r279, %r402, %r265;
	st.local.u32 	[%rd12+200], %r279;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r273, %r273, 7;
	 shf.r.clamp.b32    t2, %r273, %r273, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r273, 3;
	 xor.b32            %r272, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r275, %r275, 17;
	 shf.r.clamp.b32    t2, %r275, %r275, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r275, 10;
	 xor.b32            %r274, t1, t2;
	}
	// inline asm
	add.s32 	%r403, %r274, %r272;
	add.s32 	%r404, %r403, %r305;
	add.s32 	%r283, %r404, %r269;
	st.local.u32 	[%rd12+204], %r283;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r277, %r277, 7;
	 shf.r.clamp.b32    t2, %r277, %r277, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r277, 3;
	 xor.b32            %r276, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r279, %r279, 17;
	 shf.r.clamp.b32    t2, %r279, %r279, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r279, 10;
	 xor.b32            %r278, t1, t2;
	}
	// inline asm
	add.s32 	%r405, %r278, %r276;
	add.s32 	%r406, %r405, %r309;
	add.s32 	%r287, %r406, %r273;
	st.local.u32 	[%rd12+208], %r287;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r281, %r281, 7;
	 shf.r.clamp.b32    t2, %r281, %r281, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r281, 3;
	 xor.b32            %r280, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r283, %r283, 17;
	 shf.r.clamp.b32    t2, %r283, %r283, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r283, 10;
	 xor.b32            %r282, t1, t2;
	}
	// inline asm
	add.s32 	%r407, %r282, %r280;
	add.s32 	%r408, %r407, %r313;
	add.s32 	%r291, %r408, %r277;
	st.local.u32 	[%rd12+212], %r291;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r285, %r285, 7;
	 shf.r.clamp.b32    t2, %r285, %r285, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r285, 3;
	 xor.b32            %r284, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r287, %r287, 17;
	 shf.r.clamp.b32    t2, %r287, %r287, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r287, 10;
	 xor.b32            %r286, t1, t2;
	}
	// inline asm
	add.s32 	%r409, %r286, %r284;
	add.s32 	%r410, %r409, %r317;
	add.s32 	%r295, %r410, %r281;
	st.local.u32 	[%rd12+216], %r295;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r289, %r289, 7;
	 shf.r.clamp.b32    t2, %r289, %r289, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r289, 3;
	 xor.b32            %r288, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r291, %r291, 17;
	 shf.r.clamp.b32    t2, %r291, %r291, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r291, 10;
	 xor.b32            %r290, t1, t2;
	}
	// inline asm
	add.s32 	%r411, %r290, %r288;
	add.s32 	%r412, %r411, %r321;
	add.s32 	%r299, %r412, %r285;
	st.local.u32 	[%rd12+220], %r299;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r293, %r293, 7;
	 shf.r.clamp.b32    t2, %r293, %r293, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r293, 3;
	 xor.b32            %r292, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r295, %r295, 17;
	 shf.r.clamp.b32    t2, %r295, %r295, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r295, 10;
	 xor.b32            %r294, t1, t2;
	}
	// inline asm
	add.s32 	%r413, %r294, %r292;
	add.s32 	%r414, %r413, %r275;
	add.s32 	%r303, %r414, %r289;
	st.local.u32 	[%rd12+224], %r303;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r297, %r297, 7;
	 shf.r.clamp.b32    t2, %r297, %r297, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r297, 3;
	 xor.b32            %r296, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r299, %r299, 17;
	 shf.r.clamp.b32    t2, %r299, %r299, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r299, 10;
	 xor.b32            %r298, t1, t2;
	}
	// inline asm
	add.s32 	%r415, %r298, %r296;
	add.s32 	%r416, %r415, %r279;
	add.s32 	%r307, %r416, %r293;
	st.local.u32 	[%rd12+228], %r307;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r301, %r301, 7;
	 shf.r.clamp.b32    t2, %r301, %r301, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r301, 3;
	 xor.b32            %r300, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r303, %r303, 17;
	 shf.r.clamp.b32    t2, %r303, %r303, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r303, 10;
	 xor.b32            %r302, t1, t2;
	}
	// inline asm
	add.s32 	%r417, %r302, %r300;
	add.s32 	%r418, %r417, %r283;
	add.s32 	%r311, %r418, %r297;
	st.local.u32 	[%rd12+232], %r311;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r305, %r305, 7;
	 shf.r.clamp.b32    t2, %r305, %r305, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r305, 3;
	 xor.b32            %r304, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r307, %r307, 17;
	 shf.r.clamp.b32    t2, %r307, %r307, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r307, 10;
	 xor.b32            %r306, t1, t2;
	}
	// inline asm
	add.s32 	%r419, %r306, %r304;
	add.s32 	%r420, %r419, %r287;
	add.s32 	%r315, %r420, %r301;
	st.local.u32 	[%rd12+236], %r315;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r309, %r309, 7;
	 shf.r.clamp.b32    t2, %r309, %r309, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r309, 3;
	 xor.b32            %r308, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r311, %r311, 17;
	 shf.r.clamp.b32    t2, %r311, %r311, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r311, 10;
	 xor.b32            %r310, t1, t2;
	}
	// inline asm
	add.s32 	%r421, %r310, %r308;
	add.s32 	%r422, %r421, %r291;
	add.s32 	%r319, %r422, %r305;
	st.local.u32 	[%rd12+240], %r319;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r313, %r313, 7;
	 shf.r.clamp.b32    t2, %r313, %r313, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r313, 3;
	 xor.b32            %r312, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r315, %r315, 17;
	 shf.r.clamp.b32    t2, %r315, %r315, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r315, 10;
	 xor.b32            %r314, t1, t2;
	}
	// inline asm
	add.s32 	%r423, %r314, %r312;
	add.s32 	%r424, %r423, %r295;
	add.s32 	%r323, %r424, %r309;
	st.local.u32 	[%rd12+244], %r323;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r317, %r317, 7;
	 shf.r.clamp.b32    t2, %r317, %r317, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r317, 3;
	 xor.b32            %r316, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r319, %r319, 17;
	 shf.r.clamp.b32    t2, %r319, %r319, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r319, 10;
	 xor.b32            %r318, t1, t2;
	}
	// inline asm
	add.s32 	%r425, %r318, %r316;
	add.s32 	%r426, %r425, %r299;
	add.s32 	%r427, %r426, %r313;
	st.local.u32 	[%rd12+248], %r427;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r321, %r321, 7;
	 shf.r.clamp.b32    t2, %r321, %r321, 18;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r321, 3;
	 xor.b32            %r320, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r323, %r323, 17;
	 shf.r.clamp.b32    t2, %r323, %r323, 19;
	 xor.b32            t1, t1, t2;
	 shr.u32            t2, %r323, 10;
	 xor.b32            %r322, t1, t2;
	}
	// inline asm
	add.s32 	%r428, %r322, %r320;
	add.s32 	%r429, %r428, %r303;
	add.s32 	%r430, %r429, %r317;
	st.local.u32 	[%rd12+252], %r430;
	mov.u32 	%r615, -1694144372;
	mov.u32 	%r614, 1359893119;
	mov.u32 	%r613, -1521486534;
	mov.u32 	%r612, 1013904242;
	mov.u32 	%r611, -1150833019;
	mov.u32 	%r610, 1779033703;
	mov.u32 	%r609, 528734635;
	mov.u32 	%r608, 1541459225;
	bra.uni 	BB0_13;

BB0_63:
	mul.wide.u32 	%rd81, %r607, 4;
	add.s64 	%rd82, %rd12, %rd81;
	ld.local.u32 	%r606, [%rd82];

BB0_13:
	add.s64 	%rd98, %rd21, 1;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r610, %r610, 2;
	 shf.r.clamp.b32    t2, %r610, %r610, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r610, %r610, 22;
	 xor.b32            %r431, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r614, %r614, 6;
	 shf.r.clamp.b32    t2, %r614, %r614, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r614, %r614, 25;
	 xor.b32            %r433, t1, t2;
	}
	// inline asm
	not.b32 	%r435, %r614;
	and.b32  	%r436, %r609, %r435;
	and.b32  	%r437, %r614, %r615;
	xor.b32  	%r438, %r436, %r437;
	mul.wide.u32 	%rd60, %r607, 4;
	mov.u64 	%rd61, k;
	add.s64 	%rd62, %rd61, %rd60;
	add.s32 	%r439, %r438, %r608;
	add.s32 	%r440, %r439, %r433;
	ld.const.u32 	%r441, [%rd62];
	add.s32 	%r442, %r440, %r441;
	add.s32 	%r22, %r442, %r606;
	add.s32 	%r608, %r22, %r613;
	add.s32 	%r443, %r608, 1541459225;
	setp.gt.u32	%p8, %r443, 65535;
	setp.eq.s32	%p9, %r607, 60;
	and.pred  	%p10, %p9, %p8;
	@%p10 bra 	BB0_5;

	add.s64 	%rd98, %rd21, 1;
	xor.b32  	%r448, %r611, %r612;
	and.b32  	%r449, %r610, %r448;
	and.b32  	%r450, %r611, %r612;
	xor.b32  	%r451, %r449, %r450;
	add.s32 	%r452, %r431, %r451;
	add.s32 	%r613, %r452, %r22;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r613, %r613, 2;
	 shf.r.clamp.b32    t2, %r613, %r613, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r613, %r613, 22;
	 xor.b32            %r444, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r608, %r608, 6;
	 shf.r.clamp.b32    t2, %r608, %r608, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r608, %r608, 25;
	 xor.b32            %r446, t1, t2;
	}
	// inline asm
	not.b32 	%r453, %r608;
	and.b32  	%r454, %r615, %r453;
	and.b32  	%r455, %r608, %r614;
	xor.b32  	%r456, %r454, %r455;
	add.s32 	%r457, %r607, 1;
	mul.wide.u32 	%rd63, %r457, 4;
	add.s64 	%rd65, %rd61, %rd63;
	add.s64 	%rd66, %rd12, %rd63;
	add.s32 	%r458, %r456, %r609;
	add.s32 	%r459, %r458, %r446;
	ld.const.u32 	%r460, [%rd65];
	add.s32 	%r461, %r459, %r460;
	ld.local.u32 	%r462, [%rd66];
	add.s32 	%r26, %r461, %r462;
	add.s32 	%r609, %r26, %r612;
	setp.eq.s32	%p11, %r457, 60;
	add.s32 	%r463, %r609, 1541459225;
	setp.gt.u32	%p12, %r463, 65535;
	and.pred  	%p13, %p11, %p12;
	@%p13 bra 	BB0_5;

	add.s64 	%rd98, %rd21, 1;
	xor.b32  	%r468, %r610, %r611;
	and.b32  	%r469, %r613, %r468;
	and.b32  	%r470, %r610, %r611;
	xor.b32  	%r471, %r469, %r470;
	add.s32 	%r472, %r444, %r471;
	add.s32 	%r612, %r472, %r26;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r612, %r612, 2;
	 shf.r.clamp.b32    t2, %r612, %r612, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r612, %r612, 22;
	 xor.b32            %r464, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r609, %r609, 6;
	 shf.r.clamp.b32    t2, %r609, %r609, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r609, %r609, 25;
	 xor.b32            %r466, t1, t2;
	}
	// inline asm
	not.b32 	%r473, %r609;
	and.b32  	%r474, %r614, %r473;
	and.b32  	%r475, %r609, %r608;
	xor.b32  	%r476, %r474, %r475;
	add.s32 	%r477, %r607, 2;
	mul.wide.u32 	%rd67, %r477, 4;
	add.s64 	%rd69, %rd61, %rd67;
	add.s64 	%rd70, %rd12, %rd67;
	add.s32 	%r478, %r476, %r615;
	add.s32 	%r479, %r478, %r466;
	ld.const.u32 	%r480, [%rd69];
	add.s32 	%r481, %r479, %r480;
	ld.local.u32 	%r482, [%rd70];
	add.s32 	%r30, %r481, %r482;
	add.s32 	%r615, %r30, %r611;
	setp.eq.s32	%p14, %r477, 60;
	add.s32 	%r483, %r615, 1541459225;
	setp.gt.u32	%p15, %r483, 65535;
	and.pred  	%p16, %p14, %p15;
	@%p16 bra 	BB0_5;

	add.s64 	%rd98, %rd21, 1;
	xor.b32  	%r488, %r613, %r610;
	and.b32  	%r489, %r612, %r488;
	and.b32  	%r490, %r613, %r610;
	xor.b32  	%r491, %r489, %r490;
	add.s32 	%r492, %r464, %r491;
	add.s32 	%r611, %r492, %r30;
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r611, %r611, 2;
	 shf.r.clamp.b32    t2, %r611, %r611, 13;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r611, %r611, 22;
	 xor.b32            %r484, t1, t2;
	}
	// inline asm
	// inline asm
	{
	 .reg .u32 t1;
	 .reg .u32 t2;
	 shf.r.clamp.b32    t1, %r615, %r615, 6;
	 shf.r.clamp.b32    t2, %r615, %r615, 11;
	 xor.b32            t1, t1, t2;
	 shf.r.clamp.b32    t2, %r615, %r615, 25;
	 xor.b32            %r486, t1, t2;
	}
	// inline asm
	not.b32 	%r493, %r615;
	and.b32  	%r494, %r608, %r493;
	and.b32  	%r495, %r615, %r609;
	xor.b32  	%r496, %r494, %r495;
	add.s32 	%r497, %r607, 3;
	mul.wide.u32 	%rd71, %r497, 4;
	add.s64 	%rd73, %rd61, %rd71;
	add.s64 	%rd74, %rd12, %rd71;
	add.s32 	%r498, %r496, %r614;
	add.s32 	%r499, %r498, %r486;
	ld.const.u32 	%r500, [%rd73];
	add.s32 	%r501, %r499, %r500;
	ld.local.u32 	%r502, [%rd74];
	add.s32 	%r34, %r501, %r502;
	add.s32 	%r614, %r34, %r610;
	setp.eq.s32	%p17, %r497, 60;
	add.s32 	%r503, %r614, 1541459225;
	setp.gt.u32	%p18, %r503, 65535;
	and.pred  	%p19, %p17, %p18;
	@%p19 bra 	BB0_5;

	xor.b32  	%r504, %r612, %r613;
	and.b32  	%r505, %r611, %r504;
	and.b32  	%r506, %r612, %r613;
	xor.b32  	%r507, %r505, %r506;
	add.s32 	%r508, %r484, %r507;
	add.s32 	%r610, %r508, %r34;
	add.s32 	%r607, %r607, 4;
	setp.lt.u32	%p20, %r607, 64;
	@%p20 bra 	BB0_63;

	add.s32 	%r39, %r610, 1779033703;
	st.local.u32 	[%rd4+64], %r39;
	add.s32 	%r511, %r611, -1150833019;
	st.local.u32 	[%rd4+68], %r511;
	add.s32 	%r512, %r612, 1013904242;
	st.local.u32 	[%rd4+72], %r512;
	add.s32 	%r513, %r613, -1521486534;
	st.local.u32 	[%rd4+76], %r513;
	add.s32 	%r514, %r614, 1359893119;
	st.local.u32 	[%rd4+80], %r514;
	add.s32 	%r515, %r615, -1694144372;
	st.local.u32 	[%rd4+84], %r515;
	add.s32 	%r516, %r609, 528734635;
	st.local.u32 	[%rd4+88], %r516;
	add.s32 	%r596, %r608, 1541459225;
	st.local.u32 	[%rd4+92], %r596;
	mov.u32 	%r40, 0;
	mov.u32 	%r618, %r40;
	bra.uni 	BB0_19;

BB0_62:
	mul.wide.s32 	%rd79, %r618, 4;
	add.s64 	%rd80, %rd4, %rd79;
	ld.local.u32 	%r39, [%rd80+64];

BB0_19:
	mov.pred 	%p90, 0;
	mov.u32 	%r619, 1;
	setp.lt.s32	%p22, %r39, 0;
	@%p22 bra 	BB0_20;

	add.s32 	%r620, %r40, 1;
	and.b32  	%r520, %r39, 1073741824;
	mov.u32 	%r619, 2;
	setp.ne.s32	%p24, %r520, 0;
	@%p24 bra 	BB0_21;

	add.s32 	%r620, %r40, 2;
	and.b32  	%r522, %r39, 536870912;
	mov.u32 	%r619, 3;
	setp.ne.s32	%p26, %r522, 0;
	@%p26 bra 	BB0_21;

	add.s32 	%r620, %r40, 3;
	and.b32  	%r524, %r39, 268435456;
	mov.u32 	%r619, 4;
	setp.ne.s32	%p28, %r524, 0;
	@%p28 bra 	BB0_21;

	add.s32 	%r620, %r40, 4;
	and.b32  	%r526, %r39, 134217728;
	mov.u32 	%r619, 5;
	setp.ne.s32	%p30, %r526, 0;
	@%p30 bra 	BB0_21;

	add.s32 	%r620, %r40, 5;
	and.b32  	%r528, %r39, 67108864;
	mov.u32 	%r619, 6;
	setp.ne.s32	%p32, %r528, 0;
	@%p32 bra 	BB0_21;

	add.s32 	%r620, %r40, 6;
	and.b32  	%r530, %r39, 33554432;
	mov.u32 	%r619, 7;
	setp.ne.s32	%p34, %r530, 0;
	@%p34 bra 	BB0_21;

	add.s32 	%r620, %r40, 7;
	and.b32  	%r532, %r39, 16777216;
	mov.u32 	%r619, 8;
	setp.ne.s32	%p36, %r532, 0;
	@%p36 bra 	BB0_21;

	add.s32 	%r620, %r40, 8;
	and.b32  	%r534, %r39, 8388608;
	mov.u32 	%r619, 9;
	setp.ne.s32	%p38, %r534, 0;
	@%p38 bra 	BB0_21;

	add.s32 	%r620, %r40, 9;
	and.b32  	%r536, %r39, 4194304;
	mov.u32 	%r619, 10;
	setp.ne.s32	%p40, %r536, 0;
	@%p40 bra 	BB0_21;

	add.s32 	%r620, %r40, 10;
	and.b32  	%r538, %r39, 2097152;
	mov.u32 	%r619, 11;
	setp.ne.s32	%p42, %r538, 0;
	@%p42 bra 	BB0_21;

	add.s32 	%r620, %r40, 11;
	and.b32  	%r540, %r39, 1048576;
	mov.u32 	%r619, 12;
	setp.ne.s32	%p44, %r540, 0;
	@%p44 bra 	BB0_21;

	add.s32 	%r620, %r40, 12;
	and.b32  	%r542, %r39, 524288;
	mov.u32 	%r619, 13;
	setp.ne.s32	%p46, %r542, 0;
	@%p46 bra 	BB0_21;

	add.s32 	%r620, %r40, 13;
	and.b32  	%r544, %r39, 262144;
	mov.u32 	%r619, 14;
	setp.ne.s32	%p48, %r544, 0;
	@%p48 bra 	BB0_21;

	add.s32 	%r620, %r40, 14;
	and.b32  	%r546, %r39, 131072;
	mov.u32 	%r619, 15;
	setp.ne.s32	%p50, %r546, 0;
	@%p50 bra 	BB0_21;

	add.s32 	%r620, %r40, 15;
	and.b32  	%r548, %r39, 65536;
	mov.u32 	%r619, 16;
	setp.ne.s32	%p52, %r548, 0;
	@%p52 bra 	BB0_21;

	add.s32 	%r620, %r40, 16;
	and.b32  	%r550, %r39, 32768;
	mov.u32 	%r619, 17;
	setp.ne.s32	%p54, %r550, 0;
	@%p54 bra 	BB0_21;

	add.s32 	%r620, %r40, 17;
	and.b32  	%r552, %r39, 16384;
	mov.u32 	%r619, 18;
	setp.ne.s32	%p56, %r552, 0;
	@%p56 bra 	BB0_21;

	add.s32 	%r620, %r40, 18;
	and.b32  	%r554, %r39, 8192;
	mov.u32 	%r619, 19;
	setp.ne.s32	%p58, %r554, 0;
	@%p58 bra 	BB0_21;

	add.s32 	%r620, %r40, 19;
	and.b32  	%r556, %r39, 4096;
	mov.u32 	%r619, 20;
	setp.ne.s32	%p60, %r556, 0;
	@%p60 bra 	BB0_21;

	add.s32 	%r620, %r40, 20;
	and.b32  	%r558, %r39, 2048;
	mov.u32 	%r619, 21;
	setp.ne.s32	%p62, %r558, 0;
	@%p62 bra 	BB0_21;

	add.s32 	%r620, %r40, 21;
	and.b32  	%r560, %r39, 1024;
	mov.u32 	%r619, 22;
	setp.ne.s32	%p64, %r560, 0;
	@%p64 bra 	BB0_21;

	add.s32 	%r620, %r40, 22;
	and.b32  	%r562, %r39, 512;
	mov.u32 	%r619, 23;
	setp.ne.s32	%p66, %r562, 0;
	@%p66 bra 	BB0_21;

	add.s32 	%r620, %r40, 23;
	and.b32  	%r564, %r39, 256;
	mov.u32 	%r619, 24;
	setp.ne.s32	%p68, %r564, 0;
	@%p68 bra 	BB0_21;

	add.s32 	%r620, %r40, 24;
	and.b32  	%r566, %r39, 128;
	mov.u32 	%r619, 25;
	setp.ne.s32	%p70, %r566, 0;
	@%p70 bra 	BB0_21;

	add.s32 	%r620, %r40, 25;
	and.b32  	%r568, %r39, 64;
	mov.u32 	%r619, 26;
	setp.ne.s32	%p72, %r568, 0;
	@%p72 bra 	BB0_21;

	add.s32 	%r620, %r40, 26;
	and.b32  	%r570, %r39, 32;
	mov.u32 	%r619, 27;
	setp.ne.s32	%p74, %r570, 0;
	@%p74 bra 	BB0_21;

	add.s32 	%r620, %r40, 27;
	and.b32  	%r572, %r39, 16;
	mov.u32 	%r619, 28;
	setp.ne.s32	%p76, %r572, 0;
	@%p76 bra 	BB0_21;

	add.s32 	%r620, %r40, 28;
	and.b32  	%r574, %r39, 8;
	mov.u32 	%r619, 29;
	setp.ne.s32	%p78, %r574, 0;
	@%p78 bra 	BB0_21;

	add.s32 	%r620, %r40, 29;
	and.b32  	%r576, %r39, 4;
	mov.u32 	%r619, 30;
	setp.ne.s32	%p80, %r576, 0;
	@%p80 bra 	BB0_21;

	add.s32 	%r620, %r40, 30;
	and.b32  	%r578, %r39, 2;
	mov.u32 	%r619, 31;
	setp.ne.s32	%p82, %r578, 0;
	@%p82 bra 	BB0_21;

	add.s32 	%r620, %r40, 31;
	and.b32  	%r580, %r39, 1;
	setp.eq.b32	%p84, %r580, 1;
	mov.pred 	%p90, -1;
	mov.u32 	%r619, 32;
	@!%p84 bra 	BB0_56;
	bra.uni 	BB0_21;

BB0_56:
	add.s32 	%r40, %r40, 32;
	add.s32 	%r618, %r618, 1;
	setp.lt.s32	%p86, %r618, 8;
	@%p86 bra 	BB0_62;
	bra.uni 	BB0_57;

BB0_20:
	mov.u32 	%r620, %r40;

BB0_21:
	selp.u32	%r581, 1, 0, %p90;
	add.s32 	%r75, %r581, %r618;
	selp.b32	%r76, 0, %r619, %p90;
	mov.u32 	%r582, 24;
	sub.s32 	%r77, %r582, %r76;
	setp.gt.s32	%p85, %r77, -1;
	mul.wide.s32 	%rd75, %r75, 4;
	add.s64 	%rd76, %rd4, %rd75;
	ld.local.u32 	%r78, [%rd76+64];
	@%p85 bra 	BB0_54;
	bra.uni 	BB0_22;

BB0_54:
	shr.u32 	%r591, %r78, %r77;
	and.b32  	%r621, %r591, 255;
	bra.uni 	BB0_55;

BB0_22:
	add.s32 	%r583, %r76, -24;
	shl.b32 	%r584, %r78, %r583;
	add.s32 	%r585, %r75, 1;
	mul.wide.s32 	%rd77, %r585, 4;
	add.s64 	%rd78, %rd4, %rd77;
	ld.local.u32 	%r586, [%rd78+64];
	shr.u32 	%r587, %r586, 1;
	mov.u32 	%r588, 55;
	sub.s32 	%r589, %r588, %r76;
	shr.u32 	%r590, %r587, %r589;
	add.s32 	%r621, %r590, %r584;

BB0_55:
	shl.b32 	%r592, %r620, 8;
	add.s32 	%r40, %r621, %r592;

BB0_57:
	add.s64 	%rd98, %rd21, 1;
	ld.param.u32 	%r597, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_1];
	setp.le.u32	%p87, %r40, %r597;
	@%p87 bra 	BB0_5;

	mov.u16 	%rs5, 1;
	st.volatile.global.u8 	[%rd10], %rs5;
	st.volatile.global.u8 	[%rd2], %rs5;
	mov.u32 	%r623, 0;
	mov.u64 	%rd99, %rd4;
	mov.u64 	%rd100, %rd11;

BB0_59:
	.pragma "nounroll";
	ld.local.v4.u8 	{%rs6, %rs7, %rs8, %rs9}, [%rd99+32];
	st.global.u8 	[%rd100], %rs9;
	st.global.u8 	[%rd100+1], %rs8;
	st.global.u8 	[%rd100+2], %rs7;
	st.global.u8 	[%rd100+3], %rs6;
	add.s64 	%rd100, %rd100, 4;
	add.s64 	%rd99, %rd99, 4;
	add.s32 	%r623, %r623, 4;
	setp.lt.s32	%p88, %r623, 20;
	@%p88 bra 	BB0_59;

	ld.local.v2.u8 	{%rs14, %rs15}, [%rd4+54];
	ld.local.u8 	%rs18, [%rd4+53];
	st.global.u8 	[%rd11+20], %rs15;
	st.global.u8 	[%rd11+21], %rs14;
	st.global.u8 	[%rd11+22], %rs18;
	mov.u32 	%r624, 0;
	mov.u64 	%rd101, %rd4;
	mov.u64 	%rd102, %rd3;

BB0_61:
	.pragma "nounroll";
	add.s64 	%rd98, %rd21, 1;
	ld.local.v4.u8 	{%rs19, %rs20, %rs21, %rs22}, [%rd101+64];
	st.global.u8 	[%rd102], %rs22;
	st.global.u8 	[%rd102+1], %rs21;
	st.global.u8 	[%rd102+2], %rs20;
	st.global.u8 	[%rd102+3], %rs19;
	add.s64 	%rd102, %rd102, 4;
	add.s64 	%rd101, %rd101, 4;
	add.s32 	%r624, %r624, 4;
	setp.lt.s32	%p89, %r624, 32;
	@%p89 bra 	BB0_61;

BB0_5:
	mov.u64 	%rd21, %rd98;
	mul.hi.s64 	%rd43, %rd21, -6640827866535438581;
	add.s64 	%rd44, %rd43, %rd21;
	shr.u64 	%rd45, %rd44, 63;
	shr.s64 	%rd46, %rd44, 6;
	add.s64 	%rd47, %rd46, %rd45;
	mul.lo.s64 	%rd48, %rd47, 100;
	sub.s64 	%rd49, %rd21, %rd48;
	setp.ne.s64	%p4, %rd49, 99;
	@%p4 bra 	BB0_10;

	ld.volatile.global.u8 	%rs4, [%rd2];
	setp.eq.s16	%p5, %rs4, 0;
	@%p5 bra 	BB0_10;

	mov.u32 	%r602, %tid.x;
	mov.u32 	%r601, %ctaid.x;
	mov.u32 	%r600, %ntid.x;
	mad.lo.s32 	%r599, %r600, %r601, %r602;
	ld.param.u64 	%rd85, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_7];
	cvta.to.global.u64 	%rd84, %rd85;
	cvt.u64.u32	%rd83, %r599;
	shl.b64 	%rd50, %rd83, 3;
	add.s64 	%rd51, %rd84, %rd50;
	st.global.u64 	[%rd51], %rd21;
	setp.ne.s32	%p6, %r599, 1;
	@%p6 bra 	BB0_9;

	ld.param.u64 	%rd87, [_Z13kernel_sha256PhjS_PVbS1_PVljPl_param_5];
	cvta.to.global.u64 	%rd86, %rd87;
	st.volatile.global.u64 	[%rd86], %rd21;

BB0_9:
	ret;
}


